{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: Pienso, luego predigo üí°**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Juan Mi√±o\n",
    "- Nombre de alumno 2: Diego Espinoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/juansebm/MDS7202/tree/main/Lab%2011%20-%20Agentes%20Autonomos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack es un entorno de decisi√≥n secuencial modelado como un Proceso de Decisi√≥n de Markov (MDP), donde un jugador busca vencer al dealer obteniendo una suma de cartas lo m√°s cercana posible a 21, sin pasarse. Recordemos que el Blackjack bajo la formulaci√≥n de MDP cumple con la propiedad de Markov que dice que la probabilidad del siguiente estado y recompensa depende √∫nicamente del estado actual y la acci√≥n tomada, no del historial entero. \n",
    "\n",
    "- Estados (Observaciones): Cada estado est√° representado por una tupla de tres valores:\n",
    "    - player_sum: suma actual de las cartas del jugador (rango 4‚Äì21),\n",
    "    - dealer_showing: valor de la carta visible del dealer (1‚Äì10),\n",
    "    - usable_ace: indicador binario (1 si el jugador tiene un as usable, es decir, que puede contar como 11 sin pasar de 21, y 0 en caso contrario).\n",
    "\n",
    "- Acciones: El agente (jugador) puede elegir entre dos acciones:\n",
    "    - hit (1): pedir una carta adicional,\n",
    "    - stick (0): detenerse y pasar el turno al dealer.\n",
    "\n",
    "- Recompensas:\n",
    "    - Victoria: +1\n",
    "    - Derrota: -1\n",
    "    - Empate: 0\n",
    "    - Victoria con Blackjack natural (21 con dos cartas): +1.5 (si natural=True), o +1 si se ignora la bonificaci√≥n.\n",
    "\n",
    "- T√©rmino del episodio: El episodio finaliza si:\n",
    "    - el jugador hace hit y su mano supera 21 (bust)\n",
    "    - el jugador hace stick y se resuelve el juego contra el dealer.\n",
    "\n",
    "Din√°mica del juego:\n",
    "El jugador comienza con dos cartas visibles; el dealer con una visible y una oculta. Las cartas se extraen con reemplazo. Las figuras (J, Q, K) valen 10, los ases valen 1 u 11 (si no provocan bust), y el resto tiene su valor num√©rico.\n",
    "\n",
    "Si el jugador elige hit y su suma supera 21, pierde inmediatamente (bust). Si elige stick, el dealer revela su carta oculta y roba hasta alcanzar una suma de 17 o m√°s. Si el dealer se pasa, el jugador gana. Si ambos permanecen dentro del l√≠mite, gana quien tenga la suma m√°s alta; si empatan, el resultado es un empate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 5000 episodios: -0.4020\n",
      "Desviaci√≥n est√°ndar: 0.8951\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 5000\n",
    "n_movimientos_maximo = 100000\n",
    "\n",
    "for episodio in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en 5000 episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = [-1.5, -0.5, 0.5, 1.25, 1.75]#estos bins sirven para que se vean las recompensas\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(recompensas, bins=bins, edgecolor='black', rwidth=0.8)\n",
    "\n",
    "plt.xticks([-1, 0, 1, 1.5])\n",
    "plt.xlabel(\"Recompensa obtenida\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de recompensas\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?\n",
    "\n",
    "- El rendimiento de la pol√≠tica aleatoria es pobre, porque la mayor√≠a de las simulaciones muestran recompensas negativas. Ahora bien, esta pol√≠tica sirve como l√≠nea base o baseline para comparar agentes entrenados mediante aprendizaje por refuerzo o reinforcemente learning, cualquier pol√≠tica que mejore el promedio de -0.38 podr√° considerarse una mejora. \n",
    "\n",
    "¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?\n",
    "\n",
    "- Un promedio negativo de las recompensas para 5000 simulaciones/partidas y una cantidad m√°xima de 100 pasos/movimientos permitidos por partida, implica que las partidas en donde se pierde tomando aleatoriamente decisiones es mayor que aquellas en las que se gana. Es decir, en nuestro caso la cantidad de partidas perdidas es aproximadamente el doble que la cantidad de partidas ganadas. Por otro lado, podemos decir que el puntaje de las partidas empatadas no afecta en nada porque la recompensa de dichas partidas es nula, por lo que en el promedio de las recompensas nunca se ver√°n reflejadas las partidas empatadas o que quedan en `draw`. Esto adem√°s, se podr√≠a interpretar como que el mecanismo, aunque sea aleatorio, nos diese el resultado de un dado o moneda cargada, con la probabilidad dada por la ley de Laplace m√°s inclinada hacia el caso de partidas perdidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `Stable Baselines3 (SB3)` es una biblioteca de python que se usa para entrenar y evaluar agentes de aprendizaje por refuerzo (RL), y adem√°s es implementada sobre PyTorch (usada por lo dem√°s, para implementar modelos de RL que son bastantes cercanos al estado del arte). Por otro lado, esta ofrece una colecci√≥n de algoritmos RL listos para usar con soporte para entornos de Gymnasium. De la clase de aprendizaje reforzado, podemos tomar como ejemplo el alfortitmo DQN (tambi√©n conocido como [Deep Q-Network](https://markelsanz14.medium.com/introducci√≥n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)).\n",
    "\n",
    " En particular, seg√∫n dicha referencia, ese algoritmo combina el algoritmo Q-learning con redes neuronales profundas (que son fant√°sticas para aproximar funciones no lineales), que en realidad son dos redes neuronales, la primera de las cuales aproxima la funci√≥n Q(s, a, $\\theta$) (del estado `s` y acci√≥n `a` actuales) mientras que la segunda parametrizada por $\\theta'$ aproxima los valores Q pero del siguiente estado s' y acci√≥n a'. Aqu√≠ el aprendizaje ocurre en la red primera y no en la objetivo (la segunda), donde los parametros del aprendizaje de la primera se copian a la segunda transmitiendo el aprendizaje. Aqu√≠ queremos minimizar una funci√≥n de p√©rdida que es la resta de la ecuaci√≥n de bellman en DQN. \n",
    "\n",
    "Esta funci√≥n de p√©rdida se minimiza usando el algoritmo de descenso de gradientes. En este contexto, el agente no es m√°s que el objeto `model`, el cual usa el algoritmo DQN como su cerebro para aprender/tomar decisiones, porque por definici√≥n un agente es el componente que 1) observa el estado del entorno o ambiente, 2) toma decisiones, 3) recibe recompensas, 4) actualiza su pol√≠tica para mejorar su comportamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e605bd84d44429aae88d5e1e38f221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1055da340>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente Blackjack\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, exportamos el modelo entrenado a un archivo .zip en pocas l√≠neas de c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model.save(\"dqn_blackjack\")\n",
    "del model\n",
    "model = DQN.load(\"dqn_blackjack\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.114, 0.9460465104845533)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model, model.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo es el performance de su agente?\n",
    "\n",
    "- El agente que usa DQN por detr√°s tiene un promedio de recompensas de -0.3, lo que es ligeramente mayor al caso del baseline que eleg√≠a aleatoriamente los movimientos por cada partida de blackjack (-0.053 que puede cambiar y -0.38 respectivamente).\n",
    "\n",
    "¬øEs mejor o peor que el escenario baseline?\n",
    "\n",
    "- El performance, como se dijo en la parte anterior, es mejor que el escenario baseline porque el agente entrenado aprende una pol√≠tica que maximiza la recompensa esperada, mientras que la pol√≠tica aleatoria no incorpora ninguna estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo vemos que clase de Python pertenecen los estados, usando el m√©todo .reset para el `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el tipo de clase de python del estado es:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "estado, info = env.reset()\n",
    "print(\"el tipo de clase de python del estado es: \", type(estado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def obtener_accion(modelo, estado):\n",
    "    \"\"\"Esta funci√≥n obtiene la acci√≥n que el modelo tomar√≠a \n",
    "    para cierto estado.\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo que se usar√° para predecir el movimiento del agente\n",
    "                (que como ya nos habremos dado cuenta, es el DQN).\n",
    "        estado: estado de la partida. \n",
    "\n",
    "    Returns:\n",
    "        accion.item(): la accion es un array o tensor, mientras que agregar \n",
    "                       .item() extrae el valor num√©rico simple. Aqu√≠\n",
    "                       deterministic=True indica que queremos la acci√≥n m√°s\n",
    "                       probable u √≥ptima, sin exploraci√≥n aleatoria. Por √∫ltimo,\n",
    "                       esta acci√≥n es un 1 o un 0, que representa la acci√≥n que \n",
    "                       tomar√° el agente (hit/stick respectivamente). \n",
    "    \"\"\"\n",
    "    accion, informacion = modelo.predict(estado, deterministic=True)\n",
    "    return accion.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que el estado es una 3-tupla de la forma (player_sum, dealer_card, usable_ace), entonces el jugador que tiene 6 (de suma), el dealer muestra 7 y el jugador no tiene un As usable se representar√° por la tupla (6, 7, False), mientras que el jugador que tiene 19, cuyo dealer muestra 3 y el jugador tiene un As usable tendr√° una tupla que lo representa que es (19, 3, True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): 1\n",
      "Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "estado2 = (19, 3, True)\n",
    "\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): {accion1}\")   # 0 = stick, 1 = hit\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): {accion2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acci√≥n que toma el agente para el primer estado es = 1, es decir, hit. En este contexto, el jugador tiene una suma baja (6), y el dealer muestra un 7, que es una carta fuerte, por lo que la estrategia b√°sica recomienda pedir carta (hit) para mejorar la mano, ya que quedarse con 6 casi seguro perder√≠a. \n",
    "\n",
    "Por otro lado, el jugador en el segundo estado tiene una mano fuerte (19), con un As usable que ayuda a no pasarse, mientras que el dealer muestra una carta d√©bil (3), por lo que podemos concluir que la estrategia b√°sica recomendar√≠a quedarse (stick) con un 19 para no arriesgar pasarse.\n",
    "\n",
    "Por lo tanto, ambas decisiones del agente entrenado por DQM son consistentes con las estrategias que una persona tomar√≠a en la vida real en el juego blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (8.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El par√°metro `Continuous=True` son acciones continuas, donde el agente puede elegir valores continuos reales para los motores, es decir, puede controlar la potencia con precisi√≥n variable (digamos, motor principal a potencia 40%, motor lateral derecho al 20%, etc.). Mientras que si ponemos `continuous=False` el agente elige entre un conjunto finito de acciones predefinidas (por ejemplo, encender motor principal, motor lateral izquierdo o derecho, o no hacer nada, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac√°`:\n",
    "\n",
    "LunarLander es un entorno de simulaci√≥n donde el objetivo es aterrizar una nave espacial suavemente en una plataforma ubicada en (0, 0). Es un cl√°sico problema de optimizaci√≥n de trayectorias, que est√° inspirado en el principio del m√°ximo de Pontryagin. Para este existen dos versiones; 1) continuous=False (por defecto) indica acciones discretas, mientras que continuous=True indica acciones continuas.\n",
    "\n",
    "- Espacio de Acciones: \n",
    "    - Discreto `continuous=False`\n",
    "        - 0: no hacer nada\n",
    "        - 1: encender motor lateral izquierdo\n",
    "        - 2: encender motor principal\n",
    "        - 3: encender motor lateral derecho\n",
    "    - Continuo `continuous=True`: la acci√≥n es un vector [main, lateral] $\\epsilon$ [-1, 1]:\n",
    "        - main < 0: motor principal apagado\n",
    "        - 0 < main < 1: motor principal de 50% a 100% de potencia\n",
    "        - lateral < -0.5: motor izquierdo\n",
    "        - lateral > 0.5: motor derecho\n",
    "        - la potencia lateral tambi√©n escala de 50% a 100%\n",
    "\n",
    "- Espacio de Observaci√≥n: Es un vector de 8 dimensiones;\n",
    "    - x, y: posici√≥n\n",
    "    - vx, vy: velocidad lineal\n",
    "    - angle: √°ngulo\n",
    "    - angular_velocity: velocidad angular\n",
    "    - leg1_contact, leg2_contact: contacto de piernas en el suelo\n",
    "\n",
    "- Recompensas: \n",
    "    - son recompensas por paso:\n",
    "        - mejora por acercarse a la base, reducir la velocidad, o mantener horizontalidad\n",
    "        - se suman +10 puntos por cada pierna tocando el suelo\n",
    "        - se restan -0.03 por uso lateral, y -0.3 por uso del motor principal\n",
    "    - recompensa final:\n",
    "        - +100 si aterriza correctamente\n",
    "        - -100 si se estrella\n",
    "    - se considera un episodio exitoso como aquel cuya recompensa total es mayor o igual a 200.\n",
    "\n",
    "En comparaci√≥n con blackjack: en este ambiente LunarLander las acciones controlan motores para aterrizar suavemente, mientras que en blackjack las acciones controlan decisiones de juego con din√°mica estoc√°stica (es decir, aleatoria, al menos en el caso del baseline que ya hab√≠amos visto) y reglas fijas (es decir, las reglas del juego). Lunar Lander requiero modelos que manejen acciones continuas, mientras que Blackjack es ideal para introducir MDP's y enfoques de exploraci√≥n vs explotaci√≥n. Por otro lado, en este ambiente las decisiones son infinitas en cantidad (porque el tipo de acci√≥n es continua), mientras que en el caso del blackjack solo hay 2 posibles decisiones (Hit y Stick). El riesgo de tomar una acci√≥n tambi√©n es diferente, porque un mal movimiento puede causar ca√≠da o choque de una nave, mientras que en el primer caso no puede sino provocar una p√©rdida simb√≥lica de un juego. \n",
    "\n",
    "Ahora bien, el ambiente de Lunar Lander cumple con la propiedad de MDP (Markov Decision Process) porque el estado actual del mismo contiene toda la informaci√≥n necesaria para predecir el futuro (informaci√≥n codificada en `env.step()`), independientemente del pasado. Es decir, no hay dependencias ni variables ocultas que el agente necesite memorizar para pensar en el futuro movimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 10 episodios: -97.9389\n",
      "Desviaci√≥n est√°ndar: 73.3275\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 10\n",
    "n_movimientos_maximo = 100\n",
    "\n",
    "for episode in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env_lunar.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env_lunar.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env_lunar.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env_lunar.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en {n_simulaciones} episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el valor promedio tan negativo indica que el agente tiende mucho m√°s a fallar sistem√°ticamente que a aterrizar suavemente en su tarea, lo cual era de esperar si se tomaba una pol√≠tica (no informadas) sin conocimiento del entorno que depende del azar. Est√° cerca de -139 porque el castigo por estrellarse era -200. As√≠, explorando las decisiones al azar no obtenemos sino malos resultados. \n",
    "\n",
    "Por lo tanto, el desempe√±o de esta pol√≠tica es pobre, pero nos servir√° como una referencia m√≠nima para comparar futuras pol√≠ticas entrenadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos el modelo PPO que s√≠ es compatible con acciones continuas, como lo requiere nuestro caso. De la [siguiente referencia](https://www-geeksforgeeks-org.translate.goog/machine-learning/a-brief-introduction-to-proximal-policy-optimization/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc), podemos ver que, la Optimizaci√≥n Proximal de Pol√≠ticas (PPO) es un m√©todo que ayuda a un agente a mejorar sus acciones para obtener mejores recompensas. Al igual que otros m√©todos de gradiente de pol√≠ticas, modifica directamente la toma de decisiones del agente. Sin embargo, a diferencia de otros, la PPO a√±ade control adicional para evitar que estos cambios sean demasiado grandes. El objetivo principal de PPO es encontrar un equilibrio entre dos aspectos: 1) maximizar el objetivo (este es el n√∫cleo de la optimizaci√≥n de pol√≠ticas, donde la pol√≠tica del agente se ajusta para maximizar las recompensas esperadas) y 2) mantener las actualizaciones peque√±as (los grandes cambios pueden arruinar el aprendizaje, por lo que se debe ajustar con precisi√≥n la velocidad del aprendizaje).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12db5010f62148dc81e591a827d962e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x368274f10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "timesteps = 10000\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model_lunar = PPO(\"MlpPolicy\", env_lunar, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente LunarLander\n",
    "model_lunar.learn(total_timesteps=timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-120.399338565, 47.07241200895624)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lunar.save(\"PPO_LunarLander\")\n",
    "del model_lunar\n",
    "model_lunar = PPO.load(\"PPO_LunarLander\", env=env_lunar)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model_lunar, model_lunar.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es pero que el baseline (-120 vs -97) pero porque a√∫n no lo hemos optimizado, por lo que en la siguiente subsecci√≥n lo corregiremos, y porque adem√°s no hemos usado ning√∫n hiperpar√°metro como s√≠ lo hicimos en el caso de las decisiones aleatorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094adb95bc884ab28bc7b7606a491216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con timesteps=2000000, learning_rate=0.0005, batch_size=32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159.51521190761963, 112.16941103430986)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "pasostemporales_totales = 2000000\n",
    "\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) \n",
    "\n",
    "print(f\"Entrenando con timesteps={pasostemporales_totales}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env_lunar,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=0,\n",
    "            batch_size=batch_size)\n",
    "model.learn(total_timesteps=pasostemporales_totales, progress_bar=True)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env_lunar, n_eval_episodes=100)\n",
    "env_lunar.close()\n",
    "\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"agent_performance.gif\", width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el agente que usa PPO para optimizar el movimiento del cohete que aterriza en la luna, lo hace muy bien (con un promedio de recompensas de 159, lo que es muy cercano a 200, lo √≥ptimo). Por otro lado, podemos decir que el movimiento que realiza la astronave es casi vertical, tal y como se puede ver del gif que contiene cinco de las iteraciones/episodios/partidas. Los par√°metros que se utilizaron fueron: `learning_rate = 0.0005`,`batch_size = 32`, `pasostemporales_totales = 2000000`. Esta operaci√≥n o serie de dos millones de partidas se demor√≥ alrededor de 30 minutos, lo que indica que es bastante lento pero mas que nada porque necesita mucho entrenamiento. En general, el agente toma decisiones (que van en un espectro continuo de acciones) de manera precisa como si fuese un experto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usar los links para las api keys: https://aistudio.google.com/app/u/1/apikey y https://app.tavily.com/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5474,
     "status": "ok",
     "timestamp": 1731272598047,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 180
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\"paper_1.pdf\", \"paper_3.pdf\"]\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def cargar_texto_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    documentos = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        texto = page.extract_text()\n",
    "        if texto:\n",
    "            documentos.append(Document(page_content=texto, metadata={\"source\": path, \"page\": i}))\n",
    "    return documentos\n",
    "\n",
    "\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    docs.extend(cargar_texto_pdf(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x12a16fa10>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3},\n",
    "                                     )\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='60827877-74e6-4dcb-8b7f-73accef6a5df', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='The whole process of price optimization based on the price demand elasticity can be\\ndescribed using the following Figure 1:\\nMathematics 2021 , 9, x FOR PEER REVIEW 5 of 17 \\n \\n ticity is estimated based on the rate segmen ts [27], log‚Äìlog regression model [54], auto-\\nregressive distributed lag model [55], linear and non-linear demand function [43], logistic \\nregression [50] and multiple logistic model [56,57]. \\nTran [55] used an autoregressive distributed lag model for the evaluation of eco-\\nnomic factors and their connection to demand for a luxury hotel in the US. The study \\nworks with the price demand elastici ty coefficient that varies from ‚àí0.03 in the long term \\nto ‚àí0.02 in the short term. A study by Rossell√≥ et al. [58] focused on the demand for ac-\\ncommodation services in Germany, the UK, France and the Netherlands, while focusing \\non the elasticity of demand, findin g values of the coefficient from ‚àí0.51 to ‚àí4. A combina-'),\n",
       " Document(id='e572ce6e-3c23-40e1-9628-487afa82be65', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='Mathematics 2021 ,9, 1552 5 of 16\\nthe price demand elasticity coefÔ¨Åcient that varies from \\x000.03 in the long term to \\x000.02 in\\nthe short term. A study by Rossell √≥et al. [ 58] focused on the demand for accommodation\\nservices in Germany, the UK, France and the Netherlands, while focusing on the elasticity\\nof demand, Ô¨Ånding values of the coefÔ¨Åcient from \\x000.51 to\\x004. A combination of the\\npreviously mentioned approaches towards the elasticity of demand (destination level and\\nnarrow product level category) can be found in a study by Damonte et al. [ 59], where the\\nprice demand elasticity differs based on the level of services and the destination size, where\\nthe properly deÔ¨Åned region can bring better results than the aggregate approach. The same\\nresults are proposed by Canina and Calver as well [60].\\nA study by Hiemstra and Ismail [ 61] focused on the elasticity of demand considering\\nseveral variables and the impact of taxes as well, where the supply elasticity coefÔ¨Åcient'),\n",
       " Document(id='901af129-d545-4501-8932-5d43438b5631', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='As proposed in the literature review, there are various approaches toward measuring\\nthe price elasticity of the demand. Within this model, we assume three different options\\nthat can be applied, namely using (1) the log‚Äìlog linear model used by Petricek et al. [ 54], (2)\\nan arc elasticity estimate using the Monte Carlo simulation, or (3) an individual coefÔ¨Åcient\\nvalues setting.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"como se define el coeficiente de elasticidad precio-demanda\"\n",
    "relevant_documents = retriever.invoke(question)\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provedtoimprovethepredictions, thus\"RF-w/ocancellations\"showstheerrorswhen\n",
      "not using cancellations data, and \"RF-Net\" when taking that data into account.\n",
      "Observe in Table 4.1 that both errors were significantly reduced with the random\n",
      "forest model developed in this thesis, yielding a 9.96% Mean Percentage Error for RF\n",
      "with cancellations data.\n",
      "1In the case where ùëëùëñ= 0, the percentage error is replaced with its corresponding absolute error\n",
      "to avoid involving infinite values.\n",
      "34\n",
      "\n",
      "As proposed in the literature review, there are various approaches toward measuring\n",
      "the price elasticity of the demand. Within this model, we assume three different options\n",
      "that can be applied, namely using (1) the log‚Äìlog linear model used by Petricek et al. [ 54], (2)\n",
      "an arc elasticity estimate using the Monte Carlo simulation, or (3) an individual coefÔ¨Åcient\n",
      "values setting.\n",
      "\n",
      "4.2 Numerical results\n",
      "The first step is to select training and testing data sets to create the random forest\n",
      "model. Two-thirds of the data for 4 years of bookings (see section 3.1 for more details\n",
      "about the data) are randomly assigned to the training set, and the remaining one-\n",
      "third is the testing set. Since booking between days are assumed to be independent,\n",
      "time sequences are not considered for the splitting.\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(format_docs(relevant_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics 2021 ,9, 1552 9 of 16\n",
      "it is always necessary to respect the cost aspect in particular. The total cost of consummated\n",
      "accommodation can be determined for a suitable presentation as the sum of the Ô¨Åxed and\n",
      "variable costs expressed as unit quantities (usually on a daily basis), as follows:\n",
      "ATC b=FC\n",
      "√•b\n",
      "b=1Qb+VC\n",
      "√•b\n",
      "b=1Qb, (11)\n",
      "where index bindicates a speciÔ¨Åc reservation and Qbindicates the ‚Äúorder‚Äù of a given\n",
      "reservation in terms of the number of used capacities. It is always necessary to focus on the\n",
      "marginal revenue and marginal costs resulting from the next added unit of length of stay\n",
      "(usually one night/room). Indeed, a possible reduction in the price per unit of length of\n",
      "stay is possible only under the following conditions:\n",
      "MC b\u0014MR b, (12)\n",
      "where index bagain indicates one added unit length of stay for reservation. The speciÔ¨Åc\n",
      "setting of a possible price reduction is, therefore, always individual and again it is necessary\n",
      "\n",
      "The proposed optimized rate is mainly reÔ¨Çecting the segment characteristics, namely\n",
      "the price demand elasticity, but cannot be used as the Ô¨Ånal selling rate as this proposed\n",
      "rate still omits several other factors that have a direct impact on the product pricing.\n",
      "At the moment when it is possible to start from the basic offer price, it is time to\n",
      "determine the price of a speciÔ¨Åc reservation. This approach is what we could call dynamic\n",
      "price formation because the price changes virtually on a daily basis (in some cases even\n",
      "more often) and it also needs to always reÔ¨Çect the current situation but take into account the\n",
      "minimum acceptable price as well. When guaranteeing the price of a particular reservation,\n",
      "it is then necessary to focus on several aspects that affect it and that are appropriate (and in\n",
      "some cases necessary) to monitor.\n",
      "The creation of this price depends on several key factors [ 68‚Äì72], which can be sum-\n",
      "marized in the following:\n",
      "\u000f Length of stay (LOS);\n",
      "\u000f Occupancy (Occ);\n",
      "\n",
      "rate still omits several other factors that ha ve a direct impact on the product pricing. \n",
      "At the moment when it is possible to start from the basic offer price, it is time to \n",
      "determine the price of a specific reservation. This approach is what we could call dynamic \n",
      "price formation because the price changes virt ually on a daily basis (in some cases even \n",
      "more often) and it also needs to always refl ect the current situation but take into account \n",
      "the minimum acceptable price as well. When gu aranteeing the price of a particular reser-\n",
      "vation, it is then necessary to focus on several aspects that affect it and that are appropriate (and in some cases necessary) to monitor. \n",
      "The creation of this price depends on seve ral key factors [68‚Äì72], which can be sum-\n",
      "marized in the following: \n",
      "‚Ä¢ Length of stay (LOS); \n",
      "‚Ä¢ Occupancy (Occ); \n",
      "Figure 2. Monte Carlo simulation - price.\n",
      "The coefÔ¨Åcient of price elasticity was measured at the value \u00000.85 (as the mean\n"
     ]
    }
   ],
   "source": [
    "retriever_chain = retriever | format_docs\n",
    "print(retriever_chain.invoke(\"¬øcomo podemos crear un codigo para optimizar el precio de las habitaciones de un hotel, si queremos maximizar los ingresos?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = '''\n",
    "Eres un asistente experto en programaci√≥n e interpretaci√≥n de resultados de varios papers cient√≠ficos acerca de los Ingresos de Hoteles.\n",
    "Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci√≥n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Idioma: {language}\n",
    "Respuesta √∫til:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La informaci√≥n proporcionada describe varios enfoques para la optimizaci√≥n de tarifas de habitaciones de hotel.  Se mencionan los siguientes aspectos:\n",
      "\n",
      "* **Factores que influyen en la optimizaci√≥n de precios:**  El precio de una habitaci√≥n depende de diversos factores, incluyendo el segmento de mercado, la ventana de reserva, el tipo de habitaci√≥n, la estrategia de precios (ej. $199.99 en lugar de $200), la marca del hotel, el pa√≠s, la moneda, etc.  Adem√°s, se consideran la duraci√≥n de la estancia (LOS), la ocupaci√≥n actual (Occ) y el tiempo de antelaci√≥n de la reserva (BLT) como atributos que influyen en el precio.\n",
      "\n",
      "* **Variables y restricciones:** Se utiliza una variable indicadora (ùêºùëñùëóùëòùëô) para indicar si un precio espec√≠fico (ùëùùëñ) es seleccionado para una combinaci√≥n dada de segmento de mercado (ùëò), ventana de reserva (ùëó) y tipo de habitaci√≥n (ùëô).  Solo se puede seleccionar un precio por cada combinaci√≥n.  Algunos segmentos de mercado tienen precios fijos (ùëò‚ààùíÆ0), los cuales deben incorporarse al modelo para considerar la p√©rdida de capacidad que generan. Existe una jerarqu√≠a de tipos de habitaciones que permite actualizaciones complementarias a tipos superiores si es necesario. La cantidad de habitaciones vendidas no puede exceder la capacidad total del hotel, considerando la jerarqu√≠a de actualizaciones.\n",
      "\n",
      "* **Modelos de optimizaci√≥n:** Se mencionan varios modelos: uno que se centra en la optimizaci√≥n de precios para una fecha y duraci√≥n de estancia espec√≠fica, basado en la demanda (del hotel y sus competidores), los costos de las habitaciones y la disponibilidad del hotel. Otro modelo, implementado en Carlson Rezidor Hotel Group, se basa en la elasticidad precio de la demanda, las tarifas del mercado (competidores), la disponibilidad de habitaciones, la demanda pronosticada (basada en datos hist√≥ricos de clientes: duraci√≥n de la estancia, segmento de tarifa, d√≠a de la semana y tiempo de antelaci√≥n) y reglas de negocio. Este √∫ltimo modelo aument√≥ los ingresos de los hoteles que lo implementaron en un 2-4% m√°s que los que no lo hicieron.  Finalmente, se menciona un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda en un segmento de mercado espec√≠fico antes del horizonte de planificaci√≥n.\n",
      "\n",
      "En resumen, la informaci√≥n describe la complejidad de la optimizaci√≥n de precios en hoteles, considerando m√∫ltiples variables, restricciones y diferentes enfoques de modelado para maximizar los ingresos.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, \n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"language\": lambda _: \"espa√±ol\"\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"que informaci√≥n tienes sobre optimizacion de tarifas de habitaciones de hoteles?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define de diferentes maneras dependiendo del estudio consultado:\n",
      "\n",
      "* **Tran [55]:**  Utiliza un modelo de retraso distribuido autorregresivo y encuentra que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo para un hotel de lujo en EE. UU.\n",
      "\n",
      "* **Rossell√≥ et al. [58]:**  En un estudio sobre la demanda de servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos, encontraron valores del coeficiente que oscilan entre -0.51 y -4.\n",
      "\n",
      "* **Damonte et al. [59] y Canina y Calver [60]:**  Estos estudios indican que la elasticidad precio-demanda difiere seg√∫n el nivel de servicios y el tama√±o del destino, sugiriendo que una regi√≥n bien definida puede proporcionar mejores resultados que un enfoque agregado.\n",
      "\n",
      "En resumen, no hay una √∫nica definici√≥n del coeficiente, sino que su valor var√≠a seg√∫n el m√©todo de c√°lculo, el tipo de alojamiento y el mercado analizado.  La informaci√≥n proporcionada muestra una amplia gama de valores, desde -0.02 hasta -4.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan varias estrategias para optimizar las tarifas de habitaciones de hoteles:\n",
      "\n",
      "* **Modelos de precios din√°micos:**  Se mencionan papers (Aziz et al., 2011; Anderson & Xie, 2016) que abordan modelos de precios din√°micos para la gesti√≥n de ingresos en hoteles.  Estos modelos permiten ajustar los precios en funci√≥n de factores como la demanda y la disponibilidad.\n",
      "\n",
      "* **M√©todos de asignaci√≥n de habitaciones:**  Se describe la comparaci√≥n entre el m√©todo de red anidada y el m√©todo de precio de puja para la asignaci√≥n de habitaciones (Pimentel et al., [29]),  con el m√©todo de red anidada mostrando un mejor rendimiento.  Adem√°s, se menciona la optimizaci√≥n de la asignaci√≥n usando modelos deterministas y estoc√°sticos de demanda (Goldman et al., [30]; Weatherford [31]; De Boer et al. [32]),  en conjunto con pol√≠ticas de control de reservas (Pimentel et al., [29]).\n",
      "\n",
      "* **Optimizaci√≥n de la disponibilidad (niveles de sobreventa):** Pimentel et al. [28] desarrollaron un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda para segmentos de mercado espec√≠ficos, optimizando la disponibilidad y la asignaci√≥n a esos segmentos.\n",
      "\n",
      "* **Segmentaci√≥n del mercado:**  El estudio de Du et al. (2016) analiza la necesidad de cooperaci√≥n con terceros por parte de los proveedores de servicios que adoptan la segmentaci√≥n del mercado, lo que implica una estrategia de precios diferenciada seg√∫n el segmento.\n",
      "\n",
      "* **An√°lisis de la elasticidad precio de la demanda:** Petricek et al. (2020) identifican el comportamiento del consumidor bas√°ndose en la elasticidad precio, lo que puede informar la estrategia de precios.\n",
      "\n",
      "\n",
      "La informaci√≥n proporcionada no detalla completamente cada estrategia, pero s√≠ indica su uso en la optimizaci√≥n de ingresos de hoteles.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada menciona que la predicci√≥n de la demanda de habitaciones en hoteles se basa en datos hist√≥ricos de reservas, considerando dos variables relacionadas con el tiempo: la fecha de reserva y el tiempo hasta el consumo.  Se menciona la selecci√≥n de m√©todos como una consideraci√≥n importante, pero no se especifican m√©todos concretos utilizados.  Adem√°s, se indica que el pron√≥stico puede ser de noches de habitaci√≥n o llegadas, y puede tener diferentes niveles de agregaci√≥n (total, por categor√≠a de tarifa, por duraci√≥n de la estancia, o alguna combinaci√≥n).  Finalmente, se mencionan estudios que modelan la demanda tur√≠stica (Rossell√≥ et al., 2005),  la demanda de alojamiento en hoteles urbanos (Canina y Carvell, 2005), y el uso de simulaci√≥n de Monte Carlo para pronosticar llegadas y ocupaci√≥n hotelera (Zakhary et al., 2021).  Sin embargo, no se detallan los m√©todos espec√≠ficos empleados en cada estudio.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preguntas_respuestas = [\n",
    "    (\n",
    "        \"¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\",\n",
    "        \"El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\",\n",
    "        \"Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\",\n",
    "        \"Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "    print(f\"Pregunta: {pregunta}\")\n",
    "    print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "    respuesta_generada = rag_chain.invoke(pregunta)\n",
    "    print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_rag_chain(chunk_size=1000, k=3, search_type=\"similarity\"):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore_temp = FAISS.from_documents(splits, embedding)\n",
    "    retriever = vectorstore_temp.as_retriever(search_type=search_type, search_kwargs={\"k\": k})\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever_chain = retriever | format_docs\n",
    "\n",
    "    rag_template = '''\n",
    "    Eres un asistente experto en la interpretaci√≥n de resultados de varios papers cient√≠ficos acerca de los Ingresos de Hoteles.\n",
    "    Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "    Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "    Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "    Informaci√≥n relevante: {context}\n",
    "    Pregunta: {question}\n",
    "    Idioma: {language}\n",
    "    Respuesta √∫til:\n",
    "    '''\n",
    "    rag_prompt = PromptTemplate.from_template(rag_template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever_chain,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"language\": lambda _: \"espa√±ol\"\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 500, 'k': 3, 'search_type': 'similarity'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define a trav√©s de una f√≥rmula que incorpora un elemento de azar debido a la dependencia de la situaci√≥n del mercado actual, la cual puede diferir ligeramente de una perspectiva hist√≥rica.  La f√≥rmula exacta proporcionada es:\n",
      "\n",
      "Epd=0\n",
      "BBBB@ \n",
      "RQd2\n",
      "01\n",
      "s\u0002p\n",
      "2pe(Qd2\u0000m)2\n",
      "\n",
      "Adem√°s, se menciona que un estudio de Rossell et al. [58] encontr√≥ valores del coeficiente de elasticidad de demanda para servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos que variaban de -0.51 a -4.  Tambi√©n se indica que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo.  Sin embargo, no se proporciona una definici√≥n conceptual general del coeficiente.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, una estrategia para optimizar las tarifas de habitaciones de hoteles se basa en la elasticidad precio de la demanda.  Esta estrategia, implementada por el Carlson Rezidor Hotel Group, considera tambi√©n las tarifas de la competencia, la disponibilidad de habitaciones, la demanda pronosticada (basada en datos hist√≥ricos como la duraci√≥n de la estancia, el segmento de precio, el d√≠a de la semana y el tiempo de antelaci√≥n) y reglas de negocio.  Adicionalmente, se menciona un an√°lisis de gesti√≥n de ingresos (yield management) en el paper de Baker y Collier (1999), aunque no se detallan las estrategias espec√≠ficas utilizadas.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, los hoteles utilizan la simulaci√≥n de Monte Carlo para predecir procesos como llegadas, cancelaciones, duraci√≥n de la estancia, ausencias, estacionalidad y tendencias.  Tambi√©n se utiliza para pronosticar llegadas y ocupaci√≥n hotelera, as√≠ como para modelar llegadas, cancelaciones y reservas de grupo usando programaci√≥n no lineal.  Adicionalmente, la informaci√≥n menciona que la predicci√≥n de la demanda deber√≠a considerar dos variables relacionadas con el tiempo: el momento de la reserva y el tiempo hasta el consumo.  Finalmente, se indica que la selecci√≥n del m√©todo de pron√≥stico debe considerar el tipo de pron√≥stico (noches de habitaci√≥n o llegadas), el nivel de agregaci√≥n (total, por categor√≠a de tarifa, por duraci√≥n de la estancia o alguna combinaci√≥n), el tipo de datos (restringidos o no restringidos), el n√∫mero de per√≠odos a incluir en el pron√≥stico, qu√© datos usar, la cantidad de datos, el tratamiento de valores at√≠picos (debido a festivos o eventos especiales) y la medici√≥n de la precisi√≥n.\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 1000, 'k': 1, 'search_type': 'similarity'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada describe varios estudios que analizan el coeficiente de elasticidad precio-demanda, pero no define expl√≠citamente qu√© es.  Los estudios citados muestran rangos de valores para este coeficiente en diferentes contextos (a largo y corto plazo, diferentes pa√≠ses, niveles de servicio y tama√±o del destino), pero no ofrecen una definici√≥n formal.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Bas√°ndome en la informaci√≥n proporcionada, no se describen expl√≠citamente estrategias espec√≠ficas para optimizar las tarifas de habitaciones de hoteles en los papers citados.  Si bien los art√≠culos 45 (Aziz et al., 2011) y 46 (El Gayar et al., 2011) mencionan modelos para la gesti√≥n de ingresos de hoteles (\"Dynamic room pricing model\" y \"An integrated framework for advanced hotel revenue management\", respectivamente),  no detallan las estrategias concretas empleadas en dichos modelos.  El resto de los art√≠culos (44, 47 y 48) se centran en otros aspectos relacionados con la gesti√≥n hotelera, como la teor√≠a y pr√°ctica de la gesti√≥n de ingresos (44), la globalizaci√≥n o localizaci√≥n de preferencias del consumidor en reservas de habitaciones (47), y un estudio de precios en el sector a√©reo (48), sin profundizar en estrategias espec√≠ficas de fijaci√≥n de precios para hoteles.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada no especifica qu√© m√©todos concretos utilizan los hoteles para predecir la demanda.  Solo menciona que la predicci√≥n de la demanda de habitaciones, considerada una etapa de preprocesamiento en la gesti√≥n de ingresos (Revenue Management), debe estudiarse en detalle ya que determina la fiabilidad de la fase de optimizaci√≥n.  Se mencionan varios factores a considerar al realizar la predicci√≥n, como el tipo de pron√≥stico (noches de habitaci√≥n o llegadas), el nivel de agregaci√≥n de los datos, el tipo de datos (restringidos o no restringidos), el n√∫mero de periodos a incluir, qu√© datos usar, la cantidad de datos y el tratamiento de valores at√≠picos.  Sin embargo, no se nombran m√©todos espec√≠ficos de predicci√≥n.\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 2000, 'k': 5, 'search_type': 'mmr'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: El documento describe tres m√©todos para medir la elasticidad precio-demanda:\n",
      "\n",
      "1. **Modelo log-log lineal:**  Utilizado por Petricek et al. [54].  El documento no proporciona detalles sobre la f√≥rmula espec√≠fica de este modelo.\n",
      "\n",
      "2. **Estimaci√≥n de elasticidad de arco usando simulaci√≥n de Monte Carlo:**  Este m√©todo incorpora un elemento de probabilidad, reflejando la variabilidad de la situaci√≥n del mercado.  Se asume que el coeficiente de elasticidad precio-demanda tiene una distribuci√≥n normal N(Œº, œÉ¬≤). La f√≥rmula para este m√©todo se presenta como la ecuaci√≥n (3) y (5) en el texto, pero no se transcribe aqu√≠ por su complejidad.  La ecuaci√≥n (3) incluye  Qd1 (cantidad demandada inicial), Qd2 (cantidad demandada despu√©s del cambio de precio), P1 (precio inicial), P2 (precio final),  m (valor medio) y s (desviaci√≥n est√°ndar). La ecuaci√≥n (5) es una reformulaci√≥n de la ecuaci√≥n (4) para incorporar la distribuci√≥n de probabilidad de la elasticidad.\n",
      "\n",
      "3. **Asignaci√≥n de valores individuales de coeficientes:** El documento menciona esta opci√≥n como un tercer m√©todo, pero no proporciona m√°s detalles sobre c√≥mo se determinan estos valores individuales.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan varias estrategias para la optimizaci√≥n de tarifas de habitaciones de hotel:\n",
      "\n",
      "* **Estrategias de precios din√°micos:**  Se mencionan estudios que utilizan precios din√°micos para la gesti√≥n de ingresos en hoteles (Anderson & Xie, 2016; Bayoumi et al., 2013).  Estos modelos se basan en la variabilidad de precios y la segmentaci√≥n de la demanda.  Un ejemplo es el uso de multiplicadores de precios (Bayoumi et al., 2013).\n",
      "\n",
      "* **Optimizaci√≥n de precios usando metodolog√≠as como PERFORM:**  Koushik et al. (citados en el texto) describen la optimizaci√≥n de precios utilizando la metodolog√≠a PERFORM (herramientas de gesti√≥n de ingresos de InterContinental Hotel Group - IHG), basada en la ocupaci√≥n del hotel, la elasticidad precio-demanda y las tarifas de la competencia.  Esta metodolog√≠a utiliza un modelo determinista de Baker y Collier (citados en el texto) y considera la disponibilidad y la duraci√≥n de la estancia (LOS).\n",
      "\n",
      "* **Estrategias basadas en el tiempo de reserva (Booking Lead Time - BLT):**  Se menciona que herramientas de marketing basadas en el aumento de precios a medida que se llena la capacidad de alojamiento funcionan relativamente bien a largo plazo (citando un estudio no especificado en el texto).\n",
      "\n",
      "* **Estrategias neutrales, agresivas y conservadoras basadas en un margen de beneficio (Markup):**  Se describe una f√≥rmula para calcular un margen de beneficio basado en la ocupaci√≥n (MUp(Occ)),  y se indica que para estrategias agresivas o conservadoras, este margen se basar√≠a en una funci√≥n exponencial.  El precio de referencia (Pref) se basa en la mejor tarifa disponible (BAR) y el precio m√≠nimo (Pmin) en un an√°lisis de costos b√°sicos.\n",
      "\n",
      "\n",
      "La informaci√≥n proporcionada no detalla completamente cada estrategia, pero s√≠ indica las diferentes aproximaciones utilizadas para la optimizaci√≥n de precios en hoteles.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, los hoteles utilizan varias t√©cnicas para predecir la demanda.  Se mencionan espec√≠ficamente la regresi√≥n lineal, la regresi√≥n cuadr√°tica, la aproximaci√≥n spline y los bosques aleatorios (random forests).  De estos, los bosques aleatorios fueron seleccionados por proporcionar los mejores resultados, especialmente al manejar variables num√©ricas y categ√≥ricas.  Adicionalmente, se menciona que la predicci√≥n de la demanda en la industria hotelera debe considerar dos variables relacionadas con el tiempo: el momento de la reserva y el tiempo hasta el consumo.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "configuraciones = [\n",
    "    {\"chunk_size\": 500, \"k\": 3, \"search_type\": \"similarity\"},\n",
    "    {\"chunk_size\": 1000, \"k\": 1, \"search_type\": \"similarity\"},\n",
    "    {\"chunk_size\": 2000, \"k\": 5, \"search_type\": \"mmr\"},\n",
    "]\n",
    "\n",
    "for config in configuraciones:\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Probando configuraci√≥n: {config}\")\n",
    "    rag_chain = crear_rag_chain(**config)\n",
    "    \n",
    "    for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "        print(f\"\\nPregunta: {pregunta}\")\n",
    "        print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "        respuesta_generada = rag_chain.invoke(pregunta)\n",
    "        print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_search_tool = TavilySearchResults(max_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11757 sha256=0b7e13436c913051109ece5fa4d29212d078f86789a8b7fff35d410a96bd6ec7\n",
      "  Stored in directory: /Users/diegoespinoza/Library/Caches/pip/wheels/79/1d/c8/b64e19423cc5a2a339450ea5d145e7c8eb3d4aa2b150cde33b\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "wikpedia_search_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "tools = [wikpedia_search_tool, tavily_search_tool] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "spanish_prompt_text = \"Responde siempre en espa√±ol.\\n\\n\" + react_prompt.template\n",
    "\n",
    "prompt_es = PromptTemplate(\n",
    "    input_variables=react_prompt.input_variables,\n",
    "    template=spanish_prompt_text\n",
    ")\n",
    "\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "agent = create_react_agent(llm, tools, prompt_es)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito buscar informaci√≥n sobre algoritmos de optimizaci√≥n de precios en el contexto de reservas de hoteles.  Wikipedia podr√≠a tener informaci√≥n general sobre algoritmos de optimizaci√≥n, pero una b√∫squeda en un motor de b√∫squeda como tavily_search_results_json podr√≠a dar resultados m√°s espec√≠ficos y actuales sobre este problema particular.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"algoritmos optimizaci√≥n precios hoteles\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Crea algoritmos de Precios Autom√°ticos | Dataria RMS', 'url': 'https://www.dataria.com/blog/ayuda/crea-algoritmos-precios-automaticos/', 'content': 'Crea tus algoritmos de precios, simplifica la gesti√≥n de tarifas y mejora tu revenue. La industria hotelera se mueve r√°pido, y los precios no se quedan atr√°s. Saber cu√°l es el precio correcto en cada momento puede ser una tarea compleja, pero con el algoritmo Rate-o-matic¬Æ de Dataria, esa labor se vuelve autom√°tica. ¬øLa genialidad detr√°s de todo? Un algoritmo que, con solo recopilar algunos datos b√°sicos, establece el precio ideal para tu hotel, optimizando tus tarifas en tiempo real. [...] Los algoritmos de precios autom√°ticos de Dataria no solo mejoran la eficiencia operativa, sino que tambi√©n optimizan la rentabilidad de tu hotel. Imagina no tener que estar pendiente de las fluctuaciones del mercado constantemente. El sistema lo hace por ti, ajustando las tarifas de manera √≥ptima seg√∫n las variables clave, lo que permite aprovechar las oportunidades de ingresos en tiempo real.\\n\\n### Un conjunto ganador: Nuestra caja de herramientas para un Revenue inteligente [...] La tecnolog√≠a detr√°s de estos algoritmos de precios es tan flexible como eficiente. Gracias a su dise√±o paso a paso, permite al ecosistema hotelero no solo identificar su actual pol√≠tica de precios, sino reflexionar sobre ella. Rate-o-matic¬Æ puede adaptarse a las particularidades de cada hotel, asegurando que cada ajuste de tarifa est√© alineado con las condiciones actuales del mercado y las expectativas del hotel. ¬øEl resultado? Precios siempre justos, competitivos y alineados con la demanda,', 'score': 0.82560414}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci√≥n muestra un ejemplo de un algoritmo de optimizaci√≥n de precios para hoteles, \"Rate-o-matic¬Æ\", pero no detalla el algoritmo en s√≠.  Necesito m√°s informaci√≥n sobre los tipos de algoritmos usados para este prop√≥sito.  Wikipedia podr√≠a proporcionar una visi√≥n general de algoritmos de optimizaci√≥n relevantes.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"algoritmos de optimizaci√≥n\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Wikipedia no arroj√≥ resultados √∫tiles.  Sin embargo, la b√∫squeda en tavily_search_results_json proporcion√≥ un ejemplo de un algoritmo utilizado en la industria hotelera, aunque sin detalles t√©cnicos.  Puedo concluir bas√°ndome en esa informaci√≥n.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Existen algoritmos de optimizaci√≥n de precios para hoteles, como el \"Rate-o-matic¬Æ\" mencionado en los resultados de la b√∫squeda.  Estos algoritmos utilizan datos hist√≥ricos de reservas y variables como la demanda, la competencia y la estacionalidad para determinar la tarifa √≥ptima por habitaci√≥n que maximiza los ingresos.  Si bien no se especific√≥ el algoritmo exacto utilizado por \"Rate-o-matic¬Æ\",  es probable que se basen en t√©cnicas de aprendizaje autom√°tico, programaci√≥n din√°mica u otros m√©todos de optimizaci√≥n para encontrar la mejor combinaci√≥n de precios y ocupaci√≥n.  La informaci√≥n disponible no permite detallar el algoritmo espec√≠fico, pero confirma la existencia de soluciones algor√≠tmicas para este problema.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Existen algoritmos de optimizaci√≥n de precios para hoteles, como el \"Rate-o-matic¬Æ\" mencionado en los resultados de la b√∫squeda.  Estos algoritmos utilizan datos hist√≥ricos de reservas y variables como la demanda, la competencia y la estacionalidad para determinar la tarifa √≥ptima por habitaci√≥n que maximiza los ingresos.  Si bien no se especific√≥ el algoritmo exacto utilizado por \"Rate-o-matic¬Æ\",  es probable que se basen en t√©cnicas de aprendizaje autom√°tico, programaci√≥n din√°mica u otros m√©todos de optimizaci√≥n para encontrar la mejor combinaci√≥n de precios y ocupaci√≥n.  La informaci√≥n disponible no permite detallar el algoritmo espec√≠fico, pero confirma la existencia de soluciones algor√≠tmicas para este problema.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"¬øHay alg√∫n algoritmo de computaci√≥n que dada una base de datos\"\n",
    "                                  \"hist√≥rica de reservas de hotel con varias variables, pueda optimizar la\" \n",
    "                                  \"tarifa por habitaci√≥n del mismo, queriendo maximizar los ingresos?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta ac√°`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "√âxito!\n",
    "<center>\n",
    "<img src =\"https://media.tenor.com/MRQgxcelAV8AAAAM/perry-the-platypus-phineas-and-ferb.gif\" width = 400 />"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNs28AeL6L8BlEf3067k5qg",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
