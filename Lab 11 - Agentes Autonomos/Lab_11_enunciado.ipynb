{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: Pienso, luego predigo üí°**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Juan Mi√±o\n",
    "- Nombre de alumno 2: Diego Espinoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio Laboratorio 11](https://github.com/juansebm/MDS7202/tree/main/Lab%2011%20-%20Agentes%20Autonomos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    \"\"\"Esta clase extiende ObservationWrapper, lo cual\n",
    "    permite modificar las observaciones que entrega el\n",
    "    entorno sin alterar su din√°mica\"\"\"\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Constructor: recibe un entorno y llama al\n",
    "        constructor de la clase padre, y define manualmente\n",
    "        el nuevo espacio de observaci√≥n (primer valor:0-31,\n",
    "        segundo valor{carta del dealer}:0-10 y tercer valor\n",
    "        {tiene As usable o no}: 0 o 1)\n",
    "        \"\"\"\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Convierte la tupla de observaci√≥n a un array\n",
    "        de Numpy y lo aplana, es decir:\n",
    "        (21, 10, True)  ->  array([21, 10, 1])\n",
    "        \"\"\"\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Primero creamos y wrappeamos el ambiente\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack es un entorno de decisi√≥n secuencial modelado como un Proceso de Decisi√≥n de Markov (MDP), donde un jugador busca vencer al dealer obteniendo una suma de cartas lo m√°s cercana posible a 21, sin pasarse. Recordemos que el Blackjack bajo la formulaci√≥n de MDP cumple con la propiedad de Markov que dice que la probabilidad del siguiente estado y recompensa depende √∫nicamente del estado actual y la acci√≥n tomada, no del historial entero. \n",
    "\n",
    "- Estados (Observaciones): Cada estado est√° representado por una tupla de tres valores:\n",
    "    - player_sum: suma actual de las cartas del jugador (rango 4‚Äì21),\n",
    "    - dealer_showing: valor de la carta visible del dealer (1‚Äì10),\n",
    "    - usable_ace: indicador binario (1 si el jugador tiene un as usable, es decir, que puede contar como 11 sin pasar de 21, y 0 en caso contrario).\n",
    "\n",
    "- Acciones: El agente (jugador) puede elegir entre dos acciones:\n",
    "    - hit (1): pedir una carta adicional,\n",
    "    - stick (0): detenerse y pasar el turno al dealer.\n",
    "\n",
    "- Recompensas:\n",
    "    - Victoria: +1\n",
    "    - Derrota: -1\n",
    "    - Empate: 0\n",
    "    - Victoria con Blackjack natural (21 con dos cartas): +1.5 (si natural=True), o +1 si se ignora la bonificaci√≥n.\n",
    "\n",
    "- T√©rmino del episodio: El episodio finaliza si:\n",
    "    - el jugador hace hit y su mano supera 21 (bust)\n",
    "    - el jugador hace stick y se resuelve el juego contra el dealer.\n",
    "\n",
    "Din√°mica del juego:\n",
    "El jugador comienza con dos cartas visibles; el dealer con una visible y una oculta. Las cartas se extraen con reemplazo. Las figuras (J, Q, K) valen 10, los ases valen 1 u 11 (si no provocan bust), y el resto tiene su valor num√©rico.\n",
    "\n",
    "Si el jugador elige hit y su suma supera 21, pierde inmediatamente (bust). Si elige stick, el dealer revela su carta oculta y roba hasta alcanzar una suma de 17 o m√°s. Si el dealer se pasa, el jugador gana. Si ambos permanecen dentro del l√≠mite, gana quien tenga la suma m√°s alta; si empatan, el resultado es un empate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 5000 episodios: -0.3902\n",
      "Desviaci√≥n est√°ndar: 0.8955\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 5000\n",
    "n_movimientos_maximo = 100000#por simulaci√≥n\n",
    "\n",
    "for episodio in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env.action_space.sample()#acciones aleatorias\n",
    "        estado, recompensa, done, truncado, info = env.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)#vamos guardando las recompensas \n",
    "\n",
    "env.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en 5000 episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = [-1.5, -0.5, 0.5, 1.25, 1.75]#estos bins sirven para que se vean las recompensas\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(recompensas, bins=bins, edgecolor='black', rwidth=0.8)\n",
    "\n",
    "plt.xticks([-1, 0, 1, 1.5])\n",
    "plt.xlabel(\"Recompensa obtenida\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de recompensas\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?\n",
    "\n",
    "- El rendimiento de la pol√≠tica aleatoria es pobre, porque la mayor√≠a de las simulaciones muestran recompensas negativas. Ahora bien, esta pol√≠tica sirve como l√≠nea base o baseline para comparar agentes entrenados mediante aprendizaje por refuerzo o reinforcemente learning, cualquier pol√≠tica que mejore el promedio de -0.38 podr√° considerarse una mejora. \n",
    "\n",
    "¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?\n",
    "\n",
    "- Un promedio negativo de las recompensas para 5000 simulaciones/partidas y una cantidad m√°xima de 100 pasos/movimientos permitidos por partida, implica que las partidas en donde se pierde tomando aleatoriamente decisiones es mayor que aquellas en las que se gana. Es decir, en nuestro caso la cantidad de partidas perdidas es aproximadamente el doble que la cantidad de partidas ganadas. Por otro lado, podemos decir que el puntaje de las partidas empatadas no afecta en nada porque la recompensa de dichas partidas es nula, por lo que en el promedio de las recompensas nunca se ver√°n reflejadas las partidas empatadas o que quedan en `draw`. Esto adem√°s, se podr√≠a interpretar como que el mecanismo, aunque sea aleatorio, nos diese el resultado de un dado o moneda cargada, con la probabilidad dada por la ley de Laplace m√°s inclinada hacia el caso de partidas perdidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `Stable Baselines3 (SB3)` es una biblioteca de python que se usa para entrenar y evaluar agentes de aprendizaje por refuerzo (RL), y adem√°s es implementada sobre PyTorch (usada por lo dem√°s, para implementar modelos de RL que son bastantes cercanos al estado del arte). Por otro lado, esta ofrece una colecci√≥n de algoritmos RL listos para usar con soporte para entornos de Gymnasium. De la clase de aprendizaje reforzado, podemos tomar como ejemplo el alfortitmo DQN (tambi√©n conocido como [Deep Q-Network](https://markelsanz14.medium.com/introducci√≥n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)).\n",
    "\n",
    " En particular, seg√∫n dicha referencia, ese algoritmo combina el algoritmo Q-learning con redes neuronales profundas (que son fant√°sticas para aproximar funciones no lineales), que en realidad son dos redes neuronales, la primera de las cuales aproxima la funci√≥n Q(s, a, $\\theta$) (del estado `s` y acci√≥n `a` actuales) mientras que la segunda parametrizada por $\\theta'$ aproxima los valores Q pero del siguiente estado s' y acci√≥n a'. Aqu√≠ el aprendizaje ocurre en la red primera y no en la objetivo (la segunda), donde los parametros del aprendizaje de la primera se copian a la segunda transmitiendo el aprendizaje. Aqu√≠ queremos minimizar una funci√≥n de p√©rdida que es la resta de la ecuaci√≥n de bellman en DQN. \n",
    "\n",
    "Esta funci√≥n de p√©rdida se minimiza usando el algoritmo de descenso de gradientes. En este contexto, el agente no es m√°s que el objeto `model`, el cual usa el algoritmo DQN como su cerebro para aprender/tomar decisiones, porque por definici√≥n un agente es el componente que 1) observa el estado del entorno o ambiente, 2) toma decisiones, 3) recibe recompensas, 4) actualiza su pol√≠tica para mejorar su comportamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83131c99c17d4340985ef4f353634a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x34d9214f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente Blackjack (con 200 mil pasos temporales)\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, exportamos el modelo entrenado a un archivo .zip en pocas l√≠neas de c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model.save(\"dqn_blackjack\")\n",
    "del model\n",
    "model = DQN.load(\"dqn_blackjack\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.056, 0.9533435896884186)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model, model.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo es el performance de su agente?\n",
    "\n",
    "- El agente que usa DQN por detr√°s tiene un promedio de recompensas de -0.3, lo que es ligeramente mayor al caso del baseline que eleg√≠a aleatoriamente los movimientos por cada partida de blackjack (-0.053 que puede cambiar y -0.38 respectivamente).\n",
    "\n",
    "¬øEs mejor o peor que el escenario baseline?\n",
    "\n",
    "- El performance, como se dijo en la parte anterior, es mejor que el escenario baseline porque el agente entrenado aprende una pol√≠tica que maximiza la recompensa esperada, mientras que la pol√≠tica aleatoria no incorpora ninguna estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo vemos que clase de Python pertenecen los estados, usando el m√©todo .reset para el `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el tipo de clase de python del estado es:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "estado, info = env.reset()\n",
    "print(\"el tipo de clase de python del estado es: \", type(estado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def obtener_accion(modelo, estado):\n",
    "    \"\"\"Esta funci√≥n obtiene la acci√≥n que el modelo tomar√≠a \n",
    "    para cierto estado.\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo que se usar√° para predecir el movimiento del agente\n",
    "                (que como ya nos habremos dado cuenta, es el DQN).\n",
    "        estado: estado de la partida. \n",
    "\n",
    "    Returns:\n",
    "        accion.item(): la accion es un array o tensor, mientras que agregar \n",
    "                       .item() extrae el valor num√©rico simple. Aqu√≠\n",
    "                       deterministic=True indica que queremos la acci√≥n m√°s\n",
    "                       probable u √≥ptima, sin exploraci√≥n aleatoria. Por √∫ltimo,\n",
    "                       esta acci√≥n es un 1 o un 0, que representa la acci√≥n que \n",
    "                       tomar√° el agente (hit/stick respectivamente). \n",
    "    \"\"\"\n",
    "    accion, informacion = modelo.predict(estado, deterministic=True)\n",
    "    return accion.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que el estado es una 3-tupla de la forma (player_sum, dealer_card, usable_ace), entonces el jugador que tiene 6 (de suma), el dealer muestra 7 y el jugador no tiene un As usable se representar√° por la tupla (6, 7, False), mientras que el jugador que tiene 19, cuyo dealer muestra 3 y el jugador tiene un As usable tendr√° una tupla que lo representa que es (19, 3, True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): 1\n",
      "Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "estado2 = (19, 3, True)\n",
    "\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): {accion1}\")   # 0 = stick, 1 = hit\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): {accion2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acci√≥n que toma el agente para el primer estado es = 1, es decir, hit. En este contexto, el jugador tiene una suma baja (6), y el dealer muestra un 7, que es una carta fuerte, por lo que la estrategia b√°sica recomienda pedir carta (hit) para mejorar la mano, ya que quedarse con 6 casi seguro perder√≠a. \n",
    "\n",
    "Por otro lado, el jugador en el segundo estado tiene una mano fuerte (19), con un As usable que ayuda a no pasarse, mientras que el dealer muestra una carta d√©bil (3), por lo que podemos concluir que la estrategia b√°sica recomendar√≠a quedarse (stick) con un 19 para no arriesgar pasarse.\n",
    "\n",
    "Por lo tanto, ambas decisiones del agente entrenado por DQM son consistentes con las estrategias que una persona tomar√≠a en la vida real en el juego blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (8.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El par√°metro `Continuous=True` son acciones continuas, donde el agente puede elegir valores continuos reales para los motores, es decir, puede controlar la potencia con precisi√≥n variable (digamos, motor principal a potencia 40%, motor lateral derecho al 20%, etc.). Mientras que si ponemos `continuous=False` el agente elige entre un conjunto finito de acciones predefinidas (por ejemplo, encender motor principal, motor lateral izquierdo o derecho, o no hacer nada, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()#reseteamos el entorno\n",
    "    img = model.env.render()#obtenemos la imagen inicial \n",
    "    done = False#mientras no se termine el episodio\n",
    "    while not done:\n",
    "      images.append(img)#a√±adimos la imagen a la lista de imagenes\n",
    "      action, _ = model.predict(obs)#usamos el modelo para predecir la siguiente accion a tomar dado el estado obs\n",
    "      obs, reward, done, info = model.env.step(action)#aplicamos la accion anterior predicha para obtener recompensa, observaci√≥n, etc.\n",
    "      img = model.env.render(mode=\"rgb_array\")#por ultimo actualizamos la imagen del entorno con rgb_array\n",
    "\n",
    "  #ahora creamos y guardamos un archivo GIF, y solo se toman las imagenes\n",
    "  #con indice par (if i%2==0), para reducir la cantidad total de frames y el tama√±o \n",
    "  #final del gif, mientras que fps=29 define que el gif tendr√° 29 frames por segundo,\n",
    "  #para que se vea fluido\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac√°`:\n",
    "\n",
    "LunarLander es un entorno de simulaci√≥n donde el objetivo es aterrizar una nave espacial suavemente en una plataforma ubicada en (0, 0). Es un cl√°sico problema de optimizaci√≥n de trayectorias, que est√° inspirado en el principio del m√°ximo de Pontryagin. Para este existen dos versiones; 1) continuous=False (por defecto) indica acciones discretas, mientras que continuous=True indica acciones continuas.\n",
    "\n",
    "- Espacio de Acciones: \n",
    "    - Discreto `continuous=False`\n",
    "        - 0: no hacer nada\n",
    "        - 1: encender motor lateral izquierdo\n",
    "        - 2: encender motor principal\n",
    "        - 3: encender motor lateral derecho\n",
    "    - Continuo `continuous=True`: la acci√≥n es un vector [main, lateral] $\\epsilon$ [-1, 1]:\n",
    "        - main < 0: motor principal apagado\n",
    "        - 0 < main < 1: motor principal de 50% a 100% de potencia\n",
    "        - lateral < -0.5: motor izquierdo\n",
    "        - lateral > 0.5: motor derecho\n",
    "        - la potencia lateral tambi√©n escala de 50% a 100%\n",
    "\n",
    "- Espacio de Observaci√≥n: Es un vector de 8 dimensiones;\n",
    "    - x, y: posici√≥n\n",
    "    - vx, vy: velocidad lineal\n",
    "    - angle: √°ngulo\n",
    "    - angular_velocity: velocidad angular\n",
    "    - leg1_contact, leg2_contact: contacto de piernas en el suelo\n",
    "\n",
    "- Recompensas: \n",
    "    - son recompensas por paso:\n",
    "        - mejora por acercarse a la base, reducir la velocidad, o mantener horizontalidad\n",
    "        - se suman +10 puntos por cada pierna tocando el suelo\n",
    "        - se restan -0.03 por uso lateral, y -0.3 por uso del motor principal\n",
    "    - recompensa final:\n",
    "        - +100 si aterriza correctamente\n",
    "        - -100 si se estrella\n",
    "    - se considera un episodio exitoso como aquel cuya recompensa total es mayor o igual a 200.\n",
    "\n",
    "En comparaci√≥n con blackjack: en este ambiente LunarLander las acciones controlan motores para aterrizar suavemente, mientras que en blackjack las acciones controlan decisiones de juego con din√°mica estoc√°stica (es decir, aleatoria, al menos en el caso del baseline que ya hab√≠amos visto) y reglas fijas (es decir, las reglas del juego). Lunar Lander requiero modelos que manejen acciones continuas, mientras que Blackjack es ideal para introducir MDP's y enfoques de exploraci√≥n vs explotaci√≥n. Por otro lado, en este ambiente las decisiones son infinitas en cantidad (porque el tipo de acci√≥n es continua), mientras que en el caso del blackjack solo hay 2 posibles decisiones (Hit y Stick). El riesgo de tomar una acci√≥n tambi√©n es diferente, porque un mal movimiento puede causar ca√≠da o choque de una nave, mientras que en el primer caso no puede sino provocar una p√©rdida simb√≥lica de un juego. \n",
    "\n",
    "Ahora bien, el ambiente de Lunar Lander cumple con la propiedad de MDP (Markov Decision Process) porque el estado actual del mismo contiene toda la informaci√≥n necesaria para predecir el futuro (informaci√≥n codificada en `env.step()`), independientemente del pasado. Es decir, no hay dependencias ni variables ocultas que el agente necesite memorizar para pensar en el futuro movimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 10 episodios: -98.1122\n",
      "Desviaci√≥n est√°ndar: 84.7593\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 10\n",
    "n_movimientos_maximo = 100\n",
    "\n",
    "for episode in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env_lunar.reset()#reseteamos el estado del ambiente\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env_lunar.action_space.sample()#movimientos aleatorios\n",
    "        estado, recompensa, done, truncado, info = env_lunar.step(accion)\n",
    "        recompensa_total += recompensa    \n",
    "        #done -> se lleg√≥ a la meta / se fracas√≥, truncado -> el episodio\n",
    "        #termin√≥ por superar l√≠mite m√°ximo de pasos \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env_lunar.close()#cerramos el entorno\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en {n_simulaciones} episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el valor promedio tan negativo indica que el agente tiende mucho m√°s a fallar sistem√°ticamente que a aterrizar suavemente en su tarea, lo cual era de esperar si se tomaba una pol√≠tica (no informadas) sin conocimiento del entorno que depende del azar. Est√° cerca de -139 porque el castigo por estrellarse era -200. As√≠, explorando las decisiones al azar no obtenemos sino malos resultados. \n",
    "\n",
    "Por lo tanto, el desempe√±o de esta pol√≠tica es pobre, pero nos servir√° como una referencia m√≠nima para comparar futuras pol√≠ticas entrenadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos el modelo PPO que s√≠ es compatible con acciones continuas, como lo requiere nuestro caso. De la [siguiente referencia](https://www-geeksforgeeks-org.translate.goog/machine-learning/a-brief-introduction-to-proximal-policy-optimization/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc), podemos ver que, la Optimizaci√≥n Proximal de Pol√≠ticas (PPO) es un m√©todo que ayuda a un agente a mejorar sus acciones para obtener mejores recompensas. Al igual que otros m√©todos de gradiente de pol√≠ticas, modifica directamente la toma de decisiones del agente. Sin embargo, a diferencia de otros, la PPO a√±ade control adicional para evitar que estos cambios sean demasiado grandes. El objetivo principal de PPO es encontrar un equilibrio entre dos aspectos: 1) maximizar el objetivo (este es el n√∫cleo de la optimizaci√≥n de pol√≠ticas, donde la pol√≠tica del agente se ajusta para maximizar las recompensas esperadas) y 2) mantener las actualizaciones peque√±as (los grandes cambios pueden arruinar el aprendizaje, por lo que se debe ajustar con precisi√≥n la velocidad del aprendizaje).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3d8dea293b4249b69d6da15413aea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x116dc6e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "timesteps = 10000\n",
    "\n",
    "#ahora inicializamos el agente, interesantemente, MlpPolicy indica\n",
    "#que el agente utilizar√° una pol√≠tica parametrizada por una red neuronal \n",
    "#Multicapa o Multi Layer Perceptron, es decir una red feedforward simple \n",
    "#para tomar decisiones, mientras que verbose=0 indica que no se mostrar√°n \n",
    "#mensajes durante el entrenamiento.\n",
    "model_lunar = PPO(\"MlpPolicy\", env_lunar, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente LunarLander\n",
    "model_lunar.learn(total_timesteps=timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-197.62498416, 122.97973896477689)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lunar.save(\"PPO_LunarLander\")\n",
    "del model_lunar\n",
    "model_lunar = PPO.load(\"PPO_LunarLander\", env=env_lunar)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model_lunar, model_lunar.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es pero que el baseline (-120 vs -97) pero porque a√∫n no lo hemos optimizado, por lo que en la siguiente subsecci√≥n lo corregiremos, y porque adem√°s no hemos usado ning√∫n hiperpar√°metro como s√≠ lo hicimos en el caso de las decisiones aleatorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee67e709cf1421d8b2e90ff08ef47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con timesteps=2000000, learning_rate=0.0005, batch_size=32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(226.27098315217071, 88.30650412391931)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "pasostemporales_totales = 2000000\n",
    "\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) \n",
    "\n",
    "print(f\"Entrenando con timesteps={pasostemporales_totales}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env_lunar,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=0,\n",
    "            batch_size=batch_size)\n",
    "model.learn(total_timesteps=pasostemporales_totales, progress_bar=True)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env_lunar, n_eval_episodes=100)\n",
    "env_lunar.close()\n",
    "\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"agent_performance.gif\", width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el agente que usa PPO para optimizar el movimiento del cohete que aterriza en la luna, lo hace muy bien (con un promedio de recompensas de 159, lo que es muy cercano a 200, lo √≥ptimo). Por otro lado, podemos decir que el movimiento que realiza la astronave es casi vertical, tal y como se puede ver del gif que contiene cinco de las iteraciones/episodios/partidas. Los par√°metros que se utilizaron fueron: `learning_rate = 0.0005`,`batch_size = 32`, `pasostemporales_totales = 2000000`. Esta operaci√≥n o serie de dos millones de partidas se demor√≥ alrededor de 30 minutos, lo que indica que es bastante lento pero mas que nada porque necesita mucho entrenamiento. En general, el agente toma decisiones (que van en un espectro continuo de acciones) de manera precisa como si fuese un experto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usar los links para las api keys: https://huggingface.co/settings/tokens/new?tokenType=read, https://platform.openai.com/api-keys, https://aistudio.google.com/app/u/1/apikey y https://app.tavily.com/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5474,
     "status": "ok",
     "timestamp": 1731272598047,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 180
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "ruta_documentos = [\"paper_1.pdf\", \"paper_2.pdf\"]\n",
    "\n",
    "assert len(ruta_documentos) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in ruta_documentos)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, definimos nuestro modelo grande del lenguaje o Large Language Model (LLM), en donde usaremos gemini-1.5-flash, que pertenece a una familia de modelos multimodales desarrollados por Google DeepMind (Google AI). Este modelo fue dise√±ado para ofrecer un balance ideal entre velocidad, eficiencia, y capacidad para procesar largos contextos multimodales (texto, im√°genes, audio y video, etc) gracias a que su arquitectura es una mezcla de varios expertos. En particular, esta versi√≥n [1.5 de Gemini](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/?utm_source=chatgpt.com#gemini-model-updates) es √∫til para aplicaciones de alta frecuencia y gran volumen, como chatbots, an√°lisis de documentos, etc. (Como alternativa usamos GPT-3-turbo porque por alguna raz√≥n la api de gemini de google ten√≠a una cantidad m√°xima de \"consultas\" que se le pueden hacer, de alrededor de 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,#controla la aleatoriedad en la generaci√≥n de texto,\n",
    "                  #donde 0 indica que la salida ser√° determinista\n",
    "    max_tokens=None,#no se limita la cantidad m√°xima de tokens\n",
    "    timeout=None,#no hay l√≠mite de tiempo expl√≠cito para la respuesta\n",
    "    max_retries=2,#en caso de error o fallo en la conexi√≥n/generaci√≥n,\n",
    "                  #este par√°metro intentar√° 2 veces m√°s para obtener una \n",
    "                  #respuesta exitosa, quiz√°s este par√°metro es el que \n",
    "                  #m√°s tarde provoque que las consultas acaben r√°pido\n",
    "                  #la cantidad m√°xima de consultas que se le puede hacer \n",
    "                  #de manera gratuita.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construir_vectorstore_desde_pdfs(rutas_pdf: List[str],\n",
    "                                     modelo_embeddings: str = \"models/embedding-001\",\n",
    "                                     chunk_size: int = 1000,\n",
    "                                     chunk_overlap: int = 200) -> FAISS:\n",
    "    \"\"\"\n",
    "    Construye un vectorstore FAISS a partir de una lista de archivos PDF.\n",
    "\n",
    "    Par√°metros:\n",
    "    - rutas_pdf: lista de rutas a archivos PDF.\n",
    "    - modelo_embeddings: modelo de embeddings de Google Generative AI.\n",
    "    - chunk_size: tama√±o de cada chunk de texto.\n",
    "    - chunk_overlap: superposici√≥n entre chunks.\n",
    "\n",
    "    Retorna:\n",
    "    - vectorstore FAISS listo para b√∫squeda sem√°ntica.\n",
    "    \"\"\"\n",
    "    \n",
    "    def cargar_texto_pdf(ruta):\n",
    "        \"\"\"funci√≥n auxiliar que carga los documentos o papers\n",
    "        desde los cuales generar√° las respuestas m√°s tarde la LLM\"\"\"\n",
    "        lector = PdfReader(ruta)\n",
    "        documentos = []\n",
    "        for indice, pagina in enumerate(lector.pages):\n",
    "            texto = pagina.extract_text()\n",
    "            if texto:\n",
    "                documentos.append(Document(page_content=texto, metadata={\"source\": ruta, \"page\": indice}))\n",
    "        return documentos\n",
    "\n",
    "    # primero cargamos y unimos todos los documentos\n",
    "    documentos = []\n",
    "    for ruta in rutas_pdf:\n",
    "        documentos.extend(cargar_texto_pdf(ruta))\n",
    "\n",
    "    # separamos el texto en fragmentos, es decir, lo dividimos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap#indica la cantidad de palabras que se pueden \n",
    "                                   #repetir entre diferentes chunks, con solapamiento\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documentos)\n",
    "\n",
    "    # en tercer lugar ceamos los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=modelo_embeddings)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = construir_vectorstore_desde_pdfs(ruta_documentos)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3},\n",
    "                                     )\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Esta funci√≥n combina m√∫ltiples documentos o fragmentos de texto en un solo\n",
    "    bloque de teto, manteniendo una separaci√≥n visual clara\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_template = '''\n",
    "Eres un asistente experto en programaci√≥n e interpretaci√≥n de resultados de varios papers cient√≠ficos acerca de los Ingresos de Hoteles.\n",
    "Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci√≥n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Idioma: {language}\n",
    "Respuesta √∫til:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora habilitamos la soluci√≥n RAG, es decir, Retrieval Augmented Generation que es una t√©cnica para ampliar el conocimiento de nuestro modelo LLM con datos adicionales, que en nuestro caso son dos papers que nos servir√°n para un estudio de otro ramo. En el paso anterior realizamos la Indexaci√≥n (Load + Split + Embed + Store), es decir, creamos la representaci√≥n vectorial de nuestros datos con extensi√≥n .pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3},\n",
    "                                     )\n",
    "\n",
    "def construir_rag_chain(llm, retriever, function, rag_template, language=\"espa√±ol\"):\n",
    "    \"\"\"Esta funci√≥n construye el rag chain.\n",
    "\n",
    "    Argumentos:\n",
    "        llm: modelo grande del lenguaje que en nuestro caso puede ser un gemini-1.5-flash, algo de openai, etc.\n",
    "        retriever: se encarga de buscar los documentos m√°s relevantes dados como contexto para la pregunta del usuario \n",
    "        function: Una funci√≥n que transforma los documentos recuperados en texto, en nuestro caso para formatear los\n",
    "                  documentos \n",
    "        rag_template: cadena de texto que contiene el propmt con placeholders como {context}, {question}, {language}\n",
    "        language: lenguaje de la respuesta. Default como \"espa√±ol\".\n",
    "\n",
    "    Returns:\n",
    "        rag_chain\n",
    "    \"\"\"\n",
    "    retriever_chain = retriever | function\n",
    "\n",
    "    rag_prompt = PromptTemplate.from_template(rag_template)#objeto que recibe entradas (como \n",
    "                                                           #contexto y pregunta) y genera un\n",
    "                                                           #prompt (texto de entrada) para\n",
    "                                                           #el LLM\n",
    "\n",
    "    #AHORA UN pipeline funcional que conecta varios pasos mediante el operador | (pipe)\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever_chain, \n",
    "            \"question\": RunnablePassthrough(),#simplemente pasa la pregunta sin modificarla\n",
    "            \"language\": lambda _: language#funci√≥n an√≥nima que recibe cualquier entrada (_\n",
    "                                          #) y siempre retorna la variable language\n",
    "        }\n",
    "        | rag_prompt#pasa lo anterior (contexto, pregunta y lenguaje)\n",
    "        | llm#le pasa el llm el rag_prompt\n",
    "        | StrOutputParser()#ahora procesamos la salida para obtener un string limpio\n",
    "    )\n",
    "\n",
    "rag_chain = construir_rag_chain(llm, retriever, format_docs, rag_template, \"espa√±ol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define de diferentes maneras dependiendo del estudio consultado:\n",
      "\n",
      "* **Tran [55]:**  Utiliza un modelo de retraso distribuido autorregresivo y encuentra que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo para un hotel de lujo en EE. UU.\n",
      "\n",
      "* **Rossell√≥ et al. [58]:**  En un estudio sobre la demanda de servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos, encontraron valores del coeficiente que oscilan entre -0.51 y -4.\n",
      "\n",
      "* **Damonte et al. [59] y Canina y Calver [60]:**  Estos estudios indican que la elasticidad precio-demanda difiere seg√∫n el nivel de servicios y el tama√±o del destino, sugiriendo que una regi√≥n bien definida puede proporcionar mejores resultados que un enfoque agregado.  No se proporciona un valor num√©rico espec√≠fico para el coeficiente.\n",
      "\n",
      "* **Hiemstra e Ismail [61]:**  Este estudio considera la elasticidad de la demanda teniendo en cuenta varias variables y el impacto de los impuestos, pero no se proporciona un valor num√©rico espec√≠fico para el coeficiente de elasticidad precio-demanda.\n",
      "\n",
      "\n",
      "En resumen, no hay una √∫nica definici√≥n del coeficiente, sino que su valor var√≠a seg√∫n el m√©todo de c√°lculo, el tipo de hotel, la ubicaci√≥n geogr√°fica y el periodo de tiempo considerado.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan varias estrategias para optimizar las tarifas de habitaciones de hoteles:\n",
      "\n",
      "* **Modelos de precios din√°micos:**  Se mencionan papers (Aziz et al., 2011; Anderson & Xie, 2016) que abordan modelos de precios din√°micos para la gesti√≥n de ingresos en hoteles.  Estos modelos permiten ajustar los precios en funci√≥n de factores como la demanda y la disponibilidad.\n",
      "\n",
      "* **M√©todos de asignaci√≥n de habitaciones:**  Se describe la comparaci√≥n entre el m√©todo de red anidada y el m√©todo de precio de puja para la asignaci√≥n de habitaciones (Pimentel et al., [29]),  con el m√©todo de red anidada mostrando un mejor rendimiento.  Adem√°s, se menciona la optimizaci√≥n de la asignaci√≥n usando modelos deterministas y estoc√°sticos de demanda (Goldman et al., [30]; Weatherford [31]; De Boer et al. [32]),  en conjunto con pol√≠ticas de control de reservas (Pimentel et al., [29]).\n",
      "\n",
      "* **Optimizaci√≥n de la disponibilidad (niveles de sobreventa):** Pimentel et al. [28] desarrollaron un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda para segmentos de mercado espec√≠ficos, optimizando la disponibilidad y la asignaci√≥n a esos segmentos.\n",
      "\n",
      "* **Segmentaci√≥n del mercado:**  El estudio de Du et al. (2016) analiza la necesidad de cooperaci√≥n con terceros por parte de los proveedores de servicios que adoptan la segmentaci√≥n del mercado, lo que implica una estrategia de precios diferenciada seg√∫n el segmento.\n",
      "\n",
      "* **An√°lisis de la elasticidad precio de la demanda:** Petricek et al. (2020) identifican el comportamiento del consumidor bas√°ndose en la elasticidad precio, lo que puede informar la estrategia de precios.\n",
      "\n",
      "\n",
      "La informaci√≥n proporcionada no detalla completamente cada estrategia, pero s√≠ indica su uso en la optimizaci√≥n de ingresos de hoteles.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada menciona que la predicci√≥n de la demanda de habitaciones en hoteles se basa en datos hist√≥ricos de reservas, considerando dos variables relacionadas con el tiempo: la fecha de reserva y el tiempo hasta el consumo.  Se menciona la selecci√≥n de m√©todos como una consideraci√≥n importante, pero no se especifican m√©todos concretos utilizados.  Adem√°s, se indica que el pron√≥stico puede ser de noches de habitaci√≥n o llegadas, y puede agregarse a diferentes niveles (total, por categor√≠a de tarifa, por duraci√≥n de la estancia, o alguna combinaci√≥n).  Finalmente, se mencionan estudios que modelan la din√°mica de la demanda tur√≠stica (Rossell√≥ et al., 2005),  utilizan la elasticidad precio de la demanda para servicios de alojamiento (Damonte et al., 1998; Petricek et al., 2020), y emplean simulaci√≥n de Monte Carlo para pronosticar llegadas y ocupaci√≥n hotelera (Zakhary et al., 2021).  Sin embargo, no se describe en detalle qu√© m√©todos espec√≠ficos utilizan los hoteles para la predicci√≥n.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preguntas_respuestas = [\n",
    "    (\n",
    "        \"¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\",\n",
    "        \"El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\",\n",
    "        \"Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\",\n",
    "        \"Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "    print(f\"Pregunta: {pregunta}\")\n",
    "    print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "    respuesta_generada = rag_chain.invoke(pregunta)#invoke es un m√©todo que se usa para \n",
    "                                                   #invocar el pipeline con una entrada dada\n",
    "    print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tupla 1 de las preguntas-respuestas la respuesta no es la esperada en cuanto al coeficiente de elasticidad precio-demanda, pero es m√°s que nada porque la chain interpreta la pregunta como el coeficiente num√©rico que deriva de alg√∫n modelo particular relacionado con algunas referencias. En la tupla 2 de las preguntas-respuestas, la respuesta s√≠ coincide con lo esperado, principalmente porque se incluyen los tipos de estrategias que abordan la \"optimizaci√≥n\" de tarifas de habitaciones de hoteles seg√∫n precios hist√≥ricos. En la tupla 3, en efecto la respuesta calza de nuevo con lo esperado para predecir la demanda, pero no menciona los modelos espec√≠ficos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada describe c√≥mo se utiliza el coeficiente de elasticidad precio-demanda (E(pd)i) en modelos de optimizaci√≥n de precios para hoteles,  pero no define expl√≠citamente qu√© es.  Se menciona que se puede obtener a trav√©s de datos hist√≥ricos o mediante simulaci√≥n de Monte Carlo,  y que se asume que tiene una distribuci√≥n normal N(Œº, œÉ¬≤) en el modelo de programaci√≥n no lineal.  Sin embargo, no se ofrece una definici√≥n formal del coeficiente en s√≠ mismo.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan tres estrategias b√°sicas para la optimizaci√≥n de tarifas de habitaciones de hotel:\n",
      "\n",
      "1. **Estrategia neutral:**  Utiliza una funci√≥n lineal para calcular el margen de beneficio (Markup) basado en la ocupaci√≥n (MUp(Occ)), expresada como:  ùëÄùëàùëù(ùëÇùëêùëê) = (ùëé√óùê∏ùë•ùëÇùëêùëê+ùëÉùëöùëñùëõ)‚àíùëÉùëüùëíùëì, donde 'a' es un coeficiente, 'ExOcc' representa la ocupaci√≥n esperada, 'Pmin' es el precio m√≠nimo basado en un an√°lisis de costos, y 'Pref' es un precio de referencia (ej. precio BAR).  En esta estrategia, el margen de beneficio se relaciona linealmente con la ocupaci√≥n esperada.\n",
      "\n",
      "2. **Estrategia agresiva:**  Utiliza una funci√≥n exponencial para calcular el margen de beneficio.  La descripci√≥n no detalla la f√≥rmula espec√≠fica.\n",
      "\n",
      "3. **Estrategia conservadora:** Similar a la estrategia agresiva, utiliza una funci√≥n exponencial para calcular el margen de beneficio, pero con una f√≥rmula no especificada en el texto.\n",
      "\n",
      "\n",
      "Adicionalmente, se menciona que la optimizaci√≥n de precios considera la combinaci√≥n de ventana de reserva (tiempo entre la reserva y la estad√≠a), segmento de mercado y tipo de habitaci√≥n.  Se busca maximizar el ingreso total seleccionando el precio √≥ptimo para cada combinaci√≥n y la cantidad √≥ptima de habitaciones a asignar, sujeto a restricciones empresariales (como un conjunto de precios posibles determinados por la empresa).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se menciona el uso de  **bosques aleatorios (random forests)** como m√©todo para predecir la demanda diaria en un hotel.  Este m√©todo fue seleccionado por su superior rendimiento en comparaci√≥n con la regresi√≥n lineal, regresi√≥n cuadr√°tica y aproximaci√≥n spline, especialmente por su capacidad de manejar variables num√©ricas y categ√≥ricas.  Adicionalmente, se menciona el uso de **modelos de √°rboles de decisi√≥n**, tanto de clasificaci√≥n como de regresi√≥n, como componentes de los bosques aleatorios.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# configuraciones a probar {\"chunk_size\": 500, \"k\": 3, \"search_type\": \"similarity\"}, {\"chunk_size\": 1000, \"k\": 1, \"search_type\": \"similarity\"}, {\"chunk_size\": 2000, \"k\": 5, \"search_type\": \"mmr\"}\n",
    "\n",
    "def probar_configuraciones(chunk_size=2000, search_type=\"mmr\", search_kwargs={\"k\": 3}):\n",
    "    \"\"\"Esta funcion prueba diferentes configuraciones para hacer la rag_chain\n",
    "\n",
    "    Args:\n",
    "        chunk_size: Defaults to 2000.\n",
    "        search_type: Defaults to \"mmr\".\n",
    "        search_kwargs: Defaults to {\"k\": 3}.\n",
    "        \n",
    "    Output:\n",
    "        imprime las respuestas comparandolas entre s√≠ con las esperadas.\n",
    "    \"\"\"\n",
    "    vectorstore_1 = construir_vectorstore_desde_pdfs(rutas_pdf=ruta_documentos, chunk_size=chunk_size)\n",
    "\n",
    "    retriever_1 = vectorstore_1.as_retriever(search_type=search_type,\n",
    "                                        search_kwargs=search_kwargs\n",
    "                                        )\n",
    "    rag_chain_1 = construir_rag_chain(llm, retriever_1, format_docs, rag_template, \"espa√±ol\")\n",
    "\n",
    "    for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "        print(f\"Pregunta: {pregunta}\")\n",
    "        print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "        respuesta_generada = rag_chain_1.invoke(pregunta)\n",
    "        print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "probar_configuraciones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, las respuestas son m√°s apegadas (m√°s precisas) a las referencias y dice si expl√≠citamente se define dicho coeficiente de elasticidad o no, incluyendo el modelo por el cual podr√≠an llegar a calcularse. En cuanto a la segunda tupla, podemos decir que ahora da una respuesta m√°s cercana a un an√°lisis matem√°tico de cada una de las estrategias, mientras que el caso anterior era m√°s adecuado para esta pregunta. En el caso de la tercera tupla, podemos decir que ahora se enfoca m√°s en responder el tipo de modelo (como arboles de decisi√≥n o bien bosques aleatorios/ Random Forests). As√≠, podemos decir que este enfoque del RAG da respuestas m√°s enfocadas hacia el lado matem√°tico/funcional de las preguntas, quiz√°s debido a que usa un chunk_size mayor (mayor contexto para cada pregunta), y tambi√©n usa el search_type igual a \"mmr\", el cual fue seleccionado porque es una t√©cnica de recuperaci√≥n de documentos que buscar lograr un balance entre relevancia (selecciona los docs que tienen alta similitud con la consulta) y diversidad (selecciona los docs que son diferentes entre s√≠). Adem√°s, elegimos el parametro k=3 indica que se devuelven los 3 documentos m√°s relevantes. En general, deber√≠amos usar \"similarity\" cuando queremos una respuesta cerrada (como es nuestro caso), mientras que deber√≠amos usar \"mmr\" para respuestas abiertas o casos exploratorios. Por lo tanto, podemos decir que en efecto similarity es mejor en este caso que mmr, mientras que parece ser mejor la mayor cantidad posible de documentos, pero esto puede variar (porque a mayor cantidad de documentos mayor tiempo de ejecuci√≥n de cada consulta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**. El motor de b√∫squeda Tavily es un servicio dise√±ado para integrarse con modelos de lenguaje como ChatGPT o agentes RAG, y nos permite eventualmente proporcionar b√∫squedas web precisas, r√°pidas y tambi√©n enfocadas en fuentes confiables. Su principal funci√≥n es recuperar documentos relevantes desde la web que luego pueden ser utilizados como contexto para tareas como RAG, QA, y tambi√©n agentes aut√≥nomos, como lo veremos en esta secci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/2_xdwnnx5rqgtl8vh2vwkdmc0000gn/T/ipykernel_1364/1150375189.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search_tool = TavilySearchResults(max_results = 1)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_search_tool = TavilySearchResults(max_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "wikpedia_search_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "tools = [wikpedia_search_tool, tavily_search_tool] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") #Descargamos la plantilla de prompt REACT creada por hwchase17 desde el hub\n",
    "\n",
    "spanish_prompt_text = \"Responde siempre en espa√±ol.\\n\\n\" + react_prompt.template#agrego esto porque por default responde en ingl√©s\n",
    "\n",
    "prompt_es = PromptTemplate(\n",
    "    input_variables=react_prompt.input_variables,\n",
    "    template=spanish_prompt_text\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt_es)#creamos un agente REACT\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)#este es el ejecutor del agente, y maneja las llamadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito buscar informaci√≥n sobre algoritmos de optimizaci√≥n de precios en el contexto de reservas de hoteles.  Wikipedia podr√≠a tener informaci√≥n general sobre algoritmos de optimizaci√≥n, pero una b√∫squeda en un motor de b√∫squeda como tavily_search_results_json podr√≠a dar resultados m√°s espec√≠ficos y actuales sobre este problema particular.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"algoritmos optimizaci√≥n precios hoteles\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Algoritmos IA: Revolucionando la Gesti√≥n de Inventarios en Hoteles', 'url': 'https://metaversos.agency/blog/algoritmos-ia-revolucionando-la-gestion-de-inventarios-en-hoteles/', 'content': 'La **introducci√≥n de algoritmos** que optimizan la **disponibilidad y precios de las habitaciones** aporta numerosas ventajas al sector hotelero, abarcando desde el aumento de los ingresos hasta una mejora significativa de la relaci√≥n con el cliente. Estos **modelos matem√°ticos y soluciones de IA** inciden tanto en la maximizaci√≥n de la rentabilidad como en la competitividad del destino, impulsando un crecimiento sostenible y una gesti√≥n m√°s eficiente de los recursos [...] La **GESTI√ìN DE INVENTARIOS DE HOTELES** mediante algoritmos que **optimizan la disponibilidad y precios de las habitaciones en TIEMPO REAL** constituye un paso esencial en la evoluci√≥n de la industria hotelera. Al combinar la fortaleza de las t√©cnicas de **machine learning**, la estad√≠stica avanzada y la capacidad de integrar datos de canales m√∫ltiples, estos sistemas permiten reaccionar a la demanda con una agilidad ins√≥lita, incrementando los ingresos y ofreciendo a los clientes tarifas [...] La **GESTI√ìN DE INVENTARIOS DE HOTELES** contempor√°nea se cimienta en una serie de algoritmos dise√±ados para analizar la demanda y ajustar los precios y cupos de manera flexible. Estas soluciones, que frecuentemente recurren a t√©cnicas de **machine learning**, estad√≠sticas avanzadas y modelos de optimizaci√≥n, tienen la capacidad de procesar grandes vol√∫menes de datos relativos a la ocupaci√≥n, la competencia, los patrones de reserva y las fluctuaciones del mercado en **TIEMPO REAL**. El objetivo', 'score': 0.8660535}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci√≥n de tavily_search_results_json menciona algoritmos de IA y machine learning para la optimizaci√≥n de precios en hoteles, pero no especifica algoritmos concretos.  Para obtener nombres de algoritmos espec√≠ficos, usar√© Wikipedia.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"algoritmos de optimizaci√≥n\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Wikipedia no fue fruct√≠fera.  Sin embargo, la b√∫squeda en tavily_search_results_json s√≠ proporcion√≥ informaci√≥n relevante sobre la utilizaci√≥n de algoritmos de IA y machine learning para la optimizaci√≥n de precios en hoteles.  Aunque no nombra algoritmos espec√≠ficos, implica que se usan algoritmos de optimizaci√≥n avanzados, probablemente basados en modelos predictivos y aprendizaje autom√°tico.  No puedo dar nombres de algoritmos concretos sin m√°s informaci√≥n.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Existen algoritmos de computaci√≥n, basados en Inteligencia Artificial y Machine Learning, capaces de optimizar la tarifa por habitaci√≥n de un hotel para maximizar los ingresos a partir de una base de datos hist√≥rica de reservas.  Sin embargo, la informaci√≥n disponible no especifica los nombres de algoritmos concretos utilizados.  Se trata de algoritmos de optimizaci√≥n avanzados que probablemente utilizan modelos predictivos para analizar la demanda y ajustar los precios en tiempo real.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Existen algoritmos de computaci√≥n, basados en Inteligencia Artificial y Machine Learning, capaces de optimizar la tarifa por habitaci√≥n de un hotel para maximizar los ingresos a partir de una base de datos hist√≥rica de reservas.  Sin embargo, la informaci√≥n disponible no especifica los nombres de algoritmos concretos utilizados.  Se trata de algoritmos de optimizaci√≥n avanzados que probablemente utilizan modelos predictivos para analizar la demanda y ajustar los precios en tiempo real.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"¬øHay alg√∫n algoritmo de computaci√≥n que dada una base de datos\"\n",
    "                                  \"hist√≥rica de reservas de hotel con varias variables, pueda optimizar la\" \n",
    "                                  \"tarifa por habitaci√≥n del mismo, queriendo maximizar los ingresos?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante la prueba de una pregunta como la que se mostr√≥, podemos ver que primero el agente utiliz√≥ correctamente las herramientas que le dimos. Primero utiliz√≥ Tavily, donde encontr√≥ informaci√≥n actualizada sobre el uso de IA/Machine Learning en la industria hotelera. Despu√©s intent√≥ buscar en Wikipedia, pero no obtuvo resultados √∫tiles (No good Wikipedia Search Result was found). Por √∫ltimo, integr√≥ ambas b√∫squedas para generar una respuesta razonada, destacando que existen algoritmos de computaci√≥n basados en optimizaci√≥n y modelos predictivos para maximizar/optimizar ingresos, aunque no se especificaron nombres concretos. Lo cual demuestra un uso adecuado y complementario de las dos Tools seg√∫n el tipo de informaci√≥n que se quer√≠a tener.\n",
    "\n",
    "Por otro lado, creemos que el agente deber√≠a ocupar Tavily cuando la consulta requiere informaci√≥n actual o del presente, espec√≠fica o no estructurada, como lo son noticias recientes, avances tecnol√≥gicos (como estado del arte de un √°rea de investigaci√≥n), tendencias de la industria o temas emergentes, mientras que deber√≠a usar Wikipedia cuando se trata de consultas sobre conceptos generales, hist√≥ricos o bien establecidos/aceptados, como definiciones, teor√≠as cient√≠ficas, biograf√≠as, historias, eventos ya consolidados, etc. Esto es, para informaci√≥n estructurada y estable (que no cambia mucho) a lo largo del tiempo. Como nuestro caso es un problema de Machine Learning actual es muy posible que haya optado primero por Tavily precisamente porque es un problema cuya soluci√≥n puede ser estado del arte, que no necesariamente se conoce en Wikipedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool# Importamos primero que todo la clase Tool para crear herramientas reutilizables en el agente\n",
    "\n",
    "def rag_tool_func(question: str) -> str:\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"RAGTool\",\n",
    "    func=rag_tool_func,\n",
    "    description=\"Utiliza recuperaci√≥n aumentada con informaci√≥n para responder preguntas en espa√±ol sobre diversos temas, especialmente optimizaci√≥n de tarifas de hoteles.\"\n",
    ")\n",
    "\n",
    "def react_tool_func(question: str) -> str:\n",
    "    return agent_executor.invoke({\"input\": question})\n",
    "\n",
    "react_tool = Tool(\n",
    "    name=\"ReActAgentTool\",\n",
    "    func=react_tool_func,#funci√≥n que ejecuta esta herramienta\n",
    "    description=\"Agente ReAct que razona y ejecuta herramientas en espa√±ol para resolver tareas complejas.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/2_xdwnnx5rqgtl8vh2vwkdmc0000gn/T/ipykernel_1364/2296074785.py:6: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  supervisor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "tools = [rag_tool, react_tool]\n",
    "\n",
    "supervisor = initialize_agent(tools=tools,\n",
    "                              llm=llm,\n",
    "                              agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,#Elegimos el tipo de agente: uno que razona con herramientas \n",
    "                                                                          #sin entrenamiento previo y usando descripci√≥n de herramientas\n",
    "                              verbose=True#Activamos la impresi√≥n de mensajes detallados para ver qu√© hace el agente en tiempo real\n",
    ")\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito definir el coeficiente de elasticidad precio-demanda.  Creo que RAGTool puede proporcionar una definici√≥n concisa y precisa.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBasado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define de diferentes maneras dependiendo del estudio consultado:\n",
      "\n",
      "* **Tran [55]:**  Utiliza un modelo de retraso distribuido autorregresivo y encuentra que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo para un hotel de lujo en EE. UU.\n",
      "\n",
      "* **Rossell√≥ et al. [58]:**  En un estudio sobre la demanda de servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos, encontraron valores del coeficiente que oscilan entre -0.51 y -4.\n",
      "\n",
      "* **Damonte et al. [59] y Canina y Calver [60]:**  Estos estudios indican que la elasticidad precio-demanda difiere seg√∫n el nivel de servicios y el tama√±o del destino, sugiriendo que una regi√≥n bien definida puede proporcionar mejores resultados que un enfoque agregado.\n",
      "\n",
      "En resumen, no hay una √∫nica definici√≥n del coeficiente, sino que su valor var√≠a seg√∫n el m√©todo de c√°lculo, el tipo de alojamiento y el mercado analizado.  La informaci√≥n proporcionada muestra una amplia gama de valores, desde -0.02 hasta -4.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de RAGTool proporciona ejemplos de valores del coeficiente, pero no una definici√≥n general.  Necesito una definici√≥n conceptual.  Intentar√© reformular la pregunta para RAGTool.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: Define el coeficiente de elasticidad precio-demanda en t√©rminos econ√≥micos.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBasado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda mide la sensibilidad de la demanda de un bien o servicio (en este caso, servicios de alojamiento hotelero) ante variaciones en su precio.  Los estudios citados muestran que este coeficiente puede variar significativamente dependiendo de factores como el plazo (corto o largo plazo), el tipo de servicio, el tama√±o del destino y el m√©todo de c√°lculo empleado.  Se presentan valores que van desde -0.02 hasta -4, indicando una variabilidad considerable en la respuesta de la demanda a los cambios de precio.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de RAGTool es mejor, pero a√∫n no proporciona una definici√≥n formal.  Necesitar√≠a una definici√≥n m√°s precisa y concisa. Intentar√© usar ReActAgentTool para obtener una respuesta m√°s estructurada.\n",
      "\n",
      "Action: ReActAgentTool\n",
      "Action Input: Define el coeficiente de elasticidad precio-demanda.  Proporciona la f√≥rmula y explica qu√© significa cada variable.\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito definir el coeficiente de elasticidad precio-demanda, incluyendo su f√≥rmula y el significado de cada variable.  Wikipedia deber√≠a ser suficiente para esto, ya que es un concepto econ√≥mico est√°ndar.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Coeficiente de elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Intentar√© una b√∫squeda en Wikipedia con t√©rminos ligeramente diferentes.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Parece que Wikipedia no est√° funcionando correctamente. Intentar√© buscar la informaci√≥n directamente en internet usando una b√∫squeda gen√©rica.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: Definici√≥n de elasticidad precio de la demanda, f√≥rmula y variables\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Elasticidad Precio de la Demanda: Definici√≥n, Tipos y Ejemplos', 'url': 'https://economipedia.com/definiciones/elasticidad-precio-de-la-demanda.html', 'content': 'La elasticidad precio de la demanda es un concepto econ√≥mico que mide la sensibilidad de la cantidad demandada de un bien o servicio a los cambios en su precio.', 'score': 0.83549726}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: He encontrado un resultado en tavily_search_results_json que parece prometedor.  Acceder√© al enlace para obtener la informaci√≥n completa.  Como no puedo acceder a internet directamente, asumir√© que el enlace proporciona la definici√≥n, f√≥rmula y explicaci√≥n de las variables.  Basar√© mi respuesta en la descripci√≥n del resultado.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: La elasticidad precio de la demanda mide la sensibilidad de la cantidad demandada de un bien o servicio ante cambios en su precio.  Aunque no puedo proporcionar la f√≥rmula exacta sin acceder al enlace, puedo decir que la f√≥rmula generalmente involucra un cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.  Las variables ser√≠an:  la cantidad demandada inicial, la cantidad demandada final, el precio inicial y el precio final.  Un valor alto indica una demanda el√°stica (la cantidad demandada responde significativamente a los cambios de precio), mientras que un valor bajo indica una demanda inel√°stica (la cantidad demandada responde poco a los cambios de precio).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'input': 'Define el coeficiente de elasticidad precio-demanda.  Proporciona la f√≥rmula y explica qu√© significa cada variable.', 'output': 'La elasticidad precio de la demanda mide la sensibilidad de la cantidad demandada de un bien o servicio ante cambios en su precio.  Aunque no puedo proporcionar la f√≥rmula exacta sin acceder al enlace, puedo decir que la f√≥rmula generalmente involucra un cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.  Las variables ser√≠an:  la cantidad demandada inicial, la cantidad demandada final, el precio inicial y el precio final.  Un valor alto indica una demanda el√°stica (la cantidad demandada responde significativamente a los cambios de precio), mientras que un valor bajo indica una demanda inel√°stica (la cantidad demandada responde poco a los cambios de precio).'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de ReActAgentTool es una buena aproximaci√≥n, pero falta la f√≥rmula.  Aunque menciona que no puede acceder a enlaces, la f√≥rmula es bastante est√°ndar y deber√≠a poder proporcionarla.  Intentar√© reformular la solicitud a ReActAgentTool para que se enfoque en la f√≥rmula.\n",
      "\n",
      "Action: ReActAgentTool\n",
      "Action Input:  Proporciona la f√≥rmula del coeficiente de elasticidad precio-demanda y explica cada variable.\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito definir la f√≥rmula del coeficiente de elasticidad precio-demanda y explicar cada una de sus variables.  Wikipedia deber√≠a ser suficiente para esto, ya que es un concepto econ√≥mico bien establecido.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Intentar√© una b√∫squeda m√°s espec√≠fica en Wikipedia.  Probar√© con diferentes t√©rminos.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Coeficiente de elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Parece que Wikipedia no est√° siendo √∫til en este caso.  Intentar√© buscar la informaci√≥n directamente en Google utilizando un motor de b√∫squeda general.  No necesito usar las herramientas especificadas, ya que no me est√°n proporcionando la informaci√≥n necesaria.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: La f√≥rmula del coeficiente de elasticidad precio-demanda es:\n",
      "\n",
      "**Ed = (%ŒîQ / %ŒîP)**\n",
      "\n",
      "Donde:\n",
      "\n",
      "* **Ed:** representa el coeficiente de elasticidad precio de la demanda.  Indica la sensibilidad de la cantidad demandada ante un cambio en el precio.  Un valor absoluto de Ed > 1 indica una demanda el√°stica (la cantidad demandada responde de forma m√°s que proporcional a los cambios de precio), un valor absoluto de Ed < 1 indica una demanda inel√°stica (la cantidad demandada responde de forma menos que proporcional a los cambios de precio), y un valor absoluto de Ed = 1 indica una demanda unitaria (la cantidad demandada responde de forma proporcional a los cambios de precio).\n",
      "\n",
      "* **%ŒîQ:** representa el porcentaje de cambio en la cantidad demandada. Se calcula como [(Q2 - Q1) / Q1] * 100, donde Q1 es la cantidad demandada inicial y Q2 es la cantidad demandada despu√©s del cambio de precio.\n",
      "\n",
      "* **%ŒîP:** representa el porcentaje de cambio en el precio. Se calcula como [(P2 - P1) / P1] * 100, donde P1 es el precio inicial y P2 es el precio despu√©s del cambio.\n",
      "\n",
      "\n",
      "Es importante notar que el signo del coeficiente de elasticidad precio de la demanda siempre ser√° negativo, ya que el precio y la cantidad demandada tienen una relaci√≥n inversa (Ley de la Demanda).  Sin embargo, se suele utilizar el valor absoluto para la interpretaci√≥n de la elasticidad.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'input': 'Proporciona la f√≥rmula del coeficiente de elasticidad precio-demanda y explica cada variable.', 'output': 'La f√≥rmula del coeficiente de elasticidad precio-demanda es:\\n\\n**Ed = (%ŒîQ / %ŒîP)**\\n\\nDonde:\\n\\n* **Ed:** representa el coeficiente de elasticidad precio de la demanda.  Indica la sensibilidad de la cantidad demandada ante un cambio en el precio.  Un valor absoluto de Ed > 1 indica una demanda el√°stica (la cantidad demandada responde de forma m√°s que proporcional a los cambios de precio), un valor absoluto de Ed < 1 indica una demanda inel√°stica (la cantidad demandada responde de forma menos que proporcional a los cambios de precio), y un valor absoluto de Ed = 1 indica una demanda unitaria (la cantidad demandada responde de forma proporcional a los cambios de precio).\\n\\n* **%ŒîQ:** representa el porcentaje de cambio en la cantidad demandada. Se calcula como [(Q2 - Q1) / Q1] * 100, donde Q1 es la cantidad demandada inicial y Q2 es la cantidad demandada despu√©s del cambio de precio.\\n\\n* **%ŒîP:** representa el porcentaje de cambio en el precio. Se calcula como [(P2 - P1) / P1] * 100, donde P1 es el precio inicial y P2 es el precio despu√©s del cambio.\\n\\n\\nEs importante notar que el signo del coeficiente de elasticidad precio de la demanda siempre ser√° negativo, ya que el precio y la cantidad demandada tienen una relaci√≥n inversa (Ley de la Demanda).  Sin embargo, se suele utilizar el valor absoluto para la interpretaci√≥n de la elasticidad.'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  ReActAgentTool provided the formula and explanation.\n",
      "\n",
      "Final Answer: El coeficiente de elasticidad precio-demanda (Ed) se calcula con la f√≥rmula:  Ed = (%ŒîQ / %ŒîP), donde %ŒîQ es el porcentaje de cambio en la cantidad demandada [(Q2 - Q1) / Q1] * 100 y %ŒîP es el porcentaje de cambio en el precio [(P2 - P1) / P1] * 100.  Q1 y P1 son la cantidad y el precio iniciales, respectivamente, mientras que Q2 y P2 son la cantidad y el precio despu√©s del cambio.  Un valor absoluto de Ed > 1 indica una demanda el√°stica, Ed < 1 una demanda inel√°stica, y Ed = 1 una demanda unitaria.  El signo de Ed siempre es negativo debido a la ley de la demanda, pero se utiliza el valor absoluto para su interpretaci√≥n.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del agente supervisor: El coeficiente de elasticidad precio-demanda (Ed) se calcula con la f√≥rmula:  Ed = (%ŒîQ / %ŒîP), donde %ŒîQ es el porcentaje de cambio en la cantidad demandada [(Q2 - Q1) / Q1] * 100 y %ŒîP es el porcentaje de cambio en el precio [(P2 - P1) / P1] * 100.  Q1 y P1 son la cantidad y el precio iniciales, respectivamente, mientras que Q2 y P2 son la cantidad y el precio despu√©s del cambio.  Un valor absoluto de Ed > 1 indica una demanda el√°stica, Ed < 1 una demanda inel√°stica, y Ed = 1 una demanda unitaria.  El signo de Ed siempre es negativo debido a la ley de la demanda, pero se utiliza el valor absoluto para su interpretaci√≥n.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito informaci√≥n sobre estrategias de optimizaci√≥n de tarifas hoteleras.  La herramienta RAGTool parece la m√°s adecuada para obtener esta informaci√≥n directamente.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBasado en la informaci√≥n proporcionada, se mencionan varias estrategias para optimizar las tarifas de habitaciones de hoteles:\n",
      "\n",
      "* **Modelos de precios din√°micos:**  Se mencionan papers (Aziz et al., 2011; Anderson & Xie, 2016) que se centran en modelos de precios din√°micos para la gesti√≥n de ingresos de hoteles.  Estos modelos permiten ajustar los precios en funci√≥n de factores como la demanda y la disponibilidad.\n",
      "\n",
      "* **M√©todos de asignaci√≥n de habitaciones:**  Se describe la comparaci√≥n entre el m√©todo de red anidada y el m√©todo de precio de puja para la asignaci√≥n de habitaciones (Pimentel et al., [29]).  El m√©todo de red anidada mostr√≥ un mejor rendimiento.  Adem√°s, Goldman et al. [30]  analizaron la optimizaci√≥n de la asignaci√≥n utilizando modelos deterministas y estoc√°sticos de demanda, prob√°ndolos con pol√≠ticas de control de reservas como precios de puja y l√≠mites de reserva anidados.\n",
      "\n",
      "* **Optimizaci√≥n de la disponibilidad (niveles de sobreventa):** Pimentel et al. [28] desarrollaron un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda en segmentos de mercado espec√≠ficos, optimizando la disponibilidad y la asignaci√≥n a esos segmentos.\n",
      "\n",
      "* **Segmentaci√≥n del mercado:**  El paper de Du et al. (2016) explora la necesidad de cooperaci√≥n con terceros por parte de los proveedores de servicios que adoptan la segmentaci√≥n del mercado, lo que implica una estrategia de precios diferenciada seg√∫n el segmento.\n",
      "\n",
      "* **Consideraci√≥n de la elasticidad precio de la demanda:** Petricek et al. (2020) identifican el comportamiento del consumidor bas√°ndose en la elasticidad precio, lo que sugiere la importancia de comprender c√≥mo los cambios de precio afectan la demanda para optimizar las tarifas.\n",
      "\n",
      "\n",
      "La informaci√≥n proporcionada no detalla completamente cada estrategia, pero s√≠ indica las √°reas de investigaci√≥n y los m√©todos utilizados para optimizar las tarifas de habitaciones de hoteles.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de RAGTool proporciona una buena base, pero podr√≠a ser mejor organizada y m√°s accesible.  No necesito m√°s informaci√≥n, solo una mejor presentaci√≥n de la informaci√≥n ya obtenida.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: Las estrategias para optimizar las tarifas de habitaciones de hotel incluyen:\n",
      "\n",
      "* **Precios Din√°micos:** Ajustar los precios en tiempo real bas√°ndose en la demanda y la disponibilidad, utilizando modelos matem√°ticos complejos.\n",
      "\n",
      "* **Optimizaci√≥n de la Asignaci√≥n de Habitaciones:**  Utilizar algoritmos (como el m√©todo de red anidada) para asignar las habitaciones a los clientes de forma que se maximice el beneficio.  Esto incluye considerar la demanda y la posibilidad de sobreventa.\n",
      "\n",
      "* **Gesti√≥n de la Disponibilidad (Sobreventa):**  Determinar cuidadosamente cu√°ntas habitaciones reservar para cada segmento de mercado, considerando la demanda predicha y el riesgo de no ocupar todas las habitaciones.\n",
      "\n",
      "* **Segmentaci√≥n del Mercado:**  Ofrecer diferentes precios a diferentes grupos de clientes (ej. viajeros de negocios vs. turistas) bas√°ndose en su disposici√≥n a pagar.\n",
      "\n",
      "* **An√°lisis de la Elasticidad Precio de la Demanda:**  Comprender c√≥mo la demanda responde a los cambios de precio para establecer precios √≥ptimos.\n",
      "\n",
      "\n",
      "Estas estrategias a menudo se utilizan en conjunto para lograr la m√°xima optimizaci√≥n de ingresos.  La implementaci√≥n efectiva requiere an√°lisis de datos, modelado predictivo y una comprensi√≥n profunda del comportamiento del consumidor.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del agente supervisor: Las estrategias para optimizar las tarifas de habitaciones de hotel incluyen:\n",
      "\n",
      "* **Precios Din√°micos:** Ajustar los precios en tiempo real bas√°ndose en la demanda y la disponibilidad, utilizando modelos matem√°ticos complejos.\n",
      "\n",
      "* **Optimizaci√≥n de la Asignaci√≥n de Habitaciones:**  Utilizar algoritmos (como el m√©todo de red anidada) para asignar las habitaciones a los clientes de forma que se maximice el beneficio.  Esto incluye considerar la demanda y la posibilidad de sobreventa.\n",
      "\n",
      "* **Gesti√≥n de la Disponibilidad (Sobreventa):**  Determinar cuidadosamente cu√°ntas habitaciones reservar para cada segmento de mercado, considerando la demanda predicha y el riesgo de no ocupar todas las habitaciones.\n",
      "\n",
      "* **Segmentaci√≥n del Mercado:**  Ofrecer diferentes precios a diferentes grupos de clientes (ej. viajeros de negocios vs. turistas) bas√°ndose en su disposici√≥n a pagar.\n",
      "\n",
      "* **An√°lisis de la Elasticidad Precio de la Demanda:**  Comprender c√≥mo la demanda responde a los cambios de precio para establecer precios √≥ptimos.\n",
      "\n",
      "\n",
      "Estas estrategias a menudo se utilizan en conjunto para lograr la m√°xima optimizaci√≥n de ingresos.  La implementaci√≥n efectiva requiere an√°lisis de datos, modelado predictivo y una comprensi√≥n profunda del comportamiento del consumidor.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito informaci√≥n sobre los m√©todos de predicci√≥n de demanda hotelera.  La herramienta RAGTool parece adecuada para obtener esta informaci√≥n.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLa informaci√≥n proporcionada menciona que la predicci√≥n de la demanda de habitaciones en hoteles se basa en datos hist√≥ricos de reservas, considerando dos variables relacionadas con el tiempo: la fecha de reserva y el tiempo hasta el consumo.  Se menciona la selecci√≥n de m√©todos como una consideraci√≥n importante, pero no se especifican m√©todos concretos utilizados.  Adem√°s, se indica que el pron√≥stico puede ser de noches de habitaci√≥n o llegadas, y puede tener diferentes niveles de agregaci√≥n (total, por categor√≠a de tarifa, por duraci√≥n de la estancia, o alguna combinaci√≥n).  Finalmente, se mencionan estudios que modelan la demanda tur√≠stica (Rossell√≥ et al., 2005),  la demanda de alojamiento en hoteles urbanos (Canina y Carvell, 2005), y el uso de simulaci√≥n de Monte Carlo para pronosticar llegadas y ocupaci√≥n hotelera (Zakhary et al., 2021).  Sin embargo, no se detallan los m√©todos espec√≠ficos empleados en cada estudio.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de RAGTool proporciona informaci√≥n general pero no una lista exhaustiva de m√©todos.  Para obtener una lista m√°s completa, necesito una b√∫squeda m√°s espec√≠fica o usar ReActAgentTool para que investigue m√°s a fondo.  Intentar√© con ReActAgentTool para obtener una lista m√°s detallada de m√©todos.\n",
      "\n",
      "Action: ReActAgentTool\n",
      "Action Input:  Investiga y proporciona una lista de m√©todos que utilizan los hoteles para predecir la demanda, incluyendo ejemplos de t√©cnicas estad√≠sticas y de machine learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre los m√©todos que utilizan los hoteles para predecir la demanda, necesito informaci√≥n sobre t√©cnicas estad√≠sticas y de machine learning aplicadas a la predicci√≥n de la demanda hotelera.  Wikipedia podr√≠a proporcionar una visi√≥n general, pero es posible que Tavily ofrezca informaci√≥n m√°s actualizada y espec√≠fica sobre las t√©cnicas utilizadas en la industria hotelera.  Intentar√© primero con Wikipedia para obtener una base y luego, si es necesario, usar√© Tavily para complementar la informaci√≥n.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Predicci√≥n de la demanda hotelera\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Wikipedia no arroj√≥ resultados √∫tiles. Intentar√© con una b√∫squeda m√°s espec√≠fica en Tavily, enfoc√°ndome en las t√©cnicas estad√≠sticas y de machine learning.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"M√©todos de predicci√≥n de demanda hotelera, t√©cnicas estad√≠sticas y machine learning\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': '[PDF] T√≠tulo: Predicci√≥n de la demanda hotelera utilizando herramientas ...', 'url': 'https://dspace.uclv.edu.cu/bitstreams/46f016cb-1302-4fb5-aea9-4a647315ea7a/download', 'content': 'La regresi√≥n es una de las t√©cnicas estad√≠sticas m√°s usadas en la actualidad, la cual ha permitido el estudio de numerosos fen√≥menos en diversos', 'score': 0.64986813}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Tavily arroj√≥ un resultado que menciona la regresi√≥n como t√©cnica estad√≠stica.  Sin embargo, necesito m√°s informaci√≥n sobre otros m√©todos.  Intentar√© otras b√∫squedas en Tavily con t√©rminos m√°s espec√≠ficos.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"Predicci√≥n demanda hotelera machine learning t√©cnicas\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': '[PDF] T√≠tulo: Predicci√≥n de la demanda hotelera utilizando herramientas ...', 'url': 'https://dspace.uclv.edu.cu/bitstreams/46f016cb-1302-4fb5-aea9-4a647315ea7a/download', 'content': 'I. (2020). Aplicaci√≥n de la regresi√≥n polinomial para la caracterizaci√≥n de la curva del COVID-19, mediante t√©cnicas de machine learning.', 'score': 0.81630427}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Tavily sigue proporcionando resultados limitados, principalmente enfocados en un solo PDF.  Para obtener una lista m√°s completa de m√©todos, necesitar√© buscar informaci√≥n en otras fuentes.  Dado que no tengo acceso a otras fuentes, har√© lo mejor posible para sintetizar la informaci√≥n disponible y proporcionar una respuesta lo m√°s completa posible bas√°ndome en el conocimiento general sobre predicci√≥n de demanda y machine learning.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: Bas√°ndome en la informaci√≥n limitada y en el conocimiento general, los hoteles utilizan una variedad de m√©todos para predecir la demanda.  Estos incluyen t√©cnicas estad√≠sticas como la **regresi√≥n (lineal, polinomial, etc.)**, que analiza la relaci√≥n entre variables como la ocupaci√≥n hist√≥rica, precios, eventos locales y estacionalidad para predecir la demanda futura.  Tambi√©n se utilizan modelos de **series temporales**, que analizan patrones en datos hist√≥ricos para predecir tendencias futuras.\n",
      "\n",
      "En cuanto a las t√©cnicas de machine learning, se pueden aplicar algoritmos como las **redes neuronales**, que pueden identificar patrones complejos en grandes conjuntos de datos, incluyendo factores externos como el clima o eventos especiales.  Tambi√©n se utilizan algoritmos de **aprendizaje supervisado**, como las **m√°quinas de vectores de soporte (SVM)** o los **√°rboles de decisi√≥n**, para predecir la demanda bas√°ndose en datos hist√≥ricos etiquetados.  Finalmente, t√©cnicas de **clustering** pueden ayudar a segmentar a los clientes y predecir su comportamiento de reserva.\n",
      "\n",
      "Es importante destacar que la informaci√≥n obtenida de las b√∫squedas fue limitada.  Una investigaci√≥n m√°s exhaustiva utilizando otras bases de datos acad√©micas o informes de la industria hotelera proporcionar√≠a una lista m√°s completa y detallada de los m√©todos utilizados.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'input': 'Investiga y proporciona una lista de m√©todos que utilizan los hoteles para predecir la demanda, incluyendo ejemplos de t√©cnicas estad√≠sticas y de machine learning.', 'output': 'Bas√°ndome en la informaci√≥n limitada y en el conocimiento general, los hoteles utilizan una variedad de m√©todos para predecir la demanda.  Estos incluyen t√©cnicas estad√≠sticas como la **regresi√≥n (lineal, polinomial, etc.)**, que analiza la relaci√≥n entre variables como la ocupaci√≥n hist√≥rica, precios, eventos locales y estacionalidad para predecir la demanda futura.  Tambi√©n se utilizan modelos de **series temporales**, que analizan patrones en datos hist√≥ricos para predecir tendencias futuras.\\n\\nEn cuanto a las t√©cnicas de machine learning, se pueden aplicar algoritmos como las **redes neuronales**, que pueden identificar patrones complejos en grandes conjuntos de datos, incluyendo factores externos como el clima o eventos especiales.  Tambi√©n se utilizan algoritmos de **aprendizaje supervisado**, como las **m√°quinas de vectores de soporte (SVM)** o los **√°rboles de decisi√≥n**, para predecir la demanda bas√°ndose en datos hist√≥ricos etiquetados.  Finalmente, t√©cnicas de **clustering** pueden ayudar a segmentar a los clientes y predecir su comportamiento de reserva.\\n\\nEs importante destacar que la informaci√≥n obtenida de las b√∫squedas fue limitada.  Una investigaci√≥n m√°s exhaustiva utilizando otras bases de datos acad√©micas o informes de la industria hotelera proporcionar√≠a una lista m√°s completa y detallada de los m√©todos utilizados.'}\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 22\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPregunta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpregunta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRespuesta esperada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrespuesta_correcta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m respuesta_generada = \u001b[43msupervisor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRespuesta del agente supervisor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrespuesta_generada\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1328\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m   1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1354\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1351\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1353\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:800\u001b[39m, in \u001b[36mAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[32m    789\u001b[39m \n\u001b[32m    790\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m \u001b[33;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[32m    798\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    799\u001b[39m full_inputs = \u001b[38;5;28mself\u001b[39m.get_full_inputs(intermediate_steps, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m full_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_parser.parse(full_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1342\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1318\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     **kwargs: Any,\n\u001b[32m   1330\u001b[39m ) -> ChatResult:\n\u001b[32m   1331\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1332\u001b[39m         messages,\n\u001b[32m   1333\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1340\u001b[39m         tool_choice=tool_choice,\n\u001b[32m   1341\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:210\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    205\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:192\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 22\n}\n]"
     ]
    }
   ],
   "source": [
    "#replica pregunta 2.1.4\n",
    "\n",
    "for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "    print(f\"Pregunta: {pregunta}\")\n",
    "    print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "    respuesta_generada = supervisor.invoke({\"input\": pregunta})[\"output\"]\n",
    "    print(f\"Respuesta del agente supervisor: {respuesta_generada}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad lo que parece hacer el Agente Supervisor es ir revisando que la respuesta tenga sentido seg√∫n la pregunta, y de hecho parece ir modificando levemente el sentido de la pregunta para que sea m√°s f√°cil encontrar una respuesta adecuada a la misma. En este caso podemos ver que primero intenta usar la RAGtool dos veces y luego la ReactTool, pero como existe un n√∫mero m√°ximo de \"consultas\" que se pueden hacer usando estas APIs, se deduce que con el tiempo suficiente y con las credenciales necesarias, podr√≠a llegar a una respuesta mucho m√°s precisa y tambi√©n m√°s acorde a lo que se esperaba de la primera pregunta (¬øC√≥mo se define el coeficiente de precio elasticidad?): \"el coeficiente de elasticidad precio demanda describe el cambio porcentual del precio que experimentar√≠a al aumentar en un uno por ciento la demanda\", lo cual es una definici√≥n a nivel de econom√≠a.\n",
    "\n",
    "De hecho, intentamos usar GPT-turbo y otras variaciones de la API de google para reemplazar el large language model y no parece funcionar, por lo que preferimos dejar la respuesta visible, incluso con los errores. Por esto tambi√©n, no volvimos a correr el c√≥digo, porque de otra manera podr√≠a ser que la cantidad de consultas se acabaran y no lograsemos poner en el codigo los resultados de al menos las primeras consultas (antes del agente supervisor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "El router (Enrutador) es un √∫nico agente que elige una tool (o sub-agente) seg√∫n la entrada y tiene una l√≥gica expl√≠cita (por ejemplo, reglas o embeddings) para decidir a qu√© herramienta encaminar la pregunta.\n",
    "\n",
    "Por otro lado, el multiagente es un agente supervisor que tiene acceso a varias tools, donde cada tool encapsula una cadena o agente diferente (por ejemplo, un RAG y un ReAct). Este supervisor razona sobre la tarea y decide cu√°l usar en cada paso (o incluso usar m√°s de una si el framework lo permite), cuyas decisiones se basan en planificaci√≥n y razonamiento, t√≠picamente usando un prompt ReAct.\n",
    "\n",
    "El agente supervisor puede razonar antes de usar herramientas, combinar pasos y responder en forma m√°s natural ante preguntas complejas o ambiguas. No depende de reglas fijas o embebimientos preentrenados. Sin embargo, El supervisor puede llamar al LLM (en nuestro caso Gemini-1.5-flash) muchas veces durante su razonamiento. Esto lo hace m√°s lento, m√°s caro y m√°s propenso a errores como ResourceExhausted (es decir, que se queda sin recursos de consultas, porque hay un l√≠mite de la cantidad de consultas gratuitas), como ya lo dijimos en la secci√≥n anterior. Dicho de otra manera, es m√°s costoso que usar simplemente CHAINS, porque en general usar un agente es m√°s costoso que usar una CHAIN, por el tiempo y por el dinero (con el cual se podr√≠an hacer m√°s consultas por m√°s tiempo y obtener mejores resultados). Pero a la vez es m√°s flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://471fdc29f8060b37bb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://471fdc29f8060b37bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 854, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/chat_interface.py\", line 537, in _wrapper\n",
      "    async for chunk in submit_fn(*args, **kwargs):\n",
      "        yield chunk\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/chat_interface.py\", line 951, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 743, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        run_sync_iterator_async, self.iterator, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 726, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/var/folders/09/2_xdwnnx5rqgtl8vh2vwkdmc0000gn/T/ipykernel_3438/3589718755.py\", line 9, in agent_response\n",
      "    response = supervisor.invoke({\"input\": message})\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1620, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "        name_to_tool_map,\n",
      "    ...<3 lines>...\n",
      "        run_manager=run_manager,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1328, in _take_next_step\n",
      "    for a in self._iter_next_step(\n",
      "             ~~~~~~~~~~~~~~~~~~~~^\n",
      "        name_to_tool_map,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        run_manager,\n",
      "        ^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1354, in _iter_next_step\n",
      "    output = self._action_agent.plan(\n",
      "        intermediate_steps,\n",
      "        callbacks=run_manager.get_child() if run_manager else None,\n",
      "        **inputs,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 800, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 319, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **self.llm_kwargs,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1342, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ~~~~~~~~~~~~~~~~^\n",
      "        request=request,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        metadata=self.default_metadata,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 210, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 208, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 192, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "        request,\n",
      "    ...<2 lines>...\n",
      "        metadata=metadata,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "        target,\n",
      "    ...<3 lines>...\n",
      "        on_error=on_error,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "        exc,\n",
      "    ...<6 lines>...\n",
      "        timeout,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  response = supervisor.invoke({\"input\": message})\n",
    "\n",
    "  assert type(response) == str, \"response de route_question debe ser string\"\n",
    "\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "√âxito!\n",
    "<center>\n",
    "<img src =\"https://media.tenor.com/MRQgxcelAV8AAAAM/perry-the-platypus-phineas-and-ferb.gif\" width = 400 />"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNs28AeL6L8BlEf3067k5qg",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
