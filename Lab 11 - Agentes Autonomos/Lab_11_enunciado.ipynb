{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: Pienso, luego predigo üí°**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Juan Mi√±o\n",
    "- Nombre de alumno 2: Diego Espinoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/...../)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack es un entorno de decisi√≥n secuencial modelado como un Proceso de Decisi√≥n de Markov (MDP), donde un jugador busca vencer al dealer obteniendo una suma de cartas lo m√°s cercana posible a 21, sin pasarse. Recordemos que el Blackjack bajo la formulaci√≥n de MDP cumple con la propiedad de Markov que dice que la probabilidad del siguiente estado y recompensa depende √∫nicamente del estado actual y la acci√≥n tomada, no del historial entero. \n",
    "\n",
    "- Estados (Observaciones): Cada estado est√° representado por una tupla de tres valores:\n",
    "    - player_sum: suma actual de las cartas del jugador (rango 4‚Äì21),\n",
    "    - dealer_showing: valor de la carta visible del dealer (1‚Äì10),\n",
    "    - usable_ace: indicador binario (1 si el jugador tiene un as usable, es decir, que puede contar como 11 sin pasar de 21, y 0 en caso contrario).\n",
    "\n",
    "- Acciones: El agente (jugador) puede elegir entre dos acciones:\n",
    "    - hit (1): pedir una carta adicional,\n",
    "    - stick (0): detenerse y pasar el turno al dealer.\n",
    "\n",
    "- Recompensas:\n",
    "    - Victoria: +1\n",
    "    - Derrota: -1\n",
    "    - Empate: 0\n",
    "    - Victoria con Blackjack natural (21 con dos cartas): +1.5 (si natural=True), o +1 si se ignora la bonificaci√≥n.\n",
    "\n",
    "- T√©rmino del episodio: El episodio finaliza si:\n",
    "    - el jugador hace hit y su mano supera 21 (bust)\n",
    "    - el jugador hace stick y se resuelve el juego contra el dealer.\n",
    "\n",
    "Din√°mica del juego:\n",
    "El jugador comienza con dos cartas visibles; el dealer con una visible y una oculta. Las cartas se extraen con reemplazo. Las figuras (J, Q, K) valen 10, los ases valen 1 u 11 (si no provocan bust), y el resto tiene su valor num√©rico.\n",
    "\n",
    "Si el jugador elige hit y su suma supera 21, pierde inmediatamente (bust). Si elige stick, el dealer revela su carta oculta y roba hasta alcanzar una suma de 17 o m√°s. Si el dealer se pasa, el jugador gana. Si ambos permanecen dentro del l√≠mite, gana quien tenga la suma m√°s alta; si empatan, el resultado es un empate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 5000 episodios: -0.4020\n",
      "Desviaci√≥n est√°ndar: 0.8951\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 5000\n",
    "n_movimientos_maximo = 100000\n",
    "\n",
    "for episodio in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en 5000 episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = [-1.5, -0.5, 0.5, 1.25, 1.75]#estos bins sirven para que se vean las recompensas\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(recompensas, bins=bins, edgecolor='black', rwidth=0.8)\n",
    "\n",
    "plt.xticks([-1, 0, 1, 1.5])\n",
    "plt.xlabel(\"Recompensa obtenida\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de recompensas\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?\n",
    "\n",
    "- El rendimiento de la pol√≠tica aleatoria es pobre, porque la mayor√≠a de las simulaciones muestran recompensas negativas. Ahora bien, esta pol√≠tica sirve como l√≠nea base o baseline para comparar agentes entrenados mediante aprendizaje por refuerzo o reinforcemente learning, cualquier pol√≠tica que mejore el promedio de -0.38 podr√° considerarse una mejora. \n",
    "\n",
    "¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?\n",
    "\n",
    "- Un promedio negativo de las recompensas para 5000 simulaciones/partidas y una cantidad m√°xima de 100 pasos/movimientos permitidos por partida, implica que las partidas en donde se pierde tomando aleatoriamente decisiones es mayor que aquellas en las que se gana. Es decir, en nuestro caso la cantidad de partidas perdidas es aproximadamente el doble que la cantidad de partidas ganadas. Por otro lado, podemos decir que el puntaje de las partidas empatadas no afecta en nada porque la recompensa de dichas partidas es nula, por lo que en el promedio de las recompensas nunca se ver√°n reflejadas las partidas empatadas o que quedan en `draw`. Esto adem√°s, se podr√≠a interpretar como que el mecanismo, aunque sea aleatorio, nos diese el resultado de un dado o moneda cargada, con la probabilidad dada por la ley de Laplace m√°s inclinada hacia el caso de partidas perdidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `Stable Baselines3 (SB3)` es una biblioteca de python que se usa para entrenar y evaluar agentes de aprendizaje por refuerzo (RL), y adem√°s es implementada sobre PyTorch (usada por lo dem√°s, para implementar modelos de RL que son bastantes cercanos al estado del arte). Por otro lado, esta ofrece una colecci√≥n de algoritmos RL listos para usar con soporte para entornos de Gymnasium. De la clase de aprendizaje reforzado, podemos tomar como ejemplo el alfortitmo DQN (tambi√©n conocido como [Deep Q-Network](https://markelsanz14.medium.com/introducci√≥n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)).\n",
    "\n",
    " En particular, seg√∫n dicha referencia, ese algoritmo combina el algoritmo Q-learning con redes neuronales profundas (que son fant√°sticas para aproximar funciones no lineales), que en realidad son dos redes neuronales, la primera de las cuales aproxima la funci√≥n Q(s, a, $\\theta$) (del estado `s` y acci√≥n `a` actuales) mientras que la segunda parametrizada por $\\theta'$ aproxima los valores Q pero del siguiente estado s' y acci√≥n a'. Aqu√≠ el aprendizaje ocurre en la red primera y no en la objetivo (la segunda), donde los parametros del aprendizaje de la primera se copian a la segunda transmitiendo el aprendizaje. Aqu√≠ queremos minimizar una funci√≥n de p√©rdida que es la resta de la ecuaci√≥n de bellman en DQN. \n",
    "\n",
    "Esta funci√≥n de p√©rdida se minimiza usando el algoritmo de descenso de gradientes. En este contexto, el agente no es m√°s que el objeto `model`, el cual usa el algoritmo DQN como su cerebro para aprender/tomar decisiones, porque por definici√≥n un agente es el componente que 1) observa el estado del entorno o ambiente, 2) toma decisiones, 3) recibe recompensas, 4) actualiza su pol√≠tica para mejorar su comportamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e605bd84d44429aae88d5e1e38f221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1055da340>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente Blackjack\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, exportamos el modelo entrenado a un archivo .zip en pocas l√≠neas de c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model.save(\"dqn_blackjack\")\n",
    "del model\n",
    "model = DQN.load(\"dqn_blackjack\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.114, 0.9460465104845533)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model, model.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo es el performance de su agente?\n",
    "\n",
    "- El agente que usa DQN por detr√°s tiene un promedio de recompensas de -0.3, lo que es ligeramente mayor al caso del baseline que eleg√≠a aleatoriamente los movimientos por cada partida de blackjack (-0.053 que puede cambiar y -0.38 respectivamente).\n",
    "\n",
    "¬øEs mejor o peor que el escenario baseline?\n",
    "\n",
    "- El performance, como se dijo en la parte anterior, es mejor que el escenario baseline porque el agente entrenado aprende una pol√≠tica que maximiza la recompensa esperada, mientras que la pol√≠tica aleatoria no incorpora ninguna estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo vemos que clase de Python pertenecen los estados, usando el m√©todo .reset para el `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el tipo de clase de python del estado es:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "estado, info = env.reset()\n",
    "print(\"el tipo de clase de python del estado es: \", type(estado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def obtener_accion(modelo, estado):\n",
    "    \"\"\"Esta funci√≥n obtiene la acci√≥n que el modelo tomar√≠a \n",
    "    para cierto estado.\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo que se usar√° para predecir el movimiento del agente\n",
    "                (que como ya nos habremos dado cuenta, es el DQN).\n",
    "        estado: estado de la partida. \n",
    "\n",
    "    Returns:\n",
    "        accion.item(): la accion es un array o tensor, mientras que agregar \n",
    "                       .item() extrae el valor num√©rico simple. Aqu√≠\n",
    "                       deterministic=True indica que queremos la acci√≥n m√°s\n",
    "                       probable u √≥ptima, sin exploraci√≥n aleatoria. Por √∫ltimo,\n",
    "                       esta acci√≥n es un 1 o un 0, que representa la acci√≥n que \n",
    "                       tomar√° el agente (hit/stick respectivamente). \n",
    "    \"\"\"\n",
    "    accion, informacion = modelo.predict(estado, deterministic=True)\n",
    "    return accion.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que el estado es una 3-tupla de la forma (player_sum, dealer_card, usable_ace), entonces el jugador que tiene 6 (de suma), el dealer muestra 7 y el jugador no tiene un As usable se representar√° por la tupla (6, 7, False), mientras que el jugador que tiene 19, cuyo dealer muestra 3 y el jugador tiene un As usable tendr√° una tupla que lo representa que es (19, 3, True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): 1\n",
      "Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "estado2 = (19, 3, True)\n",
    "\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): {accion1}\")   # 0 = stick, 1 = hit\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): {accion2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acci√≥n que toma el agente para el primer estado es = 1, es decir, hit. En este contexto, el jugador tiene una suma baja (6), y el dealer muestra un 7, que es una carta fuerte, por lo que la estrategia b√°sica recomienda pedir carta (hit) para mejorar la mano, ya que quedarse con 6 casi seguro perder√≠a. \n",
    "\n",
    "Por otro lado, el jugador en el segundo estado tiene una mano fuerte (19), con un As usable que ayuda a no pasarse, mientras que el dealer muestra una carta d√©bil (3), por lo que podemos concluir que la estrategia b√°sica recomendar√≠a quedarse (stick) con un 19 para no arriesgar pasarse.\n",
    "\n",
    "Por lo tanto, ambas decisiones del agente entrenado por DQM son consistentes con las estrategias que una persona tomar√≠a en la vida real en el juego blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (8.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El par√°metro `Continuous=True` son acciones continuas, donde el agente puede elegir valores continuos reales para los motores, es decir, puede controlar la potencia con precisi√≥n variable (digamos, motor principal a potencia 40%, motor lateral derecho al 20%, etc.). Mientras que si ponemos `continuous=False` el agente elige entre un conjunto finito de acciones predefinidas (por ejemplo, encender motor principal, motor lateral izquierdo o derecho, o no hacer nada, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac√°`:\n",
    "\n",
    "LunarLander es un entorno de simulaci√≥n donde el objetivo es aterrizar una nave espacial suavemente en una plataforma ubicada en (0, 0). Es un cl√°sico problema de optimizaci√≥n de trayectorias, que est√° inspirado en el principio del m√°ximo de Pontryagin. Para este existen dos versiones; 1) continuous=False (por defecto) indica acciones discretas, mientras que continuous=True indica acciones continuas.\n",
    "\n",
    "- Espacio de Acciones: \n",
    "    - Discreto `continuous=False`\n",
    "        - 0: no hacer nada\n",
    "        - 1: encender motor lateral izquierdo\n",
    "        - 2: encender motor principal\n",
    "        - 3: encender motor lateral derecho\n",
    "    - Continuo `continuous=True`: la acci√≥n es un vector [main, lateral] $\\epsilon$ [-1, 1]:\n",
    "        - main < 0: motor principal apagado\n",
    "        - 0 < main < 1: motor principal de 50% a 100% de potencia\n",
    "        - lateral < -0.5: motor izquierdo\n",
    "        - lateral > 0.5: motor derecho\n",
    "        - la potencia lateral tambi√©n escala de 50% a 100%\n",
    "\n",
    "- Espacio de Observaci√≥n: Es un vector de 8 dimensiones;\n",
    "    - x, y: posici√≥n\n",
    "    - vx, vy: velocidad lineal\n",
    "    - angle: √°ngulo\n",
    "    - angular_velocity: velocidad angular\n",
    "    - leg1_contact, leg2_contact: contacto de piernas en el suelo\n",
    "\n",
    "- Recompensas: \n",
    "    - son recompensas por paso:\n",
    "        - mejora por acercarse a la base, reducir la velocidad, o mantener horizontalidad\n",
    "        - se suman +10 puntos por cada pierna tocando el suelo\n",
    "        - se restan -0.03 por uso lateral, y -0.3 por uso del motor principal\n",
    "    - recompensa final:\n",
    "        - +100 si aterriza correctamente\n",
    "        - -100 si se estrella\n",
    "    - se considera un episodio exitoso como aquel cuya recompensa total es mayor o igual a 200.\n",
    "\n",
    "En comparaci√≥n con blackjack: en este ambiente LunarLander las acciones controlan motores para aterrizar suavemente, mientras que en blackjack las acciones controlan decisiones de juego con din√°mica estoc√°stica (es decir, aleatoria, al menos en el caso del baseline que ya hab√≠amos visto) y reglas fijas (es decir, las reglas del juego). Lunar Lander requiero modelos que manejen acciones continuas, mientras que Blackjack es ideal para introducir MDP's y enfoques de exploraci√≥n vs explotaci√≥n. Por otro lado, en este ambiente las decisiones son infinitas en cantidad (porque el tipo de acci√≥n es continua), mientras que en el caso del blackjack solo hay 2 posibles decisiones (Hit y Stick). El riesgo de tomar una acci√≥n tambi√©n es diferente, porque un mal movimiento puede causar ca√≠da o choque de una nave, mientras que en el primer caso no puede sino provocar una p√©rdida simb√≥lica de un juego. \n",
    "\n",
    "Ahora bien, el ambiente de Lunar Lander cumple con la propiedad de MDP (Markov Decision Process) porque el estado actual del mismo contiene toda la informaci√≥n necesaria para predecir el futuro (informaci√≥n codificada en `env.step()`), independientemente del pasado. Es decir, no hay dependencias ni variables ocultas que el agente necesite memorizar para pensar en el futuro movimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 10 episodios: -97.9389\n",
      "Desviaci√≥n est√°ndar: 73.3275\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 10\n",
    "n_movimientos_maximo = 100\n",
    "\n",
    "for episode in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env_lunar.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env_lunar.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env_lunar.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env_lunar.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en {n_simulaciones} episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el valor promedio tan negativo indica que el agente tiende mucho m√°s a fallar sistem√°ticamente que a aterrizar suavemente en su tarea, lo cual era de esperar si se tomaba una pol√≠tica (no informadas) sin conocimiento del entorno que depende del azar. Est√° cerca de -139 porque el castigo por estrellarse era -200. As√≠, explorando las decisiones al azar no obtenemos sino malos resultados. \n",
    "\n",
    "Por lo tanto, el desempe√±o de esta pol√≠tica es pobre, pero nos servir√° como una referencia m√≠nima para comparar futuras pol√≠ticas entrenadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos el modelo PPO que s√≠ es compatible con acciones continuas, como lo requiere nuestro caso. De la [siguiente referencia](https://www-geeksforgeeks-org.translate.goog/machine-learning/a-brief-introduction-to-proximal-policy-optimization/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc), podemos ver que, la Optimizaci√≥n Proximal de Pol√≠ticas (PPO) es un m√©todo que ayuda a un agente a mejorar sus acciones para obtener mejores recompensas. Al igual que otros m√©todos de gradiente de pol√≠ticas, modifica directamente la toma de decisiones del agente. Sin embargo, a diferencia de otros, la PPO a√±ade control adicional para evitar que estos cambios sean demasiado grandes. El objetivo principal de PPO es encontrar un equilibrio entre dos aspectos: 1) maximizar el objetivo (este es el n√∫cleo de la optimizaci√≥n de pol√≠ticas, donde la pol√≠tica del agente se ajusta para maximizar las recompensas esperadas) y 2) mantener las actualizaciones peque√±as (los grandes cambios pueden arruinar el aprendizaje, por lo que se debe ajustar con precisi√≥n la velocidad del aprendizaje).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12db5010f62148dc81e591a827d962e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x368274f10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "timesteps = 10000\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model_lunar = PPO(\"MlpPolicy\", env_lunar, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente LunarLander\n",
    "model_lunar.learn(total_timesteps=timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-120.399338565, 47.07241200895624)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lunar.save(\"PPO_LunarLander\")\n",
    "del model_lunar\n",
    "model_lunar = PPO.load(\"PPO_LunarLander\", env=env_lunar)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model_lunar, model_lunar.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es pero que el baseline (-120 vs -97) pero porque a√∫n no lo hemos optimizado, por lo que en la siguiente subsecci√≥n lo corregiremos, y porque adem√°s no hemos usado ning√∫n hiperpar√°metro como s√≠ lo hicimos en el caso de las decisiones aleatorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094adb95bc884ab28bc7b7606a491216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con timesteps=2000000, learning_rate=0.0005, batch_size=32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159.51521190761963, 112.16941103430986)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "pasostemporales_totales = 2000000\n",
    "\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) \n",
    "\n",
    "print(f\"Entrenando con timesteps={pasostemporales_totales}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env_lunar,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=0,\n",
    "            batch_size=batch_size)\n",
    "model.learn(total_timesteps=pasostemporales_totales, progress_bar=True)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env_lunar, n_eval_episodes=100)\n",
    "env_lunar.close()\n",
    "\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"agent_performance.gif\", width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el agente que usa PPO para optimizar el movimiento del cohete que aterriza en la luna, lo hace muy bien (con un promedio de recompensas de 159, lo que es muy cercano a 200, lo √≥ptimo). Por otro lado, podemos decir que el movimiento que realiza la astronave es casi vertical, tal y como se puede ver del gif que contiene cinco de las iteraciones/episodios/partidas. Los par√°metros que se utilizaron fueron: `learning_rate = 0.0005`,`batch_size = 32`, `pasostemporales_totales = 2000000`. Esta operaci√≥n o serie de dos millones de partidas se demor√≥ alrededor de 30 minutos, lo que indica que es bastante lento pero mas que nada porque necesita mucho entrenamiento. En general, el agente toma decisiones (que van en un espectro continuo de acciones) de manera precisa como si fuese un experto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5474,
     "status": "ok",
     "timestamp": 1731272598047,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 180
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Deben adjuntar un m√≠nimo de 2 documentos",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m doc_paths \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# rellenar con los path a sus documentos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc_paths) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeben adjuntar un m√≠nimo de 2 documentos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m total_paginas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(\u001b[38;5;28mopen\u001b[39m(doc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mpages) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doc_paths)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m total_paginas \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP√°ginas insuficientes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_paginas\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Deben adjuntar un m√≠nimo de 2 documentos"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta ac√°`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "√âxito!\n",
    "<center>\n",
    "<img src =\"https://media.tenor.com/MRQgxcelAV8AAAAM/perry-the-platypus-phineas-and-ferb.gif\" width = 400 />"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNs28AeL6L8BlEf3067k5qg",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
