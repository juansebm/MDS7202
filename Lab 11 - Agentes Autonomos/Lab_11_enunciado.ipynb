{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: Pienso, luego predigo üí°**\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Juan Mi√±o\n",
    "- Nombre de alumno 2: Diego Espinoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/juansebm/MDS7202/tree/main/Lab%2011%20-%20Agentes%20Autonomos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack es un entorno de decisi√≥n secuencial modelado como un Proceso de Decisi√≥n de Markov (MDP), donde un jugador busca vencer al dealer obteniendo una suma de cartas lo m√°s cercana posible a 21, sin pasarse. Recordemos que el Blackjack bajo la formulaci√≥n de MDP cumple con la propiedad de Markov que dice que la probabilidad del siguiente estado y recompensa depende √∫nicamente del estado actual y la acci√≥n tomada, no del historial entero. \n",
    "\n",
    "- Estados (Observaciones): Cada estado est√° representado por una tupla de tres valores:\n",
    "    - player_sum: suma actual de las cartas del jugador (rango 4‚Äì21),\n",
    "    - dealer_showing: valor de la carta visible del dealer (1‚Äì10),\n",
    "    - usable_ace: indicador binario (1 si el jugador tiene un as usable, es decir, que puede contar como 11 sin pasar de 21, y 0 en caso contrario).\n",
    "\n",
    "- Acciones: El agente (jugador) puede elegir entre dos acciones:\n",
    "    - hit (1): pedir una carta adicional,\n",
    "    - stick (0): detenerse y pasar el turno al dealer.\n",
    "\n",
    "- Recompensas:\n",
    "    - Victoria: +1\n",
    "    - Derrota: -1\n",
    "    - Empate: 0\n",
    "    - Victoria con Blackjack natural (21 con dos cartas): +1.5 (si natural=True), o +1 si se ignora la bonificaci√≥n.\n",
    "\n",
    "- T√©rmino del episodio: El episodio finaliza si:\n",
    "    - el jugador hace hit y su mano supera 21 (bust)\n",
    "    - el jugador hace stick y se resuelve el juego contra el dealer.\n",
    "\n",
    "Din√°mica del juego:\n",
    "El jugador comienza con dos cartas visibles; el dealer con una visible y una oculta. Las cartas se extraen con reemplazo. Las figuras (J, Q, K) valen 10, los ases valen 1 u 11 (si no provocan bust), y el resto tiene su valor num√©rico.\n",
    "\n",
    "Si el jugador elige hit y su suma supera 21, pierde inmediatamente (bust). Si elige stick, el dealer revela su carta oculta y roba hasta alcanzar una suma de 17 o m√°s. Si el dealer se pasa, el jugador gana. Si ambos permanecen dentro del l√≠mite, gana quien tenga la suma m√°s alta; si empatan, el resultado es un empate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 5000 episodios: -0.3680\n",
      "Desviaci√≥n est√°ndar: 0.9085\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 5000\n",
    "n_movimientos_maximo = 100000\n",
    "\n",
    "for episodio in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en 5000 episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = [-1.5, -0.5, 0.5, 1.25, 1.75]#estos bins sirven para que se vean las recompensas\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(recompensas, bins=bins, edgecolor='black', rwidth=0.8)\n",
    "\n",
    "plt.xticks([-1, 0, 1, 1.5])\n",
    "plt.xlabel(\"Recompensa obtenida\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de recompensas\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?\n",
    "\n",
    "- El rendimiento de la pol√≠tica aleatoria es pobre, porque la mayor√≠a de las simulaciones muestran recompensas negativas. Ahora bien, esta pol√≠tica sirve como l√≠nea base o baseline para comparar agentes entrenados mediante aprendizaje por refuerzo o reinforcemente learning, cualquier pol√≠tica que mejore el promedio de -0.38 podr√° considerarse una mejora. \n",
    "\n",
    "¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?\n",
    "\n",
    "- Un promedio negativo de las recompensas para 5000 simulaciones/partidas y una cantidad m√°xima de 100 pasos/movimientos permitidos por partida, implica que las partidas en donde se pierde tomando aleatoriamente decisiones es mayor que aquellas en las que se gana. Es decir, en nuestro caso la cantidad de partidas perdidas es aproximadamente el doble que la cantidad de partidas ganadas. Por otro lado, podemos decir que el puntaje de las partidas empatadas no afecta en nada porque la recompensa de dichas partidas es nula, por lo que en el promedio de las recompensas nunca se ver√°n reflejadas las partidas empatadas o que quedan en `draw`. Esto adem√°s, se podr√≠a interpretar como que el mecanismo, aunque sea aleatorio, nos diese el resultado de un dado o moneda cargada, con la probabilidad dada por la ley de Laplace m√°s inclinada hacia el caso de partidas perdidas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `Stable Baselines3 (SB3)` es una biblioteca de python que se usa para entrenar y evaluar agentes de aprendizaje por refuerzo (RL), y adem√°s es implementada sobre PyTorch (usada por lo dem√°s, para implementar modelos de RL que son bastantes cercanos al estado del arte). Por otro lado, esta ofrece una colecci√≥n de algoritmos RL listos para usar con soporte para entornos de Gymnasium. De la clase de aprendizaje reforzado, podemos tomar como ejemplo el alfortitmo DQN (tambi√©n conocido como [Deep Q-Network](https://markelsanz14.medium.com/introducci√≥n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)).\n",
    "\n",
    " En particular, seg√∫n dicha referencia, ese algoritmo combina el algoritmo Q-learning con redes neuronales profundas (que son fant√°sticas para aproximar funciones no lineales), que en realidad son dos redes neuronales, la primera de las cuales aproxima la funci√≥n Q(s, a, $\\theta$) (del estado `s` y acci√≥n `a` actuales) mientras que la segunda parametrizada por $\\theta'$ aproxima los valores Q pero del siguiente estado s' y acci√≥n a'. Aqu√≠ el aprendizaje ocurre en la red primera y no en la objetivo (la segunda), donde los parametros del aprendizaje de la primera se copian a la segunda transmitiendo el aprendizaje. Aqu√≠ queremos minimizar una funci√≥n de p√©rdida que es la resta de la ecuaci√≥n de bellman en DQN. \n",
    "\n",
    "Esta funci√≥n de p√©rdida se minimiza usando el algoritmo de descenso de gradientes. En este contexto, el agente no es m√°s que el objeto `model`, el cual usa el algoritmo DQN como su cerebro para aprender/tomar decisiones, porque por definici√≥n un agente es el componente que 1) observa el estado del entorno o ambiente, 2) toma decisiones, 3) recibe recompensas, 4) actualiza su pol√≠tica para mejorar su comportamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb551aa66454dfead65bff8d54fead6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model = DQN(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#lo entrenamos para poder resolver el ambiente Blackjack\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m2e5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[39m, in \u001b[36mDQN.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    259\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[32m    260\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    266\u001b[39m ) -> SelfDQN:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     rollout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout.continue_training:\n\u001b[32m    339\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py:557\u001b[39m, in \u001b[36mOffPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[39m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.actor.reset_noise(env.num_envs)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# Select action randomly or according to policy\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m actions, buffer_actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[32m    560\u001b[39m new_obs, rewards, dones, infos = env.step(actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/common/off_policy_algorithm.py:390\u001b[39m, in \u001b[36mOffPolicyAlgorithm._sample_action\u001b[39m\u001b[34m(self, learning_starts, action_noise, n_envs)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    386\u001b[39m     \u001b[38;5;66;03m# Note: when using continuous actions,\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;66;03m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[32m    388\u001b[39m     \u001b[38;5;66;03m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mself._last_obs was not set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     unscaled_action, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_space, spaces.Box):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/dqn/dqn.py:255\u001b[39m, in \u001b[36mDQN.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    253\u001b[39m         action = np.array(\u001b[38;5;28mself\u001b[39m.action_space.sample())\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     action, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/common/policies.py:368\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    365\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[32m    370\u001b[39m actions = actions.cpu().numpy().reshape((-\u001b[32m1\u001b[39m, *\u001b[38;5;28mself\u001b[39m.action_space.shape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/dqn/policies.py:184\u001b[39m, in \u001b[36mDQNPolicy._predict\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> th.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/dqn/policies.py:69\u001b[39m, in \u001b[36mQNetwork._predict\u001b[39m\u001b[34m(self, observation, deterministic)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> th.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[32m     71\u001b[39m     action = q_values.argmax(dim=\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/stable_baselines3/dqn/policies.py:66\u001b[39m, in \u001b[36mQNetwork.forward\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs) -> th.Tensor:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    Predict the q-values.\u001b[39;00m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[33;03m    :param obs: Observation\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente Blackjack\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo, exportamos el modelo entrenado a un archivo .zip en pocas l√≠neas de c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [],
   "source": [
    "model.save(\"dqn_blackjack\")\n",
    "del model\n",
    "model = DQN.load(\"dqn_blackjack\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.114, 0.9460465104845533)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model, model.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo es el performance de su agente?\n",
    "\n",
    "- El agente que usa DQN por detr√°s tiene un promedio de recompensas de -0.3, lo que es ligeramente mayor al caso del baseline que eleg√≠a aleatoriamente los movimientos por cada partida de blackjack (-0.053 que puede cambiar y -0.38 respectivamente).\n",
    "\n",
    "¬øEs mejor o peor que el escenario baseline?\n",
    "\n",
    "- El performance, como se dijo en la parte anterior, es mejor que el escenario baseline porque el agente entrenado aprende una pol√≠tica que maximiza la recompensa esperada, mientras que la pol√≠tica aleatoria no incorpora ninguna estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo vemos que clase de Python pertenecen los estados, usando el m√©todo .reset para el `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el tipo de clase de python del estado es:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "estado, info = env.reset()\n",
    "print(\"el tipo de clase de python del estado es: \", type(estado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [],
   "source": [
    "def obtener_accion(modelo, estado):\n",
    "    \"\"\"Esta funci√≥n obtiene la acci√≥n que el modelo tomar√≠a \n",
    "    para cierto estado.\n",
    "\n",
    "    Args:\n",
    "        modelo: modelo que se usar√° para predecir el movimiento del agente\n",
    "                (que como ya nos habremos dado cuenta, es el DQN).\n",
    "        estado: estado de la partida. \n",
    "\n",
    "    Returns:\n",
    "        accion.item(): la accion es un array o tensor, mientras que agregar \n",
    "                       .item() extrae el valor num√©rico simple. Aqu√≠\n",
    "                       deterministic=True indica que queremos la acci√≥n m√°s\n",
    "                       probable u √≥ptima, sin exploraci√≥n aleatoria. Por √∫ltimo,\n",
    "                       esta acci√≥n es un 1 o un 0, que representa la acci√≥n que \n",
    "                       tomar√° el agente (hit/stick respectivamente). \n",
    "    \"\"\"\n",
    "    accion, informacion = modelo.predict(estado, deterministic=True)\n",
    "    return accion.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que el estado es una 3-tupla de la forma (player_sum, dealer_card, usable_ace), entonces el jugador que tiene 6 (de suma), el dealer muestra 7 y el jugador no tiene un As usable se representar√° por la tupla (6, 7, False), mientras que el jugador que tiene 19, cuyo dealer muestra 3 y el jugador tiene un As usable tendr√° una tupla que lo representa que es (19, 3, True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): 1\n",
      "Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "estado2 = (19, 3, True)\n",
    "\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 1 (6, 7, sin As): {accion1}\")   # 0 = stick, 1 = hit\n",
    "print(f\"Acci√≥n que toma el agente entrenado con DQN en estado 2 (19, 3, con As): {accion2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La acci√≥n que toma el agente para el primer estado es = 1, es decir, hit. En este contexto, el jugador tiene una suma baja (6), y el dealer muestra un 7, que es una carta fuerte, por lo que la estrategia b√°sica recomienda pedir carta (hit) para mejorar la mano, ya que quedarse con 6 casi seguro perder√≠a. \n",
    "\n",
    "Por otro lado, el jugador en el segundo estado tiene una mano fuerte (19), con un As usable que ayuda a no pasarse, mientras que el dealer muestra una carta d√©bil (3), por lo que podemos concluir que la estrategia b√°sica recomendar√≠a quedarse (stick) con un 19 para no arriesgar pasarse.\n",
    "\n",
    "Por lo tanto, ambas decisiones del agente entrenado por DQM son consistentes con las estrategias que una persona tomar√≠a en la vida real en el juego blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (8.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/diegoespinoza/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El par√°metro `Continuous=True` son acciones continuas, donde el agente puede elegir valores continuos reales para los motores, es decir, puede controlar la potencia con precisi√≥n variable (digamos, motor principal a potencia 40%, motor lateral derecho al 20%, etc.). Mientras que si ponemos `continuous=False` el agente elige entre un conjunto finito de acciones predefinidas (por ejemplo, encender motor principal, motor lateral izquierdo o derecho, o no hacer nada, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac√°`:\n",
    "\n",
    "LunarLander es un entorno de simulaci√≥n donde el objetivo es aterrizar una nave espacial suavemente en una plataforma ubicada en (0, 0). Es un cl√°sico problema de optimizaci√≥n de trayectorias, que est√° inspirado en el principio del m√°ximo de Pontryagin. Para este existen dos versiones; 1) continuous=False (por defecto) indica acciones discretas, mientras que continuous=True indica acciones continuas.\n",
    "\n",
    "- Espacio de Acciones: \n",
    "    - Discreto `continuous=False`\n",
    "        - 0: no hacer nada\n",
    "        - 1: encender motor lateral izquierdo\n",
    "        - 2: encender motor principal\n",
    "        - 3: encender motor lateral derecho\n",
    "    - Continuo `continuous=True`: la acci√≥n es un vector [main, lateral] $\\epsilon$ [-1, 1]:\n",
    "        - main < 0: motor principal apagado\n",
    "        - 0 < main < 1: motor principal de 50% a 100% de potencia\n",
    "        - lateral < -0.5: motor izquierdo\n",
    "        - lateral > 0.5: motor derecho\n",
    "        - la potencia lateral tambi√©n escala de 50% a 100%\n",
    "\n",
    "- Espacio de Observaci√≥n: Es un vector de 8 dimensiones;\n",
    "    - x, y: posici√≥n\n",
    "    - vx, vy: velocidad lineal\n",
    "    - angle: √°ngulo\n",
    "    - angular_velocity: velocidad angular\n",
    "    - leg1_contact, leg2_contact: contacto de piernas en el suelo\n",
    "\n",
    "- Recompensas: \n",
    "    - son recompensas por paso:\n",
    "        - mejora por acercarse a la base, reducir la velocidad, o mantener horizontalidad\n",
    "        - se suman +10 puntos por cada pierna tocando el suelo\n",
    "        - se restan -0.03 por uso lateral, y -0.3 por uso del motor principal\n",
    "    - recompensa final:\n",
    "        - +100 si aterriza correctamente\n",
    "        - -100 si se estrella\n",
    "    - se considera un episodio exitoso como aquel cuya recompensa total es mayor o igual a 200.\n",
    "\n",
    "En comparaci√≥n con blackjack: en este ambiente LunarLander las acciones controlan motores para aterrizar suavemente, mientras que en blackjack las acciones controlan decisiones de juego con din√°mica estoc√°stica (es decir, aleatoria, al menos en el caso del baseline que ya hab√≠amos visto) y reglas fijas (es decir, las reglas del juego). Lunar Lander requiero modelos que manejen acciones continuas, mientras que Blackjack es ideal para introducir MDP's y enfoques de exploraci√≥n vs explotaci√≥n. Por otro lado, en este ambiente las decisiones son infinitas en cantidad (porque el tipo de acci√≥n es continua), mientras que en el caso del blackjack solo hay 2 posibles decisiones (Hit y Stick). El riesgo de tomar una acci√≥n tambi√©n es diferente, porque un mal movimiento puede causar ca√≠da o choque de una nave, mientras que en el primer caso no puede sino provocar una p√©rdida simb√≥lica de un juego. \n",
    "\n",
    "Ahora bien, el ambiente de Lunar Lander cumple con la propiedad de MDP (Markov Decision Process) porque el estado actual del mismo contiene toda la informaci√≥n necesaria para predecir el futuro (informaci√≥n codificada en `env.step()`), independientemente del pasado. Es decir, no hay dependencias ni variables ocultas que el agente necesite memorizar para pensar en el futuro movimiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa en 10 episodios: -97.9389\n",
      "Desviaci√≥n est√°ndar: 73.3275\n"
     ]
    }
   ],
   "source": [
    "recompensas = []\n",
    "n_simulaciones = 10\n",
    "n_movimientos_maximo = 100\n",
    "\n",
    "for episode in range(n_simulaciones):\n",
    "    done = truncado = False\n",
    "    estado, info = env_lunar.reset()\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    for paso_temporal in range(n_movimientos_maximo):\n",
    "        accion = env_lunar.action_space.sample()\n",
    "        estado, recompensa, done, truncado, info = env_lunar.step(accion)\n",
    "        recompensa_total += recompensa     \n",
    "        if done or truncado:\n",
    "            break\n",
    "    recompensas.append(recompensa_total)    \n",
    "\n",
    "env_lunar.close()\n",
    "\n",
    "average_recompensa = np.mean(recompensas)\n",
    "std_recompensa = np.std(recompensas)\n",
    "\n",
    "print(f\"Promedio de recompensa en {n_simulaciones} episodios: {average_recompensa:.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {std_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el valor promedio tan negativo indica que el agente tiende mucho m√°s a fallar sistem√°ticamente que a aterrizar suavemente en su tarea, lo cual era de esperar si se tomaba una pol√≠tica (no informadas) sin conocimiento del entorno que depende del azar. Est√° cerca de -139 porque el castigo por estrellarse era -200. As√≠, explorando las decisiones al azar no obtenemos sino malos resultados. \n",
    "\n",
    "Por lo tanto, el desempe√±o de esta pol√≠tica es pobre, pero nos servir√° como una referencia m√≠nima para comparar futuras pol√≠ticas entrenadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usaremos el modelo PPO que s√≠ es compatible con acciones continuas, como lo requiere nuestro caso. De la [siguiente referencia](https://www-geeksforgeeks-org.translate.goog/machine-learning/a-brief-introduction-to-proximal-policy-optimization/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc), podemos ver que, la Optimizaci√≥n Proximal de Pol√≠ticas (PPO) es un m√©todo que ayuda a un agente a mejorar sus acciones para obtener mejores recompensas. Al igual que otros m√©todos de gradiente de pol√≠ticas, modifica directamente la toma de decisiones del agente. Sin embargo, a diferencia de otros, la PPO a√±ade control adicional para evitar que estos cambios sean demasiado grandes. El objetivo principal de PPO es encontrar un equilibrio entre dos aspectos: 1) maximizar el objetivo (este es el n√∫cleo de la optimizaci√≥n de pol√≠ticas, donde la pol√≠tica del agente se ajusta para maximizar las recompensas esperadas) y 2) mantener las actualizaciones peque√±as (los grandes cambios pueden arruinar el aprendizaje, por lo que se debe ajustar con precisi√≥n la velocidad del aprendizaje).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12db5010f62148dc81e591a827d962e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x368274f10>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "timesteps = 10000\n",
    "\n",
    "#ahora inicializamos el agente\n",
    "model_lunar = PPO(\"MlpPolicy\", env_lunar, verbose=0)\n",
    "\n",
    "#lo entrenamos para poder resolver el ambiente LunarLander\n",
    "model_lunar.learn(total_timesteps=timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-120.399338565, 47.07241200895624)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lunar.save(\"PPO_LunarLander\")\n",
    "del model_lunar\n",
    "model_lunar = PPO.load(\"PPO_LunarLander\", env=env_lunar)\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "recompensa_promedio, desviacion_recompensa = evaluate_policy(model_lunar, model_lunar.get_env(), n_eval_episodes=1000)\n",
    "recompensa_promedio, desviacion_recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es pero que el baseline (-120 vs -97) pero porque a√∫n no lo hemos optimizado, por lo que en la siguiente subsecci√≥n lo corregiremos, y porque adem√°s no hemos usado ning√∫n hiperpar√°metro como s√≠ lo hicimos en el caso de las decisiones aleatorias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094adb95bc884ab28bc7b7606a491216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con timesteps=2000000, learning_rate=0.0005, batch_size=32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(159.51521190761963, 112.16941103430986)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "pasostemporales_totales = 2000000\n",
    "\n",
    "env_lunar = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) \n",
    "\n",
    "print(f\"Entrenando con timesteps={pasostemporales_totales}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env_lunar,\n",
    "            learning_rate=learning_rate,\n",
    "            verbose=0,\n",
    "            batch_size=batch_size)\n",
    "model.learn(total_timesteps=pasostemporales_totales, progress_bar=True)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env_lunar, n_eval_episodes=100)\n",
    "env_lunar.close()\n",
    "\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"agent_performance.gif\", width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el agente que usa PPO para optimizar el movimiento del cohete que aterriza en la luna, lo hace muy bien (con un promedio de recompensas de 159, lo que es muy cercano a 200, lo √≥ptimo). Por otro lado, podemos decir que el movimiento que realiza la astronave es casi vertical, tal y como se puede ver del gif que contiene cinco de las iteraciones/episodios/partidas. Los par√°metros que se utilizaron fueron: `learning_rate = 0.0005`,`batch_size = 32`, `pasostemporales_totales = 2000000`. Esta operaci√≥n o serie de dos millones de partidas se demor√≥ alrededor de 30 minutos, lo que indica que es bastante lento pero mas que nada porque necesita mucho entrenamiento. En general, el agente toma decisiones (que van en un espectro continuo de acciones) de manera precisa como si fuese un experto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usar los links para las api keys: https://aistudio.google.com/app/u/1/apikey y https://app.tavily.com/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5474,
     "status": "ok",
     "timestamp": 1731272598047,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 180
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\"paper_1.pdf\", \"paper_3.pdf\"]\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def cargar_texto_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    documentos = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        texto = page.extract_text()\n",
    "        if texto:\n",
    "            documentos.append(Document(page_content=texto, metadata={\"source\": path, \"page\": i}))\n",
    "    return documentos\n",
    "\n",
    "\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    docs.extend(cargar_texto_pdf(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c60f8c0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                     search_kwargs={\"k\": 3},\n",
    "                                     )\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='03ee260f-3368-482d-a92e-0836585e2db9', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='The whole process of price optimization based on the price demand elasticity can be\\ndescribed using the following Figure 1:\\nMathematics 2021 , 9, x FOR PEER REVIEW 5 of 17 \\n \\n ticity is estimated based on the rate segmen ts [27], log‚Äìlog regression model [54], auto-\\nregressive distributed lag model [55], linear and non-linear demand function [43], logistic \\nregression [50] and multiple logistic model [56,57]. \\nTran [55] used an autoregressive distributed lag model for the evaluation of eco-\\nnomic factors and their connection to demand for a luxury hotel in the US. The study \\nworks with the price demand elastici ty coefficient that varies from ‚àí0.03 in the long term \\nto ‚àí0.02 in the short term. A study by Rossell√≥ et al. [58] focused on the demand for ac-\\ncommodation services in Germany, the UK, France and the Netherlands, while focusing \\non the elasticity of demand, findin g values of the coefficient from ‚àí0.51 to ‚àí4. A combina-'),\n",
       " Document(id='a107bc37-0555-44f2-84f8-e37c6ea55b14', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='Mathematics 2021 ,9, 1552 5 of 16\\nthe price demand elasticity coefÔ¨Åcient that varies from \\x000.03 in the long term to \\x000.02 in\\nthe short term. A study by Rossell √≥et al. [ 58] focused on the demand for accommodation\\nservices in Germany, the UK, France and the Netherlands, while focusing on the elasticity\\nof demand, Ô¨Ånding values of the coefÔ¨Åcient from \\x000.51 to\\x004. A combination of the\\npreviously mentioned approaches towards the elasticity of demand (destination level and\\nnarrow product level category) can be found in a study by Damonte et al. [ 59], where the\\nprice demand elasticity differs based on the level of services and the destination size, where\\nthe properly deÔ¨Åned region can bring better results than the aggregate approach. The same\\nresults are proposed by Canina and Calver as well [60].\\nA study by Hiemstra and Ismail [ 61] focused on the elasticity of demand considering\\nseveral variables and the impact of taxes as well, where the supply elasticity coefÔ¨Åcient'),\n",
       " Document(id='377c0d7f-b2e1-476b-ad9e-75585f543c39', metadata={'source': 'paper_1.pdf', 'page': 4}, page_content='As proposed in the literature review, there are various approaches toward measuring\\nthe price elasticity of the demand. Within this model, we assume three different options\\nthat can be applied, namely using (1) the log‚Äìlog linear model used by Petricek et al. [ 54], (2)\\nan arc elasticity estimate using the Monte Carlo simulation, or (3) an individual coefÔ¨Åcient\\nvalues setting.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"como se define el coeficiente de elasticidad precio-demanda\"\n",
    "relevant_documents = retriever.invoke(question)\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole process of price optimization based on the price demand elasticity can be\n",
      "described using the following Figure 1:\n",
      "Mathematics 2021 , 9, x FOR PEER REVIEW 5 of 17 \n",
      " \n",
      " ticity is estimated based on the rate segmen ts [27], log‚Äìlog regression model [54], auto-\n",
      "regressive distributed lag model [55], linear and non-linear demand function [43], logistic \n",
      "regression [50] and multiple logistic model [56,57]. \n",
      "Tran [55] used an autoregressive distributed lag model for the evaluation of eco-\n",
      "nomic factors and their connection to demand for a luxury hotel in the US. The study \n",
      "works with the price demand elastici ty coefficient that varies from ‚àí0.03 in the long term \n",
      "to ‚àí0.02 in the short term. A study by Rossell√≥ et al. [58] focused on the demand for ac-\n",
      "commodation services in Germany, the UK, France and the Netherlands, while focusing \n",
      "on the elasticity of demand, findin g values of the coefficient from ‚àí0.51 to ‚àí4. A combina-\n",
      "\n",
      "Mathematics 2021 ,9, 1552 5 of 16\n",
      "the price demand elasticity coefÔ¨Åcient that varies from \u00000.03 in the long term to \u00000.02 in\n",
      "the short term. A study by Rossell √≥et al. [ 58] focused on the demand for accommodation\n",
      "services in Germany, the UK, France and the Netherlands, while focusing on the elasticity\n",
      "of demand, Ô¨Ånding values of the coefÔ¨Åcient from \u00000.51 to\u00004. A combination of the\n",
      "previously mentioned approaches towards the elasticity of demand (destination level and\n",
      "narrow product level category) can be found in a study by Damonte et al. [ 59], where the\n",
      "price demand elasticity differs based on the level of services and the destination size, where\n",
      "the properly deÔ¨Åned region can bring better results than the aggregate approach. The same\n",
      "results are proposed by Canina and Calver as well [60].\n",
      "A study by Hiemstra and Ismail [ 61] focused on the elasticity of demand considering\n",
      "several variables and the impact of taxes as well, where the supply elasticity coefÔ¨Åcient\n",
      "\n",
      "As proposed in the literature review, there are various approaches toward measuring\n",
      "the price elasticity of the demand. Within this model, we assume three different options\n",
      "that can be applied, namely using (1) the log‚Äìlog linear model used by Petricek et al. [ 54], (2)\n",
      "an arc elasticity estimate using the Monte Carlo simulation, or (3) an individual coefÔ¨Åcient\n",
      "values setting.\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(format_docs(relevant_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematics 2021 ,9, 1552 9 of 16\n",
      "it is always necessary to respect the cost aspect in particular. The total cost of consummated\n",
      "accommodation can be determined for a suitable presentation as the sum of the Ô¨Åxed and\n",
      "variable costs expressed as unit quantities (usually on a daily basis), as follows:\n",
      "ATC b=FC\n",
      "√•b\n",
      "b=1Qb+VC\n",
      "√•b\n",
      "b=1Qb, (11)\n",
      "where index bindicates a speciÔ¨Åc reservation and Qbindicates the ‚Äúorder‚Äù of a given\n",
      "reservation in terms of the number of used capacities. It is always necessary to focus on the\n",
      "marginal revenue and marginal costs resulting from the next added unit of length of stay\n",
      "(usually one night/room). Indeed, a possible reduction in the price per unit of length of\n",
      "stay is possible only under the following conditions:\n",
      "MC b\u0014MR b, (12)\n",
      "where index bagain indicates one added unit length of stay for reservation. The speciÔ¨Åc\n",
      "setting of a possible price reduction is, therefore, always individual and again it is necessary\n",
      "\n",
      "The proposed optimized rate is mainly reÔ¨Çecting the segment characteristics, namely\n",
      "the price demand elasticity, but cannot be used as the Ô¨Ånal selling rate as this proposed\n",
      "rate still omits several other factors that have a direct impact on the product pricing.\n",
      "At the moment when it is possible to start from the basic offer price, it is time to\n",
      "determine the price of a speciÔ¨Åc reservation. This approach is what we could call dynamic\n",
      "price formation because the price changes virtually on a daily basis (in some cases even\n",
      "more often) and it also needs to always reÔ¨Çect the current situation but take into account the\n",
      "minimum acceptable price as well. When guaranteeing the price of a particular reservation,\n",
      "it is then necessary to focus on several aspects that affect it and that are appropriate (and in\n",
      "some cases necessary) to monitor.\n",
      "The creation of this price depends on several key factors [ 68‚Äì72], which can be sum-\n",
      "marized in the following:\n",
      "\u000f Length of stay (LOS);\n",
      "\u000f Occupancy (Occ);\n",
      "\n",
      "rate still omits several other factors that ha ve a direct impact on the product pricing. \n",
      "At the moment when it is possible to start from the basic offer price, it is time to \n",
      "determine the price of a specific reservation. This approach is what we could call dynamic \n",
      "price formation because the price changes virt ually on a daily basis (in some cases even \n",
      "more often) and it also needs to always refl ect the current situation but take into account \n",
      "the minimum acceptable price as well. When gu aranteeing the price of a particular reser-\n",
      "vation, it is then necessary to focus on several aspects that affect it and that are appropriate (and in some cases necessary) to monitor. \n",
      "The creation of this price depends on seve ral key factors [68‚Äì72], which can be sum-\n",
      "marized in the following: \n",
      "‚Ä¢ Length of stay (LOS); \n",
      "‚Ä¢ Occupancy (Occ); \n",
      "Figure 2. Monte Carlo simulation - price.\n",
      "The coefÔ¨Åcient of price elasticity was measured at the value \u00000.85 (as the mean\n"
     ]
    }
   ],
   "source": [
    "retriever_chain = retriever | format_docs\n",
    "print(retriever_chain.invoke(\"¬øcomo podemos crear un codigo para optimizar el precio de las habitaciones de un hotel, si queremos maximizar los ingresos?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = '''\n",
    "Eres un asistente experto en programaci√≥n e interpretaci√≥n de resultados de varios papers cient√≠ficos acerca de los Ingresos de Hoteles.\n",
    "Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci√≥n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Idioma: {language}\n",
    "Respuesta √∫til:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La informaci√≥n proporcionada describe varios enfoques para la optimizaci√≥n de tarifas de habitaciones de hotel.  Se mencionan los siguientes aspectos:\n",
      "\n",
      "* **Factores que influyen en la optimizaci√≥n de precios:**  El precio de una habitaci√≥n depende de diversos factores, incluyendo el segmento de mercado, la ventana de reserva, el tipo de habitaci√≥n, la estrategia de precios (ej. $199.99 en lugar de $200), la marca del hotel, el pa√≠s, la moneda, etc.  Adem√°s, se consideran la duraci√≥n de la estancia (LOS), la ocupaci√≥n actual (Occ) y el tiempo de antelaci√≥n de la reserva (BLT) como atributos que influyen en el precio.\n",
      "\n",
      "* **Variables y restricciones:** Se utiliza una variable indicadora (ùêºùëñùëóùëòùëô) para indicar si un precio espec√≠fico (ùëùùëñ) es seleccionado para una combinaci√≥n dada de segmento de mercado (ùëò), ventana de reserva (ùëó) y tipo de habitaci√≥n (ùëô).  Solo se puede seleccionar un precio por cada combinaci√≥n.  Algunos segmentos de mercado tienen precios fijos (ùëò‚ààùíÆ0), los cuales deben incorporarse al modelo para considerar la p√©rdida de capacidad que generan. Existe una jerarqu√≠a de tipos de habitaciones que permite actualizaciones complementarias a tipos superiores si es necesario. La cantidad de habitaciones vendidas no puede exceder la capacidad total del hotel, considerando la jerarqu√≠a de actualizaciones.\n",
      "\n",
      "* **Modelos de optimizaci√≥n:** Se mencionan varios modelos: uno que se centra en la optimizaci√≥n de precios para una fecha y duraci√≥n de estancia espec√≠fica, basado en la demanda (del hotel y sus competidores), los costos de las habitaciones y la disponibilidad del hotel. Otro modelo, implementado en Carlson Rezidor Hotel Group, se basa en la elasticidad precio de la demanda, las tarifas del mercado (competidores), la disponibilidad de habitaciones, la demanda pronosticada (basada en datos hist√≥ricos de clientes: duraci√≥n de la estancia, segmento de tarifa, d√≠a de la semana y tiempo de antelaci√≥n) y reglas de negocio. Este √∫ltimo modelo aument√≥ los ingresos de los hoteles que lo implementaron en un 2-4% m√°s que los que no lo hicieron.  Finalmente, se menciona un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda en un segmento de mercado espec√≠fico antes del horizonte de planificaci√≥n.\n",
      "\n",
      "En resumen, la informaci√≥n describe modelos de optimizaci√≥n de precios que consideran una variedad de factores y restricciones para maximizar los ingresos de los hoteles.  Estos modelos utilizan datos hist√≥ricos y proyecciones de demanda para determinar el precio √≥ptimo para cada combinaci√≥n de variables.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, \n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"language\": lambda _: \"espa√±ol\"\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"que informaci√≥n tienes sobre optimizacion de tarifas de habitaciones de hoteles?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define de diferentes maneras dependiendo del estudio consultado:\n",
      "\n",
      "* **Tran [55]:**  Utiliza un modelo de retraso distribuido autorregresivo y encuentra que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo para un hotel de lujo en EE. UU.\n",
      "\n",
      "* **Rossell√≥ et al. [58]:**  En un estudio sobre la demanda de servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos, encontraron valores del coeficiente que oscilan entre -0.51 y -4.\n",
      "\n",
      "* **Damonte et al. [59] y Canina y Calver [60]:**  Estos estudios indican que la elasticidad precio-demanda difiere seg√∫n el nivel de servicios y el tama√±o del destino, sugiriendo que una regi√≥n bien definida puede proporcionar mejores resultados que un enfoque agregado.  No se proporciona un valor num√©rico espec√≠fico para el coeficiente.\n",
      "\n",
      "* **Hiemstra e Ismail [61]:**  Este estudio considera la elasticidad de la demanda teniendo en cuenta varias variables y el impacto de los impuestos, pero no se proporciona un valor num√©rico espec√≠fico para el coeficiente de elasticidad precio-demanda.\n",
      "\n",
      "\n",
      "En resumen, no hay una √∫nica definici√≥n del coeficiente, sino que su valor var√≠a seg√∫n el m√©todo de c√°lculo, el tipo de hotel, la ubicaci√≥n geogr√°fica y el periodo de tiempo considerado.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan varias estrategias para optimizar las tarifas de habitaciones de hoteles:\n",
      "\n",
      "* **Modelos de precios din√°micos:**  Se mencionan papers (Aziz et al., 2011; Anderson & Xie, 2016) que abordan modelos de precios din√°micos para la gesti√≥n de ingresos en hoteles.  Estos modelos permiten ajustar los precios en funci√≥n de factores como la demanda y la disponibilidad.\n",
      "\n",
      "* **M√©todos de asignaci√≥n de habitaciones:**  Se describe la comparaci√≥n entre el m√©todo de red anidada y el m√©todo de precio de puja para la asignaci√≥n de habitaciones (Pimentel et al., [29]),  con el m√©todo de red anidada mostrando un mejor rendimiento.  Adem√°s, se menciona la optimizaci√≥n de la asignaci√≥n usando modelos deterministas y estoc√°sticos de demanda (Goldman et al., [30]; Weatherford [31]; De Boer et al. [32]),  en conjunto con pol√≠ticas de control de reservas (Pimentel et al., [29]).\n",
      "\n",
      "* **Optimizaci√≥n de la disponibilidad (niveles de sobreventa):** Pimentel et al. [28] desarrollaron un m√©todo de optimizaci√≥n de ingresos basado en estimaciones de demanda para segmentos de mercado espec√≠ficos, optimizando la disponibilidad y la asignaci√≥n a esos segmentos.\n",
      "\n",
      "* **Segmentaci√≥n del mercado:**  El paper de Du et al. (2016) explora la necesidad de cooperaci√≥n con terceros por parte de los proveedores de servicios que adoptan la segmentaci√≥n del mercado, lo que implica una estrategia de precios diferenciada seg√∫n el segmento.\n",
      "\n",
      "* **An√°lisis de la elasticidad precio de la demanda:** Petricek et al. (2020) identifican el comportamiento del consumidor basado en la elasticidad precio, lo que puede informar la estrategia de precios.\n",
      "\n",
      "\n",
      "La informaci√≥n proporcionada no detalla completamente cada estrategia, pero s√≠ indica su uso en la optimizaci√≥n de ingresos de hoteles.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada menciona que la predicci√≥n de la demanda de habitaciones en hoteles se basa en datos hist√≥ricos de reservas, considerando dos variables relacionadas con el tiempo: la fecha de reserva y el tiempo hasta el consumo.  Se menciona la selecci√≥n de m√©todos como una consideraci√≥n importante, pero no se especifican m√©todos concretos utilizados.  Adem√°s, se indica que el pron√≥stico puede ser de noches de habitaci√≥n o llegadas, y puede agregarse a diferentes niveles (total, por categor√≠a de tarifa, por duraci√≥n de la estancia, o alguna combinaci√≥n).  Finalmente, se mencionan estudios que modelan la din√°mica de la demanda tur√≠stica (Rossell√≥ et al., 2005),  utilizan la elasticidad precio de la demanda para servicios de alojamiento (Damonte et al., 1998; Petricek et al., 2020), y emplean simulaci√≥n de Monte Carlo para pronosticar llegadas y ocupaci√≥n hotelera (Zakhary et al., 2021).  Sin embargo, no se describe en detalle qu√© m√©todos espec√≠ficos utilizan los hoteles para la predicci√≥n.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preguntas_respuestas = [\n",
    "    (\n",
    "        \"¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\",\n",
    "        \"El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\",\n",
    "        \"Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\",\n",
    "        \"Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "    print(f\"Pregunta: {pregunta}\")\n",
    "    print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "    respuesta_generada = rag_chain.invoke(pregunta)\n",
    "    print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_rag_chain(chunk_size=1000, k=3, search_type=\"similarity\"):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore_temp = FAISS.from_documents(splits, embedding)\n",
    "    retriever = vectorstore_temp.as_retriever(search_type=search_type, search_kwargs={\"k\": k})\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    retriever_chain = retriever | format_docs\n",
    "\n",
    "    rag_template = '''\n",
    "    Eres un asistente experto en la interpretaci√≥n de resultados de varios papers cient√≠ficos acerca de los Ingresos de Hoteles.\n",
    "    Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "    Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "    Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "    Informaci√≥n relevante: {context}\n",
    "    Pregunta: {question}\n",
    "    Idioma: {language}\n",
    "    Respuesta √∫til:\n",
    "    '''\n",
    "    rag_prompt = PromptTemplate.from_template(rag_template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever_chain,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"language\": lambda _: \"espa√±ol\"\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 500, 'k': 3, 'search_type': 'similarity'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, el coeficiente de elasticidad precio-demanda se define a trav√©s de una f√≥rmula que incorpora un elemento de azar debido a la dependencia de la situaci√≥n del mercado actual, la cual puede diferir ligeramente de una perspectiva hist√≥rica.  La f√≥rmula exacta proporcionada es:\n",
      "\n",
      "Epd=0\n",
      "BBBB@ \n",
      "RQd2\n",
      "01\n",
      "s\u0002p\n",
      "2pe(Qd2\u0000m)2\n",
      "\n",
      "Adem√°s, se menciona que un estudio de Rossell et al. [58] encontr√≥ valores del coeficiente de elasticidad de demanda para servicios de alojamiento en Alemania, Reino Unido, Francia y Pa√≠ses Bajos que variaban de -0.51 a -4.  Tambi√©n se indica que el coeficiente var√≠a de -0.03 a largo plazo a -0.02 a corto plazo.  Sin embargo, no se proporciona una definici√≥n conceptual general del coeficiente.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, una estrategia para optimizar las tarifas de habitaciones de hoteles se basa en la elasticidad precio de la demanda.  Esta estrategia, implementada por Carlson Rezidor Hotel Group, considera tambi√©n las tarifas de la competencia, la disponibilidad de habitaciones, la demanda pronosticada (basada en datos hist√≥ricos como la duraci√≥n de la estancia, el segmento de tarifa, el d√≠a de la semana y el tiempo de antelaci√≥n) y reglas de negocio.  Otros papers mencionados (Baker & Collier, 1999; Liu et al., 2014; Narangajavana et al., 2014; Coenders et al.) analizan aspectos relacionados con la gesti√≥n de ingresos y precios en hoteles y aerol√≠neas, pero no detallan las estrategias espec√≠ficas utilizadas para la optimizaci√≥n de tarifas.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, los hoteles utilizan la simulaci√≥n de Monte Carlo para predecir procesos como llegadas, cancelaciones, duraci√≥n de la estancia, ausencias, estacionalidad y tendencias.  Tambi√©n se utiliza para pronosticar llegadas y ocupaci√≥n hotelera, as√≠ como para modelar llegadas, cancelaciones y reservas de grupo usando programaci√≥n no lineal.  Adicionalmente, la informaci√≥n menciona que la predicci√≥n de la demanda deber√≠a considerar dos variables relacionadas con el tiempo: el momento de la reserva y el tiempo hasta el consumo.  Finalmente, se indica que la selecci√≥n del m√©todo de pron√≥stico debe considerar el tipo de pron√≥stico (noches de habitaci√≥n o llegadas), el nivel de agregaci√≥n (total, por categor√≠a de tarifa, por duraci√≥n de la estancia o alguna combinaci√≥n), el tipo de datos (restringidos o no restringidos), el n√∫mero de per√≠odos a incluir en el pron√≥stico, qu√© datos usar, la cantidad de datos, el tratamiento de valores at√≠picos (debido a festivos o eventos especiales) y la medici√≥n de la precisi√≥n.\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 1000, 'k': 1, 'search_type': 'similarity'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada describe varios estudios que analizan el coeficiente de elasticidad precio-demanda, pero no define expl√≠citamente qu√© es.  Los estudios citados muestran rangos de valores para este coeficiente (de -0.03 a -0.02 a largo plazo y -0.51 a -4 en otro estudio),  indicando su variabilidad seg√∫n factores como el plazo (corto o largo), el tipo de servicio, el tama√±o del destino y el nivel de agregaci√≥n de los datos.  Sin embargo, no se ofrece una definici√≥n formal del coeficiente en s√≠.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Bas√°ndome en la informaci√≥n proporcionada, no se describen expl√≠citamente estrategias espec√≠ficas para optimizar las tarifas de habitaciones de hoteles en los papers citados.  Si bien los art√≠culos 45 (Aziz et al., 2011) y 46 (El Gayar et al., 2011) mencionan modelos para la gesti√≥n de ingresos de hoteles (\"Dynamic room pricing model\" y \"An integrated framework for advanced hotel revenue management\", respectivamente),  no detallan las estrategias concretas empleadas dentro de esos modelos.  El resto de los art√≠culos (44, 47 y 48) se centran en otros aspectos relacionados con la gesti√≥n hotelera, como la teor√≠a y pr√°ctica de la gesti√≥n de ingresos (44), la globalizaci√≥n o localizaci√≥n de preferencias del consumidor en reservas de habitaciones (47), y un estudio de precios en el sector a√©reo (48), sin profundizar en estrategias espec√≠ficas de fijaci√≥n de precios para hoteles.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: La informaci√≥n proporcionada no especifica qu√© m√©todos concretos utilizan los hoteles para predecir la demanda.  Solo menciona que la predicci√≥n de la demanda de habitaciones, considerada una etapa de preprocesamiento en la gesti√≥n de ingresos (Revenue Management), debe estudiarse en detalle ya que determina la fiabilidad de la fase de optimizaci√≥n.  Se mencionan varios factores a considerar al realizar la predicci√≥n, como el tipo de pron√≥stico (noches de habitaci√≥n o llegadas), el nivel de agregaci√≥n de los datos, el tipo de datos (restringidos o no restringidos), el n√∫mero de periodos a incluir, qu√© datos usar, la cantidad de datos y el tratamiento de valores at√≠picos.  Sin embargo, no se nombran m√©todos espec√≠ficos de predicci√≥n.\n",
      "--------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Probando configuraci√≥n: {'chunk_size': 2000, 'k': 5, 'search_type': 'mmr'}\n",
      "\n",
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "Respuesta del RAG: El documento describe tres m√©todos para medir la elasticidad precio-demanda:\n",
      "\n",
      "1. **Modelo log-log lineal:**  Utilizado por Petricek et al. [54].  El documento no proporciona detalles sobre la f√≥rmula espec√≠fica de este modelo.\n",
      "\n",
      "2. **Estimaci√≥n de elasticidad de arco usando simulaci√≥n de Monte Carlo:**  Este m√©todo utiliza una simulaci√≥n de Monte Carlo para capturar la incertidumbre en la determinaci√≥n del coeficiente de elasticidad precio-demanda, represent√°ndolo como una distribuci√≥n de probabilidad normal N(Œº, œÉ¬≤).  La f√≥rmula para este m√©todo se presenta como la ecuaci√≥n (3) y (5) en el texto, pero no se transcribe aqu√≠ por su complejidad.  Estas ecuaciones incorporan la cantidad demandada (Qd1, Qd2), la media (m) y la desviaci√≥n est√°ndar (s) de la distribuci√≥n normal, y los precios iniciales y planeados (P1, P2).\n",
      "\n",
      "3. **Asignaci√≥n de valores individuales de coeficiente:**  Este m√©todo consiste en asignar directamente un valor al coeficiente de elasticidad precio-demanda para cada segmento de mercado.  No se proporciona m√°s informaci√≥n sobre c√≥mo se determinan estos valores.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\n",
      "Respuesta esperada: Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, se mencionan varias estrategias para la optimizaci√≥n de tarifas de habitaciones de hotel:\n",
      "\n",
      "* **Estrategias basadas en modelos de optimizaci√≥n:**  Se busca maximizar el ingreso total seleccionando el precio √≥ptimo para cada combinaci√≥n de ventana de reserva, segmento de mercado y tipo de habitaci√≥n, as√≠ como la cantidad √≥ptima de habitaciones a asignar.  Estos modelos incorporan restricciones empresariales, como un precio m√≠nimo (Pmin) basado en an√°lisis de costos y un precio de referencia (Pref) que puede basarse en la tarifa disponible (BAR).  El margen de beneficio se puede calcular como un coeficiente de tasa de crecimiento.\n",
      "\n",
      "* **Estrategias neutrales:**  El margen de beneficio (MUp(Occ)) se calcula con una funci√≥n lineal:  ùëÄùëàùëù(ùëÇùëêùëê) = (ùëé√óùê∏ùë•ùëÇùëêùëê+ùëÉùëöùëñùëõ)‚àíùëÉùëüùëíùëì, donde 'a' es un coeficiente, 'ExOcc' representa la ocupaci√≥n esperada, Pmin es el precio m√≠nimo y Pref es el precio de referencia.\n",
      "\n",
      "* **Estrategias agresivas y conservadoras:**  En estas estrategias, el margen de beneficio se basa en una funci√≥n exponencial (la descripci√≥n no especifica la f√≥rmula exacta).\n",
      "\n",
      "* **Estrategias basadas en la metodolog√≠a PERFORM (InterContinental Hotel Group):** Esta metodolog√≠a considera la ocupaci√≥n del hotel, la elasticidad precio-demanda y las tarifas de la competencia.  Inicialmente se basaba en el control de disponibilidad y el inventario de duraci√≥n de la estancia (LOS), conectado a un modelo determinista.  Con el desarrollo de internet y el comercio electr√≥nico, se ha pasado al precio din√°mico.\n",
      "\n",
      "* **Estrategias que consideran la ventana de reserva:** Se agrupan las reservas en intervalos de tiempo (buckets) seg√∫n la distribuci√≥n de las reservas (Tabla 3.1).  La optimizaci√≥n de precios considera estos intervalos.\n",
      "\n",
      "Finalmente, se menciona que las herramientas de marketing a largo plazo que asumen aumentos de precios mientras se llenan gradualmente las capacidades de alojamiento funcionan relativamente bien, aunque no se detalla c√≥mo se integran estas herramientas en las estrategias de optimizaci√≥n de precios.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pregunta: ¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\n",
      "Respuesta esperada: Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\n",
      "Respuesta del RAG: Basado en la informaci√≥n proporcionada, los hoteles utilizan varias t√©cnicas para predecir la demanda.  Se mencionan espec√≠ficamente la regresi√≥n lineal, la regresi√≥n cuadr√°tica, la aproximaci√≥n spline y los bosques aleatorios (random forests).  De estos, los bosques aleatorios fueron seleccionados por proporcionar los mejores resultados, especialmente al manejar variables num√©ricas y categ√≥ricas.  Adicionalmente, se menciona que la predicci√≥n de la demanda en la industria hotelera debe considerar dos variables relacionadas con el tiempo: el momento de la reserva y el tiempo hasta el consumo.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "configuraciones = [\n",
    "    {\"chunk_size\": 500, \"k\": 3, \"search_type\": \"similarity\"},\n",
    "    {\"chunk_size\": 1000, \"k\": 1, \"search_type\": \"similarity\"},\n",
    "    {\"chunk_size\": 2000, \"k\": 5, \"search_type\": \"mmr\"},\n",
    "]\n",
    "\n",
    "for config in configuraciones:\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Probando configuraci√≥n: {config}\")\n",
    "    rag_chain = crear_rag_chain(**config)\n",
    "    \n",
    "    for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "        print(f\"\\nPregunta: {pregunta}\")\n",
    "        print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "        respuesta_generada = rag_chain.invoke(pregunta)\n",
    "        print(f\"Respuesta del RAG: {respuesta_generada}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/2_xdwnnx5rqgtl8vh2vwkdmc0000gn/T/ipykernel_3438/1150375189.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search_tool = TavilySearchResults(max_results = 1)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_search_tool = TavilySearchResults(max_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "wikpedia_search_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "tools = [wikpedia_search_tool, tavily_search_tool] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "spanish_prompt_text = \"Responde siempre en espa√±ol.\\n\\n\" + react_prompt.template\n",
    "\n",
    "prompt_es = PromptTemplate(\n",
    "    input_variables=react_prompt.input_variables,\n",
    "    template=spanish_prompt_text\n",
    ")\n",
    "\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "agent = create_react_agent(llm, tools, prompt_es)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito buscar informaci√≥n sobre algoritmos de optimizaci√≥n de precios en el contexto de reservas de hoteles.  Wikipedia podr√≠a tener informaci√≥n general sobre algoritmos de optimizaci√≥n, pero una b√∫squeda en un motor de b√∫squeda como tavily_search_results_json podr√≠a dar resultados m√°s espec√≠ficos y actuales sobre este problema particular.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"algoritmos optimizaci√≥n precios hoteles\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Los 10 mejores software de precios din√°micos para hoteles en 2025', 'url': 'https://hoteltechreport.com/es/news/hotel-dynamic-pricing-software', 'content': 'Optimizaci√≥n automatizada de ingresos: utiliza algoritmos impulsados \\u200b\\u200bpor IA para ajustar las tarifas en tiempo real seg√∫n las fluctuaciones de', 'score': 0.7467494}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci√≥n proporciona un enlace a un art√≠culo sobre software de precios din√°micos para hoteles, lo que sugiere que existen algoritmos para optimizar las tarifas.  Sin embargo, no especifica los algoritmos concretos.  Intentar√© una b√∫squeda m√°s espec√≠fica en Wikipedia sobre algoritmos de optimizaci√≥n relevantes.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"algoritmos de optimizaci√≥n\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Wikipedia no arroj√≥ resultados √∫tiles.  La b√∫squeda en tavily_search_results_json, aunque proporcion√≥ un enlace a un art√≠culo relevante, no nombra algoritmos espec√≠ficos.  Intentar√© una b√∫squeda m√°s espec√≠fica en tavily_search_results_json, incluyendo t√©rminos como \"revenue management\" o \"yield management\".\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"algoritmos revenue management hoteles\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Hotel Revenue Management System - RevMaxi - Botshot', 'url': 'https://botshot.ai/products/revmaxi', 'content': 'Our revenue management platform uses ML algorithms to set hotel room prices based on booking patterns, average stay length, guest preferences, etc. Discover the', 'score': 0.81347597}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci√≥n menciona que una plataforma de gesti√≥n de ingresos utiliza algoritmos de ML (Machine Learning) para establecer los precios de las habitaciones de hotel.  Esto confirma la existencia de algoritmos, aunque no especifica cu√°les.  Puedo inferir que se utilizan algoritmos de aprendizaje autom√°tico, posiblemente incluyendo modelos de regresi√≥n, series temporales o incluso redes neuronales, para predecir la demanda y optimizar los precios.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "\n",
      "Final Answer: S√≠, existen algoritmos de computaci√≥n que pueden optimizar la tarifa por habitaci√≥n de un hotel maximizando los ingresos.  Estos algoritmos suelen basarse en t√©cnicas de aprendizaje autom√°tico (Machine Learning), como modelos de regresi√≥n, series temporales o redes neuronales, para analizar datos hist√≥ricos de reservas y predecir la demanda futura, permitiendo as√≠ ajustar din√°micamente las tarifas.  Ejemplos concretos de algoritmos no se mencionan en las fuentes consultadas, pero se indica el uso de algoritmos de ML en plataformas de gesti√≥n de ingresos hoteleros.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "S√≠, existen algoritmos de computaci√≥n que pueden optimizar la tarifa por habitaci√≥n de un hotel maximizando los ingresos.  Estos algoritmos suelen basarse en t√©cnicas de aprendizaje autom√°tico (Machine Learning), como modelos de regresi√≥n, series temporales o redes neuronales, para analizar datos hist√≥ricos de reservas y predecir la demanda futura, permitiendo as√≠ ajustar din√°micamente las tarifas.  Ejemplos concretos de algoritmos no se mencionan en las fuentes consultadas, pero se indica el uso de algoritmos de ML en plataformas de gesti√≥n de ingresos hoteleros.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"¬øHay alg√∫n algoritmo de computaci√≥n que dada una base de datos\"\n",
    "                                  \"hist√≥rica de reservas de hotel con varias variables, pueda optimizar la\" \n",
    "                                  \"tarifa por habitaci√≥n del mismo, queriendo maximizar los ingresos?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "def rag_tool_func(question: str) -> str:\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"RAGTool\",\n",
    "    func=rag_tool_func,\n",
    "    description=\"Utiliza recuperaci√≥n aumentada con informaci√≥n para responder preguntas en espa√±ol sobre diversos temas, especialmente optimizaci√≥n de tarifas de hoteles.\"\n",
    ")\n",
    "\n",
    "def react_tool_func(question: str) -> str:\n",
    "    return agent_executor.invoke({\"input\": question})\n",
    "\n",
    "react_tool = Tool(\n",
    "    name=\"ReActAgentTool\",\n",
    "    func=react_tool_func,\n",
    "    description=\"Agente ReAct que razona y ejecuta herramientas en espa√±ol para resolver tareas complejas.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "tools = [rag_tool, react_tool]\n",
    "\n",
    "supervisor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\n",
      "Respuesta esperada: El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito definir el coeficiente de elasticidad precio-demanda.  Creo que RAGTool puede proporcionar una definici√≥n concisa y precisa.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mEl documento describe tres m√©todos para medir la elasticidad precio-demanda:\n",
      "\n",
      "1. **Modelo log-log lineal:**  Utilizado por Petricek et al. [54].  No se proporciona m√°s detalle sobre su definici√≥n en este texto.\n",
      "\n",
      "2. **Estimaci√≥n de elasticidad de arco usando simulaci√≥n de Monte Carlo:**  Se describe como un m√©todo que captura el coeficiente de elasticidad precio-demanda en forma de una distribuci√≥n de probabilidad, asumiendo una distribuci√≥n normal con par√°metros N(Œº, œÉ¬≤).  La f√≥rmula para este m√©todo se presenta en la ecuaci√≥n (5), pero su interpretaci√≥n completa requiere un conocimiento previo de las variables involucradas (TR, Qi, E(pd)i, Pi, Ppi).  La ecuaci√≥n (3) muestra una f√≥rmula alternativa para Epd que incorpora la simulaci√≥n de Monte Carlo para reflejar la incertidumbre en la reacci√≥n del cliente a los cambios de precio.\n",
      "\n",
      "3. **Asignaci√≥n de valores individuales de coeficiente:** Se menciona como una opci√≥n, pero no se proporciona m√°s informaci√≥n sobre c√≥mo se define el coeficiente en este caso.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La observaci√≥n de RAGTool proporciona m√©todos para *medir* la elasticidad precio-demanda, pero no una definici√≥n clara y concisa del concepto en s√≠. Necesito una definici√≥n m√°s fundamental. Intentar√© con ReActAgentTool para ver si puede proporcionar una definici√≥n m√°s directa.\n",
      "\n",
      "Action: ReActAgentTool\n",
      "Action Input: Define el coeficiente de elasticidad precio-demanda.\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito definir el coeficiente de elasticidad precio-demanda.  Wikipedia deber√≠a tener una buena definici√≥n.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Coeficiente de elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Intentar√© una b√∫squeda diferente en Wikipedia.  Quiz√°s una frase m√°s simple.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Elasticidad precio de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Parece que Wikipedia no tiene una entrada espec√≠fica para \"elasticidad precio de la demanda\". Intentar√© buscar una definici√≥n en internet usando un motor de b√∫squeda general.  Como no tengo acceso a un motor de b√∫squeda general, intentar√© reformular la pregunta para que sea m√°s espec√≠fica y buscar en Wikipedia conceptos relacionados.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Demanda el√°stica\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Parece que Wikipedia no es la mejor fuente para una definici√≥n precisa y concisa del coeficiente de elasticidad precio-demanda.  Intentar√© reformular la pregunta para buscar informaci√≥n relacionada en Wikipedia, esperando encontrar informaci√≥n que me permita construir la definici√≥n.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Elasticidad de la demanda\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: Wikipedia no est√° resultando √∫til.  No tengo acceso a otros motores de b√∫squeda, por lo que no puedo responder a la pregunta.  Necesitar√≠a m√°s herramientas para poder definir el coeficiente de elasticidad precio-demanda.\n",
      "\n",
      "Final Answer: No puedo responder a la pregunta con las herramientas proporcionadas.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'input': 'Define el coeficiente de elasticidad precio-demanda.', 'output': 'No puedo responder a la pregunta con las herramientas proporcionadas.'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: ReActAgentTool no pudo proporcionar una respuesta.  Intentar√© reformular la pregunta para RAGTool, buscando una definici√≥n m√°s b√°sica y menos enfocada en m√©todos de medici√≥n.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: Define el coeficiente de elasticidad precio-demanda de forma sencilla.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 2\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPregunta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpregunta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRespuesta esperada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrespuesta_correcta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m respuesta_generada = \u001b[43msupervisor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRespuesta del agente supervisor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrespuesta_generada\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1328\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m   1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1411\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1433\u001b[39m, in \u001b[36mAgentExecutor._perform_agent_action\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[39m\n\u001b[32m   1431\u001b[39m         tool_run_kwargs[\u001b[33m\"\u001b[39m\u001b[33mllm_prefix\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1432\u001b[39m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     observation = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1441\u001b[39m     tool_run_kwargs = \u001b[38;5;28mself\u001b[39m._action_agent.tool_run_logging_kwargs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/tools/base.py:882\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    881\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    883\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    884\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/tools/base.py:851\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    850\u001b[39m         tool_kwargs = tool_kwargs | {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/tools/simple.py:105\u001b[39m, in \u001b[36mTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m    104\u001b[39m         kwargs[config_param] = config\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrag_tool_func\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrag_tool_func\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1255\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1251\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1252\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.\u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mcode_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1253\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1342\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1318\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     **kwargs: Any,\n\u001b[32m   1330\u001b[39m ) -> ChatResult:\n\u001b[32m   1331\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1332\u001b[39m         messages,\n\u001b[32m   1333\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1340\u001b[39m         tool_choice=tool_choice,\n\u001b[32m   1341\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:210\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    205\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:192\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 2\n}\n]"
     ]
    }
   ],
   "source": [
    "#replica pregunta 2.1.4\n",
    "preguntas_respuestas = [\n",
    "    (\n",
    "        \"¬øC√≥mo se define el coeficiente de elasticidad precio-demanda?\",\n",
    "        \"El coeficiente de elasticidad precio-demanda se define como el cambio porcentual en la cantidad demandada dividido por el cambio porcentual en el precio.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© estrategias se utilizan para optimizar las tarifas de habitaciones de hoteles?\",\n",
    "        \"Algunas estrategias incluyen precios din√°micos, segmentaci√≥n de mercado, uso de modelos de demanda y revenue management.\"\n",
    "    ),\n",
    "    (\n",
    "        \"¬øQu√© m√©todos utilizan los hoteles para predecir la demanda?\",\n",
    "        \"Los hoteles utilizan m√©todos como an√°lisis hist√≥rico de reservas, modelos de regresi√≥n y aprendizaje autom√°tico para predecir la demanda.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for pregunta, respuesta_correcta in preguntas_respuestas:\n",
    "    print(f\"Pregunta: {pregunta}\")\n",
    "    print(f\"Respuesta esperada: {respuesta_correcta}\")\n",
    "    respuesta_generada = supervisor.invoke({\"input\": pregunta})[\"output\"]\n",
    "    print(f\"Respuesta del agente supervisor: {respuesta_generada}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito informaci√≥n sobre algoritmos que se utilizan para la optimizaci√≥n de tarifas de hoteles usando datos hist√≥ricos.  La herramienta RAGTool parece la m√°s adecuada para obtener esta informaci√≥n directamente.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øQu√© algoritmos optimizan tarifas de hotel con base en datos hist√≥ricos?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVarios papers cient√≠ficos mencionan algoritmos de optimizaci√≥n de precios para hoteles basados en datos hist√≥ricos.  Un modelo se enfoc√≥ en la optimizaci√≥n de precios para una fecha y duraci√≥n de estad√≠a (LOS) espec√≠fica, bas√°ndose en la demanda (del hotel y sus competidores), costos de las habitaciones y disponibilidad.  Otro, usado por Carlson Rezidor Hotel Group, consider√≥ la elasticidad precio de la demanda, tarifas del mercado (competidores), disponibilidad, demanda pronosticada (basada en datos hist√≥ricos de clientes ‚Äî duraci√≥n de estad√≠a, segmento de tarifa, d√≠a de la semana y tiempo de anticipaci√≥n de la reserva) y reglas de negocio. Este √∫ltimo aument√≥ los ingresos de los hoteles que lo usaron entre un 2% y un 4% m√°s que los que no lo hicieron.  Finalmente, otro estudio compar√≥ m√©todos de asignaci√≥n de red anidada y de precio de puja, encontrando que el m√©todo de red anidada super√≥ al de precio de puja.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de RAGTool proporciona ejemplos de algoritmos impl√≠citos, pero no nombra algoritmos espec√≠ficos.  Para obtener nombres concretos, necesito una b√∫squeda m√°s dirigida.  Intentar√© reformular la pregunta para RAGTool.\n",
      "\n",
      "Action: RAGTool\n",
      "Action Input: ¬øQu√© algoritmos de aprendizaje autom√°tico o estad√≠sticos se utilizan com√∫nmente para la optimizaci√≥n de precios de hoteles basada en datos hist√≥ricos?  Proporciona ejemplos concretos.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLa informaci√≥n proporcionada no especifica algoritmos de aprendizaje autom√°tico o estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles basados en datos hist√≥ricos.  Si bien se menciona el uso de \"machine learning techniques\" para la predicci√≥n de demanda en un estudio (cap√≠tulo 4), no se detalla qu√© t√©cnicas espec√≠ficas se emplearon.  Tampoco se describen algoritmos en los otros papers citados.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La herramienta RAGTool no est√° proporcionando los nombres de algoritmos espec√≠ficos.  Parece que necesito una estrategia diferente.  Intentar√© usar ReActAgentTool para que investigue y me d√© ejemplos concretos.\n",
      "\n",
      "Action: ReActAgentTool\n",
      "Action Input: Encuentra ejemplos concretos de algoritmos de aprendizaje autom√°tico o estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles basada en datos hist√≥ricos.  Busca en art√≠culos acad√©micos y recursos de la industria hotelera.\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre algoritmos de aprendizaje autom√°tico y estad√≠sticos usados en la optimizaci√≥n de precios de hoteles, necesito buscar informaci√≥n en art√≠culos acad√©micos y recursos de la industria hotelera.  Como no tengo acceso directo a bases de datos acad√©micas, usar√© una b√∫squeda en internet a trav√©s de Tavily para encontrar art√≠culos relevantes que mencionen algoritmos espec√≠ficos.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"algoritmos aprendizaje autom√°tico optimizaci√≥n precios hoteles\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Algoritmos IA: Revolucionando la Gesti√≥n de Inventarios en Hoteles', 'url': 'https://metaversos.agency/blog/algoritmos-ia-revolucionando-la-gestion-de-inventarios-en-hoteles/', 'content': 'La implementaci√≥n de **algoritmos de IA** en la **GESTI√ìN DE INVENTARIOS DE HOTELES**, con la finalidad de **optimizar la disponibilidad y precios de las habitaciones en TIEMPO REAL**, conlleva ventajas innegables, pero tambi√©n implica una serie de retos que el sector no puede pasar por alto. A pesar de la evoluci√≥n favorable en las t√©cnicas de **revenue management**, los sistemas de aprendizaje autom√°tico (machine learning) y la creciente interconectividad de los PMS (Property Management [...] La **introducci√≥n de algoritmos** que optimizan la **disponibilidad y precios de las habitaciones** aporta numerosas ventajas al sector hotelero, abarcando desde el aumento de los ingresos hasta una mejora significativa de la relaci√≥n con el cliente. Estos **modelos matem√°ticos y soluciones de IA** inciden tanto en la maximizaci√≥n de la rentabilidad como en la competitividad del destino, impulsando un crecimiento sostenible y una gesti√≥n m√°s eficiente de los recursos [...] Estos algoritmos incluyen modelos estad√≠sticos y de aprendizaje autom√°tico que permiten, por ejemplo, prever con relativa precisi√≥n cu√°ntas habitaciones se ocupar√°n en un determinado d√≠a o intervalo de tiempo. Cuando la demanda se prev√© alta, el sistema sugiere incrementar los precios. Cuando la demanda no satisface las expectativas, recomienda bajarlos o lanzar promociones dirigidas a segmentos de clientes concretos, preservando el margen de ganancia. Esta labor no reemplaza el criterio del', 'score': 0.8737439}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci√≥n de Tavily proporciona una descripci√≥n general, pero no nombra algoritmos espec√≠ficos. Necesito buscar m√°s informaci√≥n, enfoc√°ndome en art√≠culos acad√©micos o reportes de la industria que detallen algoritmos concretos.  Intentar√© una b√∫squeda m√°s espec√≠fica en Google Scholar a trav√©s de Tavily.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"revenue management hotel pricing algorithms machine learning\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Modern Day Revenue Management for Hotels | KRG Hospitality', 'url': 'https://krghospitality.com/2024/09/10/modern-day-revenue-management-for-hotels/', 'content': 'This cloud-based revenue management system (RMS) uses sophisticated algorithms and machine learning to provide real-time pricing', 'score': 0.8638634}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La b√∫squeda en Tavily proporciona ejemplos, pero no algoritmos espec√≠ficos.  La informaci√≥n es demasiado general.  Intentar√© una b√∫squeda m√°s espec√≠fica en Google Scholar (aunque no tengo acceso directo a √©l, usar√© Tavily para simular una b√∫squeda en esa base de datos).  Buscar√© t√©rminos m√°s precisos relacionados con algoritmos.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"hotel pricing optimization algorithms regression trees gradient boosting\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'Maximizing Profitability and Occupancy: An Optimal Pricing Strategy ...', 'url': 'https://www.mdpi.com/1911-8074/17/9/414', 'content': 'The study used three advanced machine learning algorithms, gradient boosting, support vector regression (SVR), and neural networks, to enhance predictive', 'score': 0.7707586}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La √∫ltima b√∫squeda en Tavily ha proporcionado un resultado que menciona algoritmos espec√≠ficos: gradient boosting, support vector regression (SVR), y redes neuronales.  Esto responde a la pregunta.\n",
      "\n",
      "Final Answer: Algunos ejemplos concretos de algoritmos de aprendizaje autom√°tico y estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles, basados en datos hist√≥ricos, incluyen gradient boosting, support vector regression (SVR) y redes neuronales.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'input': 'Encuentra ejemplos concretos de algoritmos de aprendizaje autom√°tico o estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles basada en datos hist√≥ricos.  Busca en art√≠culos acad√©micos y recursos de la industria hotelera.', 'output': 'Algunos ejemplos concretos de algoritmos de aprendizaje autom√°tico y estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles, basados en datos hist√≥ricos, incluyen gradient boosting, support vector regression (SVR) y redes neuronales.'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  ReActAgentTool provided specific algorithm examples.\n",
      "\n",
      "Final Answer: Algunos algoritmos de aprendizaje autom√°tico y estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles basados en datos hist√≥ricos incluyen gradient boosting, support vector regression (SVR) y redes neuronales.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Algunos algoritmos de aprendizaje autom√°tico y estad√≠sticos utilizados para la optimizaci√≥n de precios de hoteles basados en datos hist√≥ricos incluyen gradient boosting, support vector regression (SVR) y redes neuronales.\n"
     ]
    }
   ],
   "source": [
    "pregunta = \"¬øQu√© algoritmos optimizan tarifas de hotel con base en datos hist√≥ricos?\"\n",
    "response = supervisor.invoke({\"input\": pregunta})\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "Router (Enrutador)\n",
    "\n",
    "Es un √∫nico agente que elige una tool (o sub-agente) seg√∫n la entrada.\n",
    "Tiene una l√≥gica expl√≠cita (por ejemplo, reglas o embeddings) para decidir a qu√© herramienta encaminar la pregunta.\n",
    "Se implementa con algo como MultiPromptChain o RouterChain.\n",
    "\n",
    "Multiagente (como tu supervisor)\n",
    "\n",
    "Es un agente supervisor que tiene acceso a varias tools, donde cada tool encapsula una cadena o agente diferente (por ejemplo, un RAG y un ReAct).\n",
    "El supervisor razona sobre la tarea y decide cu√°l usar en cada paso (o incluso usar m√°s de una si el framework lo permite).\n",
    "Las decisiones se basan en planificaci√≥n y razonamiento, t√≠picamente usando un prompt ReAct.\n",
    "\n",
    "El agente supervisor puede razonar antes de usar herramientas, combinar pasos y responder en forma m√°s natural ante preguntas complejas o ambiguas. No depende de reglas fijas o embebimientos preentrenados. Sin embargo, El supervisor puede llamar al LLM muchas veces durante su razonamiento. Esto lo hace m√°s lento, m√°s caro y m√°s propenso a errores como ResourceExhausted, como ya lo viste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://471fdc29f8060b37bb.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://471fdc29f8060b37bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 854, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/chat_interface.py\", line 537, in _wrapper\n",
      "    async for chunk in submit_fn(*args, **kwargs):\n",
      "        yield chunk\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/chat_interface.py\", line 951, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 743, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        run_sync_iterator_async, self.iterator, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/utils.py\", line 726, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/var/folders/09/2_xdwnnx5rqgtl8vh2vwkdmc0000gn/T/ipykernel_3438/3589718755.py\", line 9, in agent_response\n",
      "    response = supervisor.invoke({\"input\": message})\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1620, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "        name_to_tool_map,\n",
      "    ...<3 lines>...\n",
      "        run_manager=run_manager,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1328, in _take_next_step\n",
      "    for a in self._iter_next_step(\n",
      "             ~~~~~~~~~~~~~~~~~~~~^\n",
      "        name_to_tool_map,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        run_manager,\n",
      "        ^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 1354, in _iter_next_step\n",
      "    output = self._action_agent.plan(\n",
      "        intermediate_steps,\n",
      "        callbacks=run_manager.get_child() if run_manager else None,\n",
      "        **inputs,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py\", line 800, in plan\n",
      "    full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 319, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 189, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 386, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 167, in invoke\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py\", line 157, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 127, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/llm.py\", line 139, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **self.llm_kwargs,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 1342, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ~~~~~~~~~~~~~~~~^\n",
      "        request=request,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        metadata=self.default_metadata,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 210, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tenacity/__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 208, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\", line 192, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "        request,\n",
      "    ...<2 lines>...\n",
      "        metadata=metadata,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "        target,\n",
      "    ...<3 lines>...\n",
      "        on_error=on_error,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "        exc,\n",
      "    ...<6 lines>...\n",
      "        timeout,\n",
      "    )\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  response = supervisor.invoke({\"input\": message})\n",
    "\n",
    "  assert type(response) == str, \"response de route_question debe ser string\"\n",
    "\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "√âxito!\n",
    "<center>\n",
    "<img src =\"https://media.tenor.com/MRQgxcelAV8AAAAM/perry-the-platypus-phineas-and-ferb.gif\" width = 400 />"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNs28AeL6L8BlEf3067k5qg",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
